{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "train_path = '../../raw_train_artifact'\n",
    "test_path = '../../raw_test_artifact'\n",
    "embedding_path = '../../embedding_artifact'\n",
    "input_path = '../../input_artifact'\n",
    "input_split_path = '../../input_artifact/input_split'\n",
    "model_path = '../../model_artifact'\n",
    "output_path = '../../output_artifact'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gc\n",
    "gc.enable()\n",
    "import time\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns',120)\n",
    "pd.set_option('display.max_rows',2000)\n",
    "pd.set_option('precision',5)\n",
    "pd.set_option('float_format', '{:.5f}'.format)\n",
    "\n",
    "import tqdm\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from gensim.models import Word2Vec\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:27:39 INFO: Restart notebook\n",
      "==========================\n",
      "Sun Jun  7 16:27:39 2020\n",
      "==========================\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "log_path = '[1.1]Hugging Face Transformer Experiment.log'\n",
    "    \n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)-s: %(message)s', datefmt='%H:%M:%S')\n",
    "\n",
    "fh = logging.FileHandler(log_path)\n",
    "fh.setLevel(logging.INFO)\n",
    "fh.setFormatter(formatter)\n",
    "logger.addHandler(fh)\n",
    "\n",
    "sh = logging.StreamHandler(sys.stdout)\n",
    "sh.setLevel(logging.INFO)\n",
    "sh.setFormatter(formatter)\n",
    "logger.addHandler(sh)\n",
    "\n",
    "logger.info(f'Restart notebook\\n==========================\\n{time.ctime()}\\n==========================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:27:39 INFO: Device in Use: cuda\n",
      "16:27:39 INFO: CUDA Memory: Total 8.00 GB, Cached 0.00 GB, Allocated 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info('Device in Use: {}'.format(DEVICE))\n",
    "torch.cuda.empty_cache()\n",
    "t = torch.cuda.get_device_properties(DEVICE).total_memory/1024**3\n",
    "c = torch.cuda.memory_cached(DEVICE)/1024**3\n",
    "a = torch.cuda.memory_allocated(DEVICE)/1024**3\n",
    "logger.info('CUDA Memory: Total {:.2f} GB, Cached {:.2f} GB, Allocated {:.2f} GB'.format(t,c,a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_torch_module_num_of_parameter(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_bert import BertConfig, BertEncoder, BertAttention,BertSelfAttention,BertLayer,BertPooler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class Positional_Encoding_Layer(nn.Module):\n",
    "\t\"\"\"\n",
    "\tPositional encoding using sine and cosine as described in \"Attention is all you need\".\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, d_model, max_seq_len=512, dropout=0.1):\n",
    "\t\t\"\"\"\n",
    "\t\tFormula:\n",
    "\t\t| PE(pos,2i) = sin(pos/10000**(2*i/d_model))\n",
    "\t\t| PE(pos,2i+1) = cos(pos/10000**(2*i/d_model))\n",
    "\t\t\"\"\"\n",
    "\t\tsuper(Positional_Encoding_Layer, self).__init__()\n",
    "\t\tself.d_model = d_model\n",
    "\t\tself.dropout = dropout\n",
    "\t\tself.max_seq_len = max_seq_len\n",
    "\n",
    "\t\tself.dropout_layer = nn.Dropout(p=dropout)\n",
    "\t\tpe = torch.zeros(max_seq_len, d_model)                                                       # (max_seq_len, d_model)\n",
    "\t\tposition = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)                      # (max_seq_len, 1)\n",
    "\t\tdiv_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))     # (d_model/2)\n",
    "\t\tpe[:, 0::2] = torch.sin(position * div_term)\n",
    "\t\tpe[:, 1::2] = torch.cos(position * div_term) \n",
    "\t\tpe = pe.unsqueeze(0).transpose(0, 1)                                                         # (max_seq_len, 1, d_model)\n",
    "\t\tself.register_buffer('pe', pe)\n",
    "\n",
    "\tdef forward(self, inp):\n",
    "\t\tinp = inp + self.pe[:inp.size(0), :]                                                         # (n_step, batch_size, d_model)\n",
    "\t\treturn self.dropout_layer(inp)\n",
    "\n",
    "class Transformer_Encoder_Extraction_Layer(nn.Module):\n",
    "\t\"\"\"\n",
    "\tTransformer encoder as described in \"Attention is all you need\".\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, n_enc_layer, embed_size, n_head, intermediate_size, max_seq_len=512, dropout=0.1, **kwargs):\n",
    "\t\tsuper(Transformer_Encoder_Extraction_Layer, self).__init__(**kwargs)\n",
    "\t\tassert embed_size%n_head==0\n",
    "\n",
    "\t\tself.n_enc_layer = n_enc_layer\n",
    "\t\tself.embed_size = embed_size\n",
    "\t\tself.max_seq_len = max_seq_len\n",
    "\t\tself.n_head = n_head\n",
    "\t\tself.intermediate_size = intermediate_size\n",
    "\t\tself.dropout = dropout\n",
    "\n",
    "\t\tself.positional_encoder = Positional_Encoding_Layer(embed_size, max_seq_len=max_seq_len)\n",
    "\t\ttransformer_encoder_layer = TransformerEncoderLayer(embed_size, n_head, dim_feedforward=intermediate_size, dropout=dropout)\n",
    "\t\tself.transformer_encoder = TransformerEncoder(transformer_encoder_layer, n_enc_layer)\n",
    "\n",
    "\t\tself._init_weights()\n",
    "\n",
    "\tdef _init_weights(self):\n",
    "\t\tfor p in self.parameters():\n",
    "\t\t\tif p.dim() > 1:\n",
    "\t\t\t\txavier_uniform_(p)\n",
    "\n",
    "\tdef forward(self, inp, inp_padding_mask=None):\n",
    "\t\tinp = inp * np.sqrt(self.embed_size)                                           # (batch_size, n_step, embed_size)\n",
    "\t\tinp = self.positional_encoder(inp.permute(1, 0, 2))                            # (n_step, batch_size, embed_size)\n",
    "\t\tout = self.transformer_encoder(inp, src_key_padding_mask=inp_padding_mask)     # (n_step, batch_size, embed_size)\n",
    "\t\treturn out.permute(1, 0, 2)\n",
    "\n",
    "class MLP_Classification_Layer(nn.Module):\n",
    "\t\"\"\"\n",
    "\tMultilayer Perception Classification Layer\n",
    "\t- Layer 1: Linear + Batchnorm + ReLU + Dropout\n",
    "\t- Layer 2: Linear + Batchnorm + ReLU + Dropout\n",
    "\t- Layer 3: Linear\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, inp_size, out_size, dropout=0.4, **kwargs):\n",
    "\t\tsuper(MLP_Classification_Layer, self).__init__(**kwargs)\n",
    "\t\tself.inp_size = inp_size\n",
    "\t\tself.out_size = out_size\n",
    "\t\tself.dropout = dropout\n",
    "\t\t\n",
    "\t\tself.mlp_1 = nn.Linear(inp_size, 1024)\n",
    "\t\tself.batchnorm_1 = nn.BatchNorm1d(1024)\n",
    "\t\tself.mlp_dropout_1 = nn.Dropout(p=dropout)\n",
    "\t\tself.mlp_2 = nn.Linear(1024, 512)\n",
    "\t\tself.batchnorm_2 = nn.BatchNorm1d(512)\n",
    "\t\tself.mlp_dropout_2 = nn.Dropout(p=dropout)\n",
    "\t\tself.mlp_3 = nn.Linear(512, out_size)\n",
    "\t\n",
    "\tdef _init_weights(self):\n",
    "\t\tinitrange = 0.1\n",
    "\t\tself.mlp_1.weight.data.uniform_(-initrange, initrange)\n",
    "\t\tself.mlp_1.bias.data.zero_()\n",
    "\t\tself.mlp_2.weight.data.uniform_(-initrange, initrange)\n",
    "\t\tself.mlp_2.bias.data.zero_()\n",
    "\t\tself.mlp_3.weight.data.uniform_(-initrange, initrange)\n",
    "\t\tself.mlp_3.bias.data.zero_()\n",
    "\n",
    "\tdef forward(self, inp):\n",
    "\t\tmlp_out = self.mlp_1(inp)                                                         # (batch_size, 1024)\n",
    "\t\tmlp_out = self.mlp_dropout_1(F.relu(self.batchnorm_1(mlp_out)))                   # (batch_size, 1024)\n",
    "\t\tmlp_out = self.mlp_2(mlp_out)                                                     # (batch_size, 512)\n",
    "\t\tmlp_out = self.mlp_dropout_2(F.relu(self.batchnorm_2(mlp_out)))                   # (batch_size, 512)\n",
    "\t\tmlp_out = self.mlp_3(mlp_out)                                                     # (batch_size, out_size)\n",
    "\t\treturn mlp_out   \n",
    "\t\n",
    "class Transformer_Encoder_Classifier(nn.Module):\n",
    "\t\"\"\"\n",
    "\tTransformer Encoder + Multilayer Perception for Classification\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, embed_size, out_size, n_enc_layer, n_head, intermediate_size, device, transformer_dropout=0.1, mlp_dropout=0.4, **kwargs):\n",
    "\t\tsuper(Transformer_Encoder_Classifier, self).__init__(**kwargs)\n",
    "\t\t\n",
    "\t\tself.embed_size = embed_size\n",
    "\t\tself.out_size = out_size\n",
    "\t\tself.n_enc_layer = n_enc_layer\n",
    "\t\tself.n_head = n_head\n",
    "\t\tself.intermediate_size = intermediate_size\n",
    "\t\tself.device = device\n",
    "\t\tself.transformer_dropout = transformer_dropout\n",
    "\t\tself.mlp_dropout = mlp_dropout\n",
    "\n",
    "\t\tself.encoder_layer = Transformer_Encoder_Extraction_Layer(n_enc_layer, embed_size, n_head, intermediate_size, dropout=transformer_dropout)\n",
    "\t\tself.classification_layer = MLP_Classification_Layer(embed_size, out_size, dropout=mlp_dropout)\n",
    "\n",
    "\tdef get_padding_mask(self, batch_size, seq_len, inp_last_idx):\n",
    "\t\tpadding_mask = np.ones((batch_size, seq_len))\n",
    "\t\tfor index, last_idx in enumerate(inp_last_idx):\n",
    "\t\t\tpadding_mask[index,:last_idx+1] = 0\n",
    "\t\treturn torch.from_numpy(padding_mask).bool().to(self.device)\n",
    "\n",
    "\tdef forward(self, inp_embed, inp_last_idx):\n",
    "\t\tassert inp_embed.shape[0] == inp_last_idx.shape[0]\n",
    "\t\tbatch_size = inp_embed.shape[0]\n",
    "\t\tseq_len = inp_embed.shape[1]\n",
    "\t\tinp_padding_mask = self.get_padding_mask(batch_size, seq_len, inp_last_idx)\n",
    "\t\tout = self.encoder_layer(inp_embed, inp_padding_mask=inp_padding_mask)               # (batch_size, n_step, embed_size)\n",
    "\t\tpooled_buf = []\n",
    "\t\tfor index, last_idx in enumerate(inp_last_idx):\n",
    "\t\t\tpooled_buf.append(torch.mean(out[index,:last_idx+1,:], dim=0))\n",
    "\t\tout = torch.stack(pooled_buf)                                                        # (batch_size, embed_size)\n",
    "\t\tout = self.classification_layer(out)                                                 # (batch_size, out_size)\n",
    "\t\treturn out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer_Encoder_Classifier(256, 2, 6, 8, 2048, torch.device('cpu'))\n",
    "get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.from_numpy(np.concatenate([p, np.zeros((100, 4, 256))], axis=1)).float()\n",
    "inp_last_idx = np.array([5 for _ in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.6068e-01, -4.1554e-02],\n",
       "        [ 1.0114e-01,  2.2231e-01],\n",
       "        [-9.9992e-02, -1.4431e-01],\n",
       "        [ 4.9414e-01,  6.9754e-03],\n",
       "        [-8.7818e-01, -7.5826e-01],\n",
       "        [-4.2960e-01, -3.7006e-01],\n",
       "        [ 3.0174e-02,  1.6330e-01],\n",
       "        [-1.2335e-01,  2.0211e-01],\n",
       "        [ 5.0249e-02,  1.0301e-01],\n",
       "        [-9.6979e-02,  9.5880e-02],\n",
       "        [-2.7329e-01, -1.8086e-01],\n",
       "        [-1.0915e+00, -1.7455e-01],\n",
       "        [ 2.5860e-01, -2.1152e-01],\n",
       "        [-1.2127e-01, -2.3629e-01],\n",
       "        [-2.8017e-01, -2.8132e-01],\n",
       "        [-2.5161e-01, -6.1382e-01],\n",
       "        [-1.2896e-01, -3.4738e-04],\n",
       "        [ 1.0753e+00, -7.7634e-02],\n",
       "        [-5.3120e-01, -4.6385e-02],\n",
       "        [ 3.2261e-01, -3.1052e-01],\n",
       "        [-3.0064e-01,  3.7959e-01],\n",
       "        [-9.7347e-01, -4.9769e-01],\n",
       "        [-3.1129e-01, -4.8083e-01],\n",
       "        [-9.3990e-01,  6.4359e-01],\n",
       "        [ 7.3314e-01, -2.5928e-01],\n",
       "        [-5.4152e-02, -1.1831e-01],\n",
       "        [-2.7646e-01,  3.3617e-01],\n",
       "        [ 2.6645e-02, -4.0520e-02],\n",
       "        [-1.4929e-01, -9.4194e-02],\n",
       "        [-1.8190e-01, -1.0786e-01],\n",
       "        [-4.6801e-01,  3.3474e-01],\n",
       "        [-2.0168e-01, -6.8339e-01],\n",
       "        [ 4.2669e-01,  9.9397e-02],\n",
       "        [-4.0233e-02,  3.6115e-01],\n",
       "        [ 8.3760e-01,  9.4882e-02],\n",
       "        [ 1.1881e-01, -6.2542e-01],\n",
       "        [-3.5421e-01, -7.6756e-01],\n",
       "        [-6.2716e-01,  6.3754e-01],\n",
       "        [-5.4672e-01,  6.8092e-02],\n",
       "        [-1.8011e-02,  3.7025e-01],\n",
       "        [-1.8775e-01, -6.4045e-01],\n",
       "        [-2.9845e-01, -4.8033e-01],\n",
       "        [ 2.4050e-01, -7.9619e-01],\n",
       "        [-5.9155e-01,  8.4248e-01],\n",
       "        [-1.0563e+00, -9.3424e-02],\n",
       "        [-5.7225e-01,  2.3514e-01],\n",
       "        [-3.6483e-01,  1.9224e-02],\n",
       "        [-2.2103e-01,  4.7511e-01],\n",
       "        [-6.1785e-01, -1.4422e-01],\n",
       "        [ 7.3144e-01,  2.0887e-02],\n",
       "        [-9.2109e-01, -1.1970e+00],\n",
       "        [-4.3555e-01,  1.8947e-01],\n",
       "        [-6.3176e-01, -5.9155e-01],\n",
       "        [-5.5757e-01, -6.2224e-01],\n",
       "        [ 6.6352e-04,  3.7234e-01],\n",
       "        [-3.5664e-01, -5.1285e-01],\n",
       "        [ 1.1488e-01, -9.5023e-01],\n",
       "        [ 1.3188e-02, -8.3193e-01],\n",
       "        [-3.7704e-01, -1.0569e-01],\n",
       "        [-6.6950e-01,  2.0826e-01],\n",
       "        [ 2.9195e-02, -4.3539e-01],\n",
       "        [-7.3269e-01,  3.5047e-03],\n",
       "        [ 2.5017e-01, -3.8992e-02],\n",
       "        [-8.7835e-01, -3.0180e-01],\n",
       "        [ 8.8486e-01,  1.0306e-01],\n",
       "        [ 1.5727e-02, -5.3175e-01],\n",
       "        [-4.6654e-01, -1.5444e-01],\n",
       "        [ 3.4822e-01,  7.7773e-02],\n",
       "        [-3.5164e-01,  7.3535e-01],\n",
       "        [ 7.0866e-01,  2.8611e-01],\n",
       "        [-2.7669e-01,  1.3963e-01],\n",
       "        [ 1.0773e-01, -2.2451e-02],\n",
       "        [ 8.3475e-02,  2.2262e-01],\n",
       "        [ 4.7876e-01, -2.6719e-01],\n",
       "        [ 1.3349e-01, -1.0476e+00],\n",
       "        [-1.0339e-01,  1.8559e-01],\n",
       "        [ 2.3587e-01, -6.7023e-02],\n",
       "        [-1.6798e-01, -4.3480e-01],\n",
       "        [-6.9791e-01, -4.3276e-01],\n",
       "        [-1.6005e+00, -2.9726e-01],\n",
       "        [-9.1998e-01,  2.1125e-01],\n",
       "        [ 1.7305e-01, -1.9485e-01],\n",
       "        [-4.8337e-02,  1.9152e-01],\n",
       "        [ 9.5207e-02, -4.9176e-01],\n",
       "        [ 1.3370e-01, -6.5575e-01],\n",
       "        [ 1.1358e-01,  6.0135e-01],\n",
       "        [-1.7937e-02,  3.1342e-01],\n",
       "        [ 5.1784e-01,  1.1221e-01],\n",
       "        [ 3.3864e-01,  2.0200e-01],\n",
       "        [ 6.0271e-01, -1.3144e-01],\n",
       "        [ 4.8704e-01, -9.5375e-02],\n",
       "        [ 2.8970e-01,  6.2329e-01],\n",
       "        [-4.7513e-01, -1.1474e+00],\n",
       "        [-4.2959e-01,  1.7401e-02],\n",
       "        [ 2.5260e-01,  8.2822e-02],\n",
       "        [ 1.1166e+00, -2.6355e-01],\n",
       "        [-5.5114e-01, -5.1911e-01],\n",
       "        [-7.3691e-01,  5.2182e-01],\n",
       "        [ 1.4549e-01,  6.1485e-01],\n",
       "        [-5.3020e-01,  4.4062e-01]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(inp, inp_last_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ones(1,10).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(1,10).squeeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 20])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([torch.mean(torch.ones(1,10,20), dim=1).squeeze(0), torch.mean(torch.ones(1,10,20), dim=1).squeeze(0)]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0, 10, dtype=torch.float).unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 13\n",
    "torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class Positional_Encoding_Layer(nn.Module):\n",
    "\t\"\"\"\n",
    "\tPositional encoding using sine and cosine as described in \"Attention is all you need\".\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, d_model, max_seq_len=128, dropout=0.1):\n",
    "\t\t\"\"\"\n",
    "\t\tFormula:\n",
    "\t\t| PE(pos,2i) = sin(pos/10000**(2*i/d_model))\n",
    "\t\t| PE(pos,2i+1) = cos(pos/10000**(2*i/d_model))\n",
    "\t\t\"\"\"\n",
    "\t\tsuper(Positional_Encoding_Layer, self).__init__()\n",
    "\t\tself.d_model = d_model\n",
    "\t\tself.dropout = dropout\n",
    "\t\tself.max_seq_len = max_seq_len\n",
    "\n",
    "\t\tself.dropout_layer = nn.Dropout(p=dropout)\n",
    "\t\tpe = torch.zeros(max_seq_len, d_model)                                                       # (max_seq_len, d_model)\n",
    "\t\tposition = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)                      # (max_seq_len, 1)\n",
    "\t\tdiv_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))     # (d_model/2)\n",
    "\t\tpe[:, 0::2] = torch.sin(position * div_term)\n",
    "\t\tpe[:, 1::2] = torch.cos(position * div_term) \n",
    "\t\tpe = pe.unsqueeze(0).transpose(0, 1)                                                         # (max_seq_len, 1, d_model)\n",
    "\t\tself.register_buffer('pe', pe)\n",
    "\n",
    "\tdef forward(self, inp):\n",
    "\t\tinp = inp + self.pe[:inp.size(0), :]                                                         # (n_step, batch_size, d_model)\n",
    "\t\treturn self.dropout_layer(inp)\n",
    "\n",
    "class Transformer_Encoder_Extraction_Layer(nn.Module):\n",
    "\t\"\"\"\n",
    "\tTransformer encoder as described in \"Attention is all you need\".\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, n_enc_layer, embed_size, n_head, intermediate_size, max_seq_len=512, dropout=0.1, **kwargs):\n",
    "\t\tsuper(Transformer_Encoder_Extraction_Layer, self).__init__(**kwargs)\n",
    "\t\tassert embed_size%n_head==0\n",
    "\n",
    "\t\tself.n_enc_layer = n_enc_layer\n",
    "\t\tself.embed_size = embed_size\n",
    "\t\tself.max_seq_len = max_seq_len\n",
    "\t\tself.n_head = n_head\n",
    "\t\tself.intermediate_size = intermediate_size\n",
    "\t\tself.dropout = dropout\n",
    "\n",
    "\t\tself.positional_encoder = Positional_Encoding_Layer(embed_size, max_seq_len=max_seq_len)\n",
    "\t\ttransformer_encoder_layer = TransformerEncoderLayer(embed_size, n_head, dim_feedforward=intermediate_size, dropout=dropout)\n",
    "\t\tself.transformer_encoder = TransformerEncoder(transformer_encoder_layer, n_enc_layer)\n",
    "\n",
    "\t\tself._init_weights()\n",
    "\n",
    "\tdef _init_weights(self):\n",
    "\t\tfor p in self.parameters():\n",
    "\t\t\tif p.dim() > 1:\n",
    "\t\t\t\txavier_uniform_(p)\n",
    "\n",
    "\tdef forward(self, inp, inp_padding_mask=None):\n",
    "\t\tinp = inp * np.sqrt(self.embed_size)                                           # (batch_size, n_step, embed_size)\n",
    "\t\tinp = self.positional_encoder(inp.permute(1, 0, 2))                            # (n_step, batch_size, embed_size)\n",
    "\t\tout = self.transformer_encoder(inp, src_key_padding_mask=inp_padding_mask)     # (n_step, batch_size, embed_size)\n",
    "\t\treturn out.permute(1, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_torch_module_num_of_parameter(model):\n",
    "\t\"\"\"\n",
    "\tGet # of parameters in a torch module.\n",
    "\t\"\"\"\n",
    "\tmodel_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "\tparams = sum([np.prod(p.size()) for p in model_parameters])\n",
    "\treturn params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7890432"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer_Encoder_Extraction_Layer(6, 256, 8, 2048)\n",
    "get_torch_module_num_of_parameter(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.random.standard_normal((100,6,256))\n",
    "inp_1 = torch.from_numpy(np.concatenate([p, np.zeros((100, 4, 256))], axis=1)).float()\n",
    "inp_2 = torch.from_numpy(np.concatenate([p, np.ones((100, 4, 256))], axis=1)).float()\n",
    "pad_mask = torch.from_numpy(np.concatenate([np.zeros((100, 6)), np.ones((100,4))], axis=1)).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padding_mask(batch_size, seq_len, inp_last_idx):\n",
    "    padding_mask = np.ones((batch_size, seq_len))\n",
    "    for index, last_idx in enumerate(inp_last_idx):\n",
    "        padding_mask[index,:last_idx+1] = 0\n",
    "    return torch.from_numpy(padding_mask).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False,  True,  True]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_padding_mask(3,10,[5,6,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "out_1 = model(inp_1, inp_padding_mask=pad_mask)\n",
    "out_2 = model(inp_2, inp_padding_mask=pad_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = BertConfig(vocab_size=1, hidden_size=4, num_hidden_layers=8, num_attention_heads=2, intermediate_size=2048)\n",
    "model = BertModel(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.from_numpy(np.concatenate([np.random.standard_normal((1, 5, 4)), np.zeros((1, 5, 4))], axis=1)).float()\n",
    "mask = torch.from_numpy(np.concatenate([np.zeros((1, 5)), np.zeros((1, 5))], axis=1)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4829,  0.3867,  1.2085,  1.0879],\n",
       "         [-0.4869, -2.1700,  0.7577, -0.3682],\n",
       "         [-1.4087, -0.4176, -0.6754,  0.0763],\n",
       "         [-0.1583, -0.2029,  0.6745, -0.6823],\n",
       "         [ 0.4552,  0.5747, -0.3314, -1.9143],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000]]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "out= model(inputs_embeds=inp, attention_mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 4])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.5516, -0.2037,  0.9508,  0.8045],\n",
       "         [ 0.1553, -1.5379,  1.2636,  0.1190],\n",
       "         [-0.6950,  0.0802, -0.9813,  1.5961],\n",
       "         [-0.0377, -0.2082,  1.5238, -1.2779],\n",
       "         [ 0.7606,  0.8213,  0.0738, -1.6557],\n",
       "         [ 0.2063,  0.3378,  1.0872, -1.6313],\n",
       "         [ 0.4727,  0.4431,  0.7993, -1.7151],\n",
       "         [ 0.6379,  0.8671,  0.1718, -1.6768],\n",
       "         [ 0.5959,  0.3670,  0.7528, -1.7157],\n",
       "         [ 1.2472,  0.6829, -0.7249, -1.2052]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with torch padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AlbertModel, AlbertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = AlbertConfig()\n",
    "model = AlbertModel(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222595584"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_torch_module_num_of_parameter(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.from_numpy(np.concatenate([np.ones((10, 5, 128)), np.zeros((10, 5, 128))], axis=1)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "out= model(inputs_embeds=inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10, 4096])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.0415586 , -0.700448  , -0.23963146, ..., -0.34111258,\n",
       "        -0.88970816,  2.0453863 ],\n",
       "       [-1.0419453 , -0.7001726 , -0.24012905, ..., -0.34073353,\n",
       "        -0.8900057 ,  2.0456576 ],\n",
       "       [-1.0417546 , -0.7000878 , -0.24050954, ..., -0.34136513,\n",
       "        -0.8902673 ,  2.0448096 ],\n",
       "       ...,\n",
       "       [-1.0415502 , -0.70019853, -0.23981182, ..., -0.34107703,\n",
       "        -0.8901159 ,  2.0457187 ],\n",
       "       [-1.0413502 , -0.6995464 , -0.240383  , ..., -0.34042382,\n",
       "        -0.8905616 ,  2.0456984 ],\n",
       "       [-1.0413843 , -0.70032   , -0.23944165, ..., -0.34181798,\n",
       "        -0.89004594,  2.0447192 ]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0][1].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
