{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "train_path = '../../raw_train_artifact'\n",
    "test_path = '../../raw_test_artifact'\n",
    "embedding_path = '../../embedding_artifact'\n",
    "input_path = '../../input_artifact'\n",
    "input_split_path = '../../input_artifact/input_split'\n",
    "model_path = '../../model_artifact'\n",
    "output_path = '../../output_artifact'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gc\n",
    "gc.enable()\n",
    "import time\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns',120)\n",
    "pd.set_option('display.max_rows',2000)\n",
    "pd.set_option('precision',5)\n",
    "pd.set_option('float_format', '{:.5f}'.format)\n",
    "\n",
    "import tqdm\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from gensim.models import Word2Vec\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:52:29 INFO: Restart notebook\n",
      "==========================\n",
      "Mon Jun  8 14:52:29 2020\n",
      "==========================\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "log_path = '[1.1]Hugging Face Transformer Experiment.log'\n",
    "    \n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)-s: %(message)s', datefmt='%H:%M:%S')\n",
    "\n",
    "fh = logging.FileHandler(log_path)\n",
    "fh.setLevel(logging.INFO)\n",
    "fh.setFormatter(formatter)\n",
    "logger.addHandler(fh)\n",
    "\n",
    "sh = logging.StreamHandler(sys.stdout)\n",
    "sh.setLevel(logging.INFO)\n",
    "sh.setFormatter(formatter)\n",
    "logger.addHandler(sh)\n",
    "\n",
    "logger.info(f'Restart notebook\\n==========================\\n{time.ctime()}\\n==========================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# logger.info('Device in Use: {}'.format(DEVICE))\n",
    "# torch.cuda.empty_cache()\n",
    "# t = torch.cuda.get_device_properties(DEVICE).total_memory/1024**3\n",
    "# c = torch.cuda.memory_cached(DEVICE)/1024**3\n",
    "# a = torch.cuda.memory_allocated(DEVICE)/1024**3\n",
    "# logger.info('CUDA Memory: Total {:.2f} GB, Cached {:.2f} GB, Allocated {:.2f} GB'.format(t,c,a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_torch_module_num_of_parameter(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM EXP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_1 = nn.LSTM(input_size=2, hidden_size=2, bidirectional=True, batch_first=True)\n",
    "lstm_2 = nn.LSTM(input_size=4, hidden_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [torch.tensor([[1,1],[2,2],[3,3]]).float(), torch.tensor([[1,1],[2,2]]).float()]\n",
    "b = torch.nn.utils.rnn.pad_sequence(a, batch_first=True)\n",
    "inp = torch.nn.utils.rnn.pack_padded_sequence(b, batch_first=True, lengths=[3,2], enforce_sorted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden[0] size (1, 2, 4), got (2, 2, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-0e1c6b23d58b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlstm_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlstm_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\yifan wu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yifan wu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    565\u001b[0m             \u001b[0mhx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 567\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    568\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "\u001b[1;32mc:\\users\\yifan wu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    520\u001b[0m         \u001b[0mexpected_hidden_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_expected_hidden_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 522\u001b[1;33m         self.check_hidden_size(hidden[0], expected_hidden_size,\n\u001b[0m\u001b[0;32m    523\u001b[0m                                'Expected hidden[0] size {}, got {}')\n\u001b[0;32m    524\u001b[0m         self.check_hidden_size(hidden[1], expected_hidden_size,\n",
      "\u001b[1;32mc:\\users\\yifan wu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mcheck_hidden_size\u001b[1;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[0;32m    185\u001b[0m         \u001b[1;31m# type: (Tensor, Tuple[int, int, int], str) -> None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mexpected_hidden_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpected_hidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected hidden[0] size (1, 2, 4), got (2, 2, 2)"
     ]
    }
   ],
   "source": [
    "out, (h, c) = lstm_1(inp)\n",
    "\n",
    "nx, _ = lstm_2(out, (h,c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2503,  0.2214],\n",
       "         [-0.0423,  0.1840]],\n",
       "\n",
       "        [[ 0.4017, -0.1086],\n",
       "         [ 0.3544, -0.1220]]], grad_fn=<IndexSelectBackward>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2503,  0.2214],\n",
       "         [ 0.4017, -0.1086]],\n",
       "\n",
       "        [[-0.0423,  0.1840],\n",
       "         [ 0.3544, -0.1220]]], grad_fn=<PermuteBackward>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.permute(1,0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0785,  0.0740,  0.4017, -0.1086],\n",
       "         [-0.0423,  0.1840,  0.3712, -0.0433],\n",
       "         [-0.2503,  0.2214,  0.2945, -0.0142]],\n",
       "\n",
       "        [[ 0.0785,  0.0740,  0.3544, -0.1220],\n",
       "         [-0.0423,  0.1840,  0.2733, -0.0538],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000]]], grad_fn=<IndexSelectBackward>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.utils.rnn.pad_packed_sequence(out, batch_first=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0913, -0.1721],\n",
       "         [ 0.1276, -0.2379],\n",
       "         [ 0.1945, -0.2649]],\n",
       "\n",
       "        [[-0.0187, -0.1936],\n",
       "         [ 0.1482, -0.2489],\n",
       "         [ 0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0867, -0.2293],\n",
       "         [ 0.1775, -0.2689],\n",
       "         [ 0.1964, -0.3072]]], grad_fn=<IndexSelectBackward>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.utils.rnn.pad_packed_sequence(nx, batch_first=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_bert import BertConfig, BertEncoder, BertAttention,BertSelfAttention,BertLayer,BertPooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class Positional_Encoding_Layer(nn.Module):\n",
    "\t\"\"\"\n",
    "\tPositional encoding using sine and cosine as described in \"Attention is all you need\".\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, d_model, max_seq_len=512, dropout=0.1):\n",
    "\t\t\"\"\"\n",
    "\t\tFormula:\n",
    "\t\t| PE(pos,2i) = sin(pos/10000**(2*i/d_model))\n",
    "\t\t| PE(pos,2i+1) = cos(pos/10000**(2*i/d_model))\n",
    "\t\t\"\"\"\n",
    "\t\tsuper(Positional_Encoding_Layer, self).__init__()\n",
    "\t\tself.d_model = d_model\n",
    "\t\tself.dropout = dropout\n",
    "\t\tself.max_seq_len = max_seq_len\n",
    "\n",
    "\t\tself.dropout_layer = nn.Dropout(p=dropout)\n",
    "\t\tpe = torch.zeros(max_seq_len, d_model)                                                       # (max_seq_len, d_model)\n",
    "\t\tposition = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)                      # (max_seq_len, 1)\n",
    "\t\tdiv_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))     # (d_model/2)\n",
    "\t\tpe[:, 0::2] = torch.sin(position * div_term)\n",
    "\t\tpe[:, 1::2] = torch.cos(position * div_term) \n",
    "\t\tpe = pe.unsqueeze(0).transpose(0, 1)                                                         # (max_seq_len, 1, d_model)\n",
    "\t\tself.register_buffer('pe', pe)\n",
    "\n",
    "\tdef forward(self, inp):\n",
    "\t\tinp = inp + self.pe[:inp.size(0), :]                                                         # (n_step, batch_size, d_model)\n",
    "\t\treturn self.dropout_layer(inp)\n",
    "\n",
    "class Transformer_Encoder_Extraction_Layer(nn.Module):\n",
    "\t\"\"\"\n",
    "\tTransformer encoder as described in \"Attention is all you need\".\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, n_enc_layer, embed_size, n_head, intermediate_size, max_seq_len=512, dropout=0.1, **kwargs):\n",
    "\t\tsuper(Transformer_Encoder_Extraction_Layer, self).__init__(**kwargs)\n",
    "\t\tassert embed_size%n_head==0\n",
    "\n",
    "\t\tself.n_enc_layer = n_enc_layer\n",
    "\t\tself.embed_size = embed_size\n",
    "\t\tself.max_seq_len = max_seq_len\n",
    "\t\tself.n_head = n_head\n",
    "\t\tself.intermediate_size = intermediate_size\n",
    "\t\tself.dropout = dropout\n",
    "\n",
    "\t\tself.positional_encoder = Positional_Encoding_Layer(embed_size, max_seq_len=max_seq_len)\n",
    "\t\ttransformer_encoder_layer = TransformerEncoderLayer(embed_size, n_head, dim_feedforward=intermediate_size, dropout=dropout)\n",
    "\t\tself.transformer_encoder = TransformerEncoder(transformer_encoder_layer, n_enc_layer)\n",
    "\n",
    "\t\tself._init_weights()\n",
    "\n",
    "\tdef _init_weights(self):\n",
    "\t\tfor p in self.parameters():\n",
    "\t\t\tif p.dim() > 1:\n",
    "\t\t\t\txavier_uniform_(p)\n",
    "\n",
    "\tdef forward(self, inp, inp_padding_mask=None):\n",
    "\t\tinp = inp * np.sqrt(self.embed_size)                                           # (batch_size, n_step, embed_size)\n",
    "\t\tinp = self.positional_encoder(inp.permute(1, 0, 2))                            # (n_step, batch_size, embed_size)\n",
    "\t\tout = self.transformer_encoder(inp, src_key_padding_mask=inp_padding_mask)     # (n_step, batch_size, embed_size)\n",
    "\t\treturn out.permute(1, 0, 2)\n",
    "\n",
    "class MLP_Classification_Layer(nn.Module):\n",
    "\t\"\"\"\n",
    "\tMultilayer Perception Classification Layer\n",
    "\t- Layer 1: Linear + Batchnorm + ReLU + Dropout\n",
    "\t- Layer 2: Linear + Batchnorm + ReLU + Dropout\n",
    "\t- Layer 3: Linear\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, inp_size, out_size, dropout=0.4, **kwargs):\n",
    "\t\tsuper(MLP_Classification_Layer, self).__init__(**kwargs)\n",
    "\t\tself.inp_size = inp_size\n",
    "\t\tself.out_size = out_size\n",
    "\t\tself.dropout = dropout\n",
    "\t\t\n",
    "\t\tself.mlp_1 = nn.Linear(inp_size, 1024)\n",
    "\t\tself.batchnorm_1 = nn.BatchNorm1d(1024)\n",
    "\t\tself.mlp_dropout_1 = nn.Dropout(p=dropout)\n",
    "\t\tself.mlp_2 = nn.Linear(1024, 512)\n",
    "\t\tself.batchnorm_2 = nn.BatchNorm1d(512)\n",
    "\t\tself.mlp_dropout_2 = nn.Dropout(p=dropout)\n",
    "\t\tself.mlp_3 = nn.Linear(512, out_size)\n",
    "\t\n",
    "\tdef _init_weights(self):\n",
    "\t\tinitrange = 0.1\n",
    "\t\tself.mlp_1.weight.data.uniform_(-initrange, initrange)\n",
    "\t\tself.mlp_1.bias.data.zero_()\n",
    "\t\tself.mlp_2.weight.data.uniform_(-initrange, initrange)\n",
    "\t\tself.mlp_2.bias.data.zero_()\n",
    "\t\tself.mlp_3.weight.data.uniform_(-initrange, initrange)\n",
    "\t\tself.mlp_3.bias.data.zero_()\n",
    "\n",
    "\tdef forward(self, inp):\n",
    "\t\tmlp_out = self.mlp_1(inp)                                                         # (batch_size, 1024)\n",
    "\t\tmlp_out = self.mlp_dropout_1(F.relu(self.batchnorm_1(mlp_out)))                   # (batch_size, 1024)\n",
    "\t\tmlp_out = self.mlp_2(mlp_out)                                                     # (batch_size, 512)\n",
    "\t\tmlp_out = self.mlp_dropout_2(F.relu(self.batchnorm_2(mlp_out)))                   # (batch_size, 512)\n",
    "\t\tmlp_out = self.mlp_3(mlp_out)                                                     # (batch_size, out_size)\n",
    "\t\treturn mlp_out   \n",
    "\t\n",
    "class Transformer_Encoder_Classifier(nn.Module):\n",
    "\t\"\"\"\n",
    "\tTransformer Encoder + Multilayer Perception for Classification\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, embed_size, out_size, n_enc_layer, n_head, intermediate_size, device, transformer_dropout=0.1, mlp_dropout=0.4, **kwargs):\n",
    "\t\tsuper(Transformer_Encoder_Classifier, self).__init__(**kwargs)\n",
    "\t\t\n",
    "\t\tself.embed_size = embed_size\n",
    "\t\tself.out_size = out_size\n",
    "\t\tself.n_enc_layer = n_enc_layer\n",
    "\t\tself.n_head = n_head\n",
    "\t\tself.intermediate_size = intermediate_size\n",
    "\t\tself.device = device\n",
    "\t\tself.transformer_dropout = transformer_dropout\n",
    "\t\tself.mlp_dropout = mlp_dropout\n",
    "\n",
    "\t\tself.encoder_layer = Transformer_Encoder_Extraction_Layer(n_enc_layer, embed_size, n_head, intermediate_size, dropout=transformer_dropout)\n",
    "\t\tself.classification_layer = MLP_Classification_Layer(embed_size, out_size, dropout=mlp_dropout)\n",
    "\n",
    "\tdef get_padding_mask(self, batch_size, seq_len, inp_last_idx):\n",
    "\t\tpadding_mask = np.ones((batch_size, seq_len))\n",
    "\t\tfor index, last_idx in enumerate(inp_last_idx):\n",
    "\t\t\tpadding_mask[index,:last_idx+1] = 0\n",
    "\t\treturn torch.from_numpy(padding_mask).bool().to(self.device)\n",
    "\n",
    "\tdef forward(self, inp_embed, inp_last_idx):\n",
    "\t\tassert inp_embed.shape[0] == inp_last_idx.shape[0]\n",
    "\t\tbatch_size = inp_embed.shape[0]\n",
    "\t\tseq_len = inp_embed.shape[1]\n",
    "\t\tinp_padding_mask = self.get_padding_mask(batch_size, seq_len, inp_last_idx)\n",
    "\t\tout = self.encoder_layer(inp_embed, inp_padding_mask=inp_padding_mask)               # (batch_size, n_step, embed_size)\n",
    "\t\tpooled_buf = []\n",
    "\t\tfor index, last_idx in enumerate(inp_last_idx):\n",
    "\t\t\tpooled_buf.append(torch.max(out[index,:last_idx+1,:], dim=0)[0])\n",
    "\t\tout = torch.stack(pooled_buf)                                                        # (batch_size, embed_size)\n",
    "\t\tout = self.classification_layer(out)                                                 # (batch_size, out_size)\n",
    "\t\treturn out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer_Encoder_Classifier(256, 2, 6, 8, 2048, torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.random.standard_normal((100,6,256))\n",
    "inp = torch.from_numpy(np.concatenate([p, np.zeros((100, 4, 256))], axis=1)).float()\n",
    "inp_last_idx = np.array([5 for _ in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.4194e-01,  2.8054e-01],\n",
       "        [ 2.4362e-02,  8.0539e-01],\n",
       "        [-9.2631e-01,  2.7887e-01],\n",
       "        [-8.7398e-01, -2.1272e-01],\n",
       "        [-5.1911e-01,  6.4198e-01],\n",
       "        [ 3.7743e-01,  4.2349e-01],\n",
       "        [ 3.1638e-02,  1.1861e-01],\n",
       "        [-3.1806e-01,  2.7993e-01],\n",
       "        [ 8.0730e-01, -7.3080e-01],\n",
       "        [-5.1810e-01, -3.3092e-01],\n",
       "        [-1.1688e-02,  3.2414e-01],\n",
       "        [-1.6236e-01, -5.7130e-01],\n",
       "        [ 5.4187e-01, -7.0140e-01],\n",
       "        [-1.1773e+00, -8.6084e-02],\n",
       "        [-2.9260e-01,  1.8474e-01],\n",
       "        [ 5.6447e-01,  3.4316e-01],\n",
       "        [-1.6931e-01,  6.9761e-01],\n",
       "        [-5.2122e-01, -5.3940e-01],\n",
       "        [ 1.4284e+00, -3.9874e-02],\n",
       "        [-5.8163e-01, -1.2760e-01],\n",
       "        [ 1.0157e+00, -6.7378e-01],\n",
       "        [ 9.3978e-01, -4.7628e-01],\n",
       "        [ 2.1258e-01,  1.0124e+00],\n",
       "        [ 6.5808e-01, -3.0539e-02],\n",
       "        [ 3.1233e-01,  4.3065e-01],\n",
       "        [ 7.2707e-01,  1.5000e-01],\n",
       "        [ 4.3200e-01, -3.0156e-01],\n",
       "        [ 2.6709e-01,  6.5453e-02],\n",
       "        [-1.7455e-01,  1.9606e-01],\n",
       "        [ 1.1335e-01,  3.8149e-02],\n",
       "        [-1.1850e+00, -3.4550e-01],\n",
       "        [-7.2001e-01,  2.8458e-01],\n",
       "        [-3.2293e-01,  9.9903e-02],\n",
       "        [ 8.1855e-01,  3.1094e-01],\n",
       "        [ 7.2461e-01, -2.7705e-01],\n",
       "        [-4.2282e-01, -7.4102e-01],\n",
       "        [ 5.0382e-02,  5.0233e-01],\n",
       "        [-1.0803e+00,  6.2855e-01],\n",
       "        [-1.3559e-01, -4.2934e-01],\n",
       "        [ 7.8882e-01,  2.2693e-01],\n",
       "        [-7.3979e-02,  6.9879e-01],\n",
       "        [-1.4144e-01,  2.0818e-01],\n",
       "        [-1.4939e-01,  4.3142e-01],\n",
       "        [ 8.5547e-01, -8.2508e-01],\n",
       "        [-8.9211e-01, -4.4892e-01],\n",
       "        [-6.1936e-01, -2.0382e-03],\n",
       "        [-5.2739e-01,  6.2501e-01],\n",
       "        [-3.9158e-01,  1.5002e+00],\n",
       "        [-3.9382e-01,  7.5856e-01],\n",
       "        [ 4.3900e-01, -9.9002e-01],\n",
       "        [-3.3491e-01, -7.2632e-01],\n",
       "        [-4.9083e-02,  3.3897e-01],\n",
       "        [-4.2101e-01, -3.9805e-02],\n",
       "        [ 8.6413e-01,  5.7303e-01],\n",
       "        [-7.4531e-01, -1.6361e-01],\n",
       "        [-2.5259e-01,  2.7169e-01],\n",
       "        [ 3.9171e-01,  3.6578e-01],\n",
       "        [ 1.1217e-02, -3.1801e-01],\n",
       "        [-3.5138e-01, -5.6338e-01],\n",
       "        [-2.1765e-01, -1.2327e-01],\n",
       "        [-2.7186e-01,  3.7519e-01],\n",
       "        [ 1.1725e-01, -7.6917e-01],\n",
       "        [-1.8453e-01, -1.1196e-02],\n",
       "        [-1.5025e-01, -1.0848e-01],\n",
       "        [ 2.7590e-02, -4.8879e-01],\n",
       "        [-3.1410e-01, -1.7657e-01],\n",
       "        [-4.3743e-01,  5.9888e-01],\n",
       "        [-1.5965e-01, -6.8947e-02],\n",
       "        [ 1.8249e-01,  2.5539e-01],\n",
       "        [ 2.5776e-01,  1.6117e-01],\n",
       "        [-6.0152e-01, -2.4985e-01],\n",
       "        [ 6.4154e-04, -7.3043e-01],\n",
       "        [-7.4518e-01, -3.8710e-01],\n",
       "        [ 6.0488e-01, -8.1537e-02],\n",
       "        [ 2.5057e-01,  7.7861e-01],\n",
       "        [ 1.8404e-01, -2.7101e-01],\n",
       "        [-4.1052e-01,  4.9542e-01],\n",
       "        [ 5.8092e-01,  1.2579e+00],\n",
       "        [ 2.3286e-01, -1.0135e+00],\n",
       "        [ 9.3409e-01,  1.8422e-01],\n",
       "        [-2.9442e-01,  2.4716e-01],\n",
       "        [-5.1138e-01,  2.8391e-01],\n",
       "        [ 2.4200e-01, -4.0748e-01],\n",
       "        [ 1.2815e+00,  9.8498e-03],\n",
       "        [-2.8333e-01,  3.9535e-02],\n",
       "        [-6.5507e-01,  4.3177e-01],\n",
       "        [-5.9795e-01, -1.5498e-01],\n",
       "        [ 3.9184e-01,  2.3106e-01],\n",
       "        [-3.9720e-01,  1.1532e-01],\n",
       "        [-1.8655e-01,  1.8814e-01],\n",
       "        [-1.7116e-01,  7.8538e-01],\n",
       "        [ 3.5339e-01, -1.3229e-01],\n",
       "        [-9.9309e-01, -8.8625e-01],\n",
       "        [ 3.4673e-01,  2.2003e-02],\n",
       "        [ 4.0951e-01, -2.7282e-01],\n",
       "        [-3.4663e-01, -1.3855e+00],\n",
       "        [-3.3456e-01,  1.7942e-01],\n",
       "        [-5.7132e-01, -3.9619e-02],\n",
       "        [ 4.4180e-01,  4.8553e-01],\n",
       "        [ 5.7512e-02,  7.0541e-01]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(inp, inp_last_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ones(1,10).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(1,10).squeeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 20])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([torch.mean(torch.ones(1,10,20), dim=1).squeeze(0), torch.mean(torch.ones(1,10,20), dim=1).squeeze(0)]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0, 10, dtype=torch.float).unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 13\n",
    "torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class Positional_Encoding_Layer(nn.Module):\n",
    "\t\"\"\"\n",
    "\tPositional encoding using sine and cosine as described in \"Attention is all you need\".\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, d_model, max_seq_len=128, dropout=0.1):\n",
    "\t\t\"\"\"\n",
    "\t\tFormula:\n",
    "\t\t| PE(pos,2i) = sin(pos/10000**(2*i/d_model))\n",
    "\t\t| PE(pos,2i+1) = cos(pos/10000**(2*i/d_model))\n",
    "\t\t\"\"\"\n",
    "\t\tsuper(Positional_Encoding_Layer, self).__init__()\n",
    "\t\tself.d_model = d_model\n",
    "\t\tself.dropout = dropout\n",
    "\t\tself.max_seq_len = max_seq_len\n",
    "\n",
    "\t\tself.dropout_layer = nn.Dropout(p=dropout)\n",
    "\t\tpe = torch.zeros(max_seq_len, d_model)                                                       # (max_seq_len, d_model)\n",
    "\t\tposition = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)                      # (max_seq_len, 1)\n",
    "\t\tdiv_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))     # (d_model/2)\n",
    "\t\tpe[:, 0::2] = torch.sin(position * div_term)\n",
    "\t\tpe[:, 1::2] = torch.cos(position * div_term) \n",
    "\t\tpe = pe.unsqueeze(0).transpose(0, 1)                                                         # (max_seq_len, 1, d_model)\n",
    "\t\tself.register_buffer('pe', pe)\n",
    "\n",
    "\tdef forward(self, inp):\n",
    "\t\tinp = inp + self.pe[:inp.size(0), :]                                                         # (n_step, batch_size, d_model)\n",
    "\t\treturn self.dropout_layer(inp)\n",
    "\n",
    "class Transformer_Encoder_Extraction_Layer(nn.Module):\n",
    "\t\"\"\"\n",
    "\tTransformer encoder as described in \"Attention is all you need\".\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, n_enc_layer, embed_size, n_head, intermediate_size, max_seq_len=512, dropout=0.1, **kwargs):\n",
    "\t\tsuper(Transformer_Encoder_Extraction_Layer, self).__init__(**kwargs)\n",
    "\t\tassert embed_size%n_head==0\n",
    "\n",
    "\t\tself.n_enc_layer = n_enc_layer\n",
    "\t\tself.embed_size = embed_size\n",
    "\t\tself.max_seq_len = max_seq_len\n",
    "\t\tself.n_head = n_head\n",
    "\t\tself.intermediate_size = intermediate_size\n",
    "\t\tself.dropout = dropout\n",
    "\n",
    "\t\tself.positional_encoder = Positional_Encoding_Layer(embed_size, max_seq_len=max_seq_len)\n",
    "\t\ttransformer_encoder_layer = TransformerEncoderLayer(embed_size, n_head, dim_feedforward=intermediate_size, dropout=dropout)\n",
    "\t\tself.transformer_encoder = TransformerEncoder(transformer_encoder_layer, n_enc_layer)\n",
    "\n",
    "\t\tself._init_weights()\n",
    "\n",
    "\tdef _init_weights(self):\n",
    "\t\tfor p in self.parameters():\n",
    "\t\t\tif p.dim() > 1:\n",
    "\t\t\t\txavier_uniform_(p)\n",
    "\n",
    "\tdef forward(self, inp, inp_padding_mask=None):\n",
    "\t\tinp = inp * np.sqrt(self.embed_size)                                           # (batch_size, n_step, embed_size)\n",
    "\t\tinp = self.positional_encoder(inp.permute(1, 0, 2))                            # (n_step, batch_size, embed_size)\n",
    "\t\tout = self.transformer_encoder(inp, src_key_padding_mask=inp_padding_mask)     # (n_step, batch_size, embed_size)\n",
    "\t\treturn out.permute(1, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_torch_module_num_of_parameter(model):\n",
    "\t\"\"\"\n",
    "\tGet # of parameters in a torch module.\n",
    "\t\"\"\"\n",
    "\tmodel_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "\tparams = sum([np.prod(p.size()) for p in model_parameters])\n",
    "\treturn params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7890432"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer_Encoder_Extraction_Layer(6, 256, 8, 2048)\n",
    "get_torch_module_num_of_parameter(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.random.standard_normal((100,6,256))\n",
    "inp_1 = torch.from_numpy(np.concatenate([p, np.zeros((100, 4, 256))], axis=1)).float()\n",
    "inp_2 = torch.from_numpy(np.concatenate([p, np.ones((100, 4, 256))], axis=1)).float()\n",
    "pad_mask = torch.from_numpy(np.concatenate([np.zeros((100, 6)), np.ones((100,4))], axis=1)).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padding_mask(batch_size, seq_len, inp_last_idx):\n",
    "    padding_mask = np.ones((batch_size, seq_len))\n",
    "    for index, last_idx in enumerate(inp_last_idx):\n",
    "        padding_mask[index,:last_idx+1] = 0\n",
    "    return torch.from_numpy(padding_mask).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False,  True,  True]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_padding_mask(3,10,[5,6,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "out_1 = model(inp_1, inp_padding_mask=pad_mask)\n",
    "out_2 = model(inp_2, inp_padding_mask=pad_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = BertConfig(vocab_size=1, hidden_size=4, num_hidden_layers=8, num_attention_heads=2, intermediate_size=2048)\n",
    "model = BertModel(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.from_numpy(np.concatenate([np.random.standard_normal((1, 5, 4)), np.zeros((1, 5, 4))], axis=1)).float()\n",
    "mask = torch.from_numpy(np.concatenate([np.zeros((1, 5)), np.zeros((1, 5))], axis=1)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4829,  0.3867,  1.2085,  1.0879],\n",
       "         [-0.4869, -2.1700,  0.7577, -0.3682],\n",
       "         [-1.4087, -0.4176, -0.6754,  0.0763],\n",
       "         [-0.1583, -0.2029,  0.6745, -0.6823],\n",
       "         [ 0.4552,  0.5747, -0.3314, -1.9143],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000]]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "out= model(inputs_embeds=inp, attention_mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 4])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.5516, -0.2037,  0.9508,  0.8045],\n",
       "         [ 0.1553, -1.5379,  1.2636,  0.1190],\n",
       "         [-0.6950,  0.0802, -0.9813,  1.5961],\n",
       "         [-0.0377, -0.2082,  1.5238, -1.2779],\n",
       "         [ 0.7606,  0.8213,  0.0738, -1.6557],\n",
       "         [ 0.2063,  0.3378,  1.0872, -1.6313],\n",
       "         [ 0.4727,  0.4431,  0.7993, -1.7151],\n",
       "         [ 0.6379,  0.8671,  0.1718, -1.6768],\n",
       "         [ 0.5959,  0.3670,  0.7528, -1.7157],\n",
       "         [ 1.2472,  0.6829, -0.7249, -1.2052]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with torch padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AlbertModel, AlbertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = AlbertConfig()\n",
    "model = AlbertModel(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222595584"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_torch_module_num_of_parameter(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.from_numpy(np.concatenate([np.ones((10, 5, 128)), np.zeros((10, 5, 128))], axis=1)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "out= model(inputs_embeds=inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10, 4096])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.0415586 , -0.700448  , -0.23963146, ..., -0.34111258,\n",
       "        -0.88970816,  2.0453863 ],\n",
       "       [-1.0419453 , -0.7001726 , -0.24012905, ..., -0.34073353,\n",
       "        -0.8900057 ,  2.0456576 ],\n",
       "       [-1.0417546 , -0.7000878 , -0.24050954, ..., -0.34136513,\n",
       "        -0.8902673 ,  2.0448096 ],\n",
       "       ...,\n",
       "       [-1.0415502 , -0.70019853, -0.23981182, ..., -0.34107703,\n",
       "        -0.8901159 ,  2.0457187 ],\n",
       "       [-1.0413502 , -0.6995464 , -0.240383  , ..., -0.34042382,\n",
       "        -0.8905616 ,  2.0456984 ],\n",
       "       [-1.0413843 , -0.70032   , -0.23944165, ..., -0.34181798,\n",
       "        -0.89004594,  2.0447192 ]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0][1].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
