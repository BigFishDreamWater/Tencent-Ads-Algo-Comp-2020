{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "train_path = '../../raw_train_artifact'\n",
    "test_path = '../../raw_test_artifact'\n",
    "embedding_path = '../../embedding_artifact'\n",
    "input_path = '../../input_artifact'\n",
    "input_split_path = '../../input_artifact/input_split'\n",
    "model_path = '../../model_artifact'\n",
    "output_path = '../../output_artifact'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gc\n",
    "gc.enable()\n",
    "import time\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns',120)\n",
    "pd.set_option('display.max_rows',2000)\n",
    "pd.set_option('precision',5)\n",
    "pd.set_option('float_format', '{:.5f}'.format)\n",
    "\n",
    "import tqdm\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from gensim.models import Word2Vec\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:13:22 INFO: Restart notebook\n",
      "==========================\n",
      "Tue Jun  9 17:13:22 2020\n",
      "==========================\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "log_path = '[1.1]Hugging Face Transformer Experiment.log'\n",
    "    \n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)-s: %(message)s', datefmt='%H:%M:%S')\n",
    "\n",
    "fh = logging.FileHandler(log_path)\n",
    "fh.setLevel(logging.INFO)\n",
    "fh.setFormatter(formatter)\n",
    "logger.addHandler(fh)\n",
    "\n",
    "sh = logging.StreamHandler(sys.stdout)\n",
    "sh.setLevel(logging.INFO)\n",
    "sh.setFormatter(formatter)\n",
    "logger.addHandler(sh)\n",
    "\n",
    "logger.info(f'Restart notebook\\n==========================\\n{time.ctime()}\\n==========================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# logger.info('Device in Use: {}'.format(DEVICE))\n",
    "# torch.cuda.empty_cache()\n",
    "# t = torch.cuda.get_device_properties(DEVICE).total_memory/1024**3\n",
    "# c = torch.cuda.memory_cached(DEVICE)/1024**3\n",
    "# a = torch.cuda.memory_allocated(DEVICE)/1024**3\n",
    "# logger.info('CUDA Memory: Total {:.2f} GB, Cached {:.2f} GB, Allocated {:.2f} GB'.format(t,c,a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_torch_module_num_of_parameter(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM EXP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_1 = nn.LSTM(input_size=2, hidden_size=2, bidirectional=True, batch_first=True)\n",
    "lstm_2 = nn.LSTM(input_size=4, hidden_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [torch.tensor([[1,1],[2,2],[3,3]]).float(), torch.tensor([[1,1],[2,2]]).float()]\n",
    "b = torch.nn.utils.rnn.pad_sequence(a, batch_first=True)\n",
    "inp = torch.nn.utils.rnn.pack_padded_sequence(b, batch_first=True, lengths=[3,2], enforce_sorted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, (h, c) = lstm_1(inp)\n",
    "\n",
    "#nx, _ = lstm_2(out, (h,c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.view(1, 2, 2, 2)[:,1,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7098,  0.6678],\n",
       "         [-0.4610,  0.5054]],\n",
       "\n",
       "        [[ 0.2380,  0.2383],\n",
       "         [ 0.2325,  0.2292]]], grad_fn=<IndexSelectBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7098,  0.6678,  0.2380,  0.2383],\n",
       "        [-0.4610,  0.5054,  0.2325,  0.2292]], grad_fn=<UnsafeViewBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.permute(1,0,2).reshape(2,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0232,  0.8452,  0.7446,  0.5546],\n",
       "        [-0.5838,  0.6243,  0.7042,  0.5381]], grad_fn=<UnsafeViewBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.permute(1,0,2).reshape(2,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1469,  0.2804,  0.2380,  0.2383],\n",
       "         [-0.4610,  0.5054,  0.2307,  0.2592],\n",
       "         [-0.7098,  0.6678,  0.2113,  0.2379]],\n",
       "\n",
       "        [[-0.1469,  0.2804,  0.2325,  0.2292],\n",
       "         [-0.4610,  0.5054,  0.2089,  0.2109],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000]]], grad_fn=<IndexSelectBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.utils.rnn.pad_packed_sequence(out, batch_first=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0913, -0.1721],\n",
       "         [ 0.1276, -0.2379],\n",
       "         [ 0.1945, -0.2649]],\n",
       "\n",
       "        [[-0.0187, -0.1936],\n",
       "         [ 0.1482, -0.2489],\n",
       "         [ 0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0867, -0.2293],\n",
       "         [ 0.1775, -0.2689],\n",
       "         [ 0.1964, -0.3072]]], grad_fn=<IndexSelectBackward>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.utils.rnn.pad_packed_sequence(nx, batch_first=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Seq GNMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Res_LSTM_Layer(nn.Module):\n",
    "\t\"\"\"\n",
    "\tMulti-layer unidirectional LSTM with residual connection.\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, n_layer, hidden_size, dropout=0.1, **kwargs):\n",
    "\t\tsuper(Res_LSTM_Layer, self).__init__(**kwargs)\n",
    "\t\tself.n_layer = n_layer\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.dropout = dropout\n",
    "\n",
    "\t\tfor index in range(n_layer):\n",
    "\t\t\tsetattr(self, 'lstm_{}'.format(index), nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, batch_first=True, bias=True))\n",
    "\t\t\tsetattr(self, 'dropout_{}'.format(index), nn.Dropout(p=dropout))\n",
    "\n",
    "\tdef forward(self, inp, inp_len):\n",
    "\t\t_, total_length, _ = inp.shape\n",
    "\t\tfor index in range(self.n_layer):\n",
    "\t\t\tout = nn.utils.rnn.pack_padded_sequence(inp, batch_first=True, lengths=inp_len, enforce_sorted=False)\n",
    "\t\t\tout, _ = getattr(self, 'lstm_{}'.format(index))(out)\n",
    "\t\t\tout = nn.utils.rnn.pad_packed_sequence(out, batch_first=True, total_length=total_length)[0]\n",
    "\t\t\tinp = getattr(self, 'dropout_{}'.format(index))(torch.add(out, inp))\n",
    "\t\treturn inp\n",
    "\n",
    "class GNMT_Encoder_Layer(nn.Module):\n",
    "\t\"\"\"\n",
    "\tGoogle Neural Machine Translation - Encoder\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, input_size, n_layer, hidden_size, dropout=0.1, **kwargs):\n",
    "\t\tsuper(GNMT_Encoder_Layer, self).__init__(**kwargs)\n",
    "\t\tassert n_layer >= 3\n",
    "\n",
    "\t\tself.input_size = input_size\n",
    "\t\tself.n_layer = n_layer\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.dropout = dropout\n",
    "\n",
    "\t\tself.l1_bilstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True, bias=True, bidirectional=True)\n",
    "\t\tself.l1_dropout = nn.Dropout(p=dropout)\n",
    "\t\tself.l2_lstm = nn.LSTM(input_size=hidden_size*2, hidden_size=hidden_size, bias=True)\n",
    "\t\tself.l2_dropout = nn.Dropout(p=dropout)\n",
    "\t\tself.res_lstm = Res_LSTM_Layer(n_layer-2, hidden_size, dropout=dropout)\n",
    "\n",
    "\tdef forward(self, inp, inp_len):\n",
    "\t\tbatch_size, total_length, _ = inp.shape\n",
    "\t\tinp = nn.utils.rnn.pack_padded_sequence(inp, batch_first=True, lengths=inp_len, enforce_sorted=False)\n",
    "\t\tout, (h, c) = self.l1_bilstm(inp)\n",
    "\t\tbackward_hidden_state = h.view(1, 2, batch_size, self.hidden_size)[:,1,:,:].squeeze(0)              # (num_direction, batch_size, enc_hidden_size)\n",
    "\t\tbackward_cell_state = c.view(1, 2, batch_size, self.hidden_size)[:,1,:,:].squeeze(0)                # (num_direction, batch_size, enc_hidden_size)\n",
    "\t\tout = self.l1_dropout(nn.utils.rnn.pad_packed_sequence(out, batch_first=True, total_length=total_length)[0])\n",
    "\t\tout = nn.utils.rnn.pack_padded_sequence(out, batch_first=True, lengths=inp_len, enforce_sorted=False)\n",
    "\t\tout, _ = self.l2_lstm(out)\n",
    "\t\tout = self.l2_dropout(nn.utils.rnn.pad_packed_sequence(out, batch_first=True, total_length=total_length)[0])\n",
    "\t\tout = self.res_lstm(out, inp_len)\n",
    "\t\treturn out, backward_hidden_state, backward_cell_state\n",
    "\n",
    "class Additive_Attention_Layer(nn.Module):\n",
    "\t\"\"\"\n",
    "\tAdditive attention used in GNMT\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, hidden_size, **kwargs):\n",
    "\t\tsuper(Additive_Attention_Layer, self).__init__(**kwargs)\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\n",
    "\t\tself.W = nn.Linear(hidden_size*2, hidden_size)\n",
    "\t\tself.tanh = nn.Tanh()\n",
    "\t\tself.V = nn.Parameter(torch.Tensor(1, hidden_size))\n",
    "\t\tself.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "\t\tnn.init.normal_(self.V, 0, 0.1)\n",
    "\n",
    "\tdef forward(self, query, values, mask):\n",
    "\t\t\"\"\"\n",
    "\t\t: query:  (batch_size, hidden_size)\n",
    "\t\t: values: (batch_size, seq_len, hidden_size)\n",
    "\t\t: mask:   (batch_size, seq_len)\n",
    "\t\t\"\"\"\n",
    "\t\tbatch_size, seq_len, hidden_size = values.shape\n",
    "\n",
    "\t\tquery = query.unsqueeze(1).expand(-1, seq_len, -1)\n",
    "\t\tscore = self.tanh(self.W(torch.cat((query, values), dim=2)))                              # (batch_size, seq_len, hidden_size)\n",
    "\t\tscore = torch.bmm(self.V.squeeze(1).expand(batch_size, -1, -1), score.permute(0,2,1))     # (batch_size, 1, seq_len)\n",
    "\t\tscore = self.softmax(torch.add(score, mask.unsqueeze(1)))                                 # (batch_size, 1, seq_len)\n",
    "\t\tcontext = torch.bmm(score, values).squeeze(1)                                             # (batch_size, hidden_size)\n",
    "\n",
    "\t\treturn context\n",
    "\n",
    "class Res_Attn_LSTM_Layer(nn.Module):\n",
    "\t\"\"\"\n",
    "\tMulti-layer unidirectional LSTM with residual connection and attention.\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, n_layer, hidden_size, dropout=0.1, **kwargs):\n",
    "\t\tsuper(Res_Attn_LSTM_Layer, self).__init__(**kwargs)\n",
    "\t\tself.n_layer = n_layer\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.dropout = dropout\n",
    "\n",
    "\t\tfor index in range(n_layer):\n",
    "\t\t\tsetattr(self, 'lstm_{}'.format(index), nn.LSTM(input_size=2*hidden_size, hidden_size=hidden_size, batch_first=True, bias=True))\n",
    "\t\t\tsetattr(self, 'dropout_{}'.format(index), nn.Dropout(p=dropout))\n",
    "\n",
    "\tdef forward(self, hidden_states, context_vectors, inp_len):\n",
    "\t\t_, total_length, _ = hidden_states.shape\n",
    "\t\tfor index in range(self.n_layer):\n",
    "\t\t\tout = nn.utils.rnn.pack_padded_sequence(torch.cat((hidden_states, context_vectors), dim=2), batch_first=True, lengths=inp_len, enforce_sorted=False)\n",
    "\t\t\tout, _ = getattr(self, 'lstm_{}'.format(index))(out)\n",
    "\t\t\tout = nn.utils.rnn.pad_packed_sequence(out, batch_first=True, total_length=total_length)[0]\n",
    "\t\t\thidden_states = getattr(self, 'dropout_{}'.format(index))(torch.add(out, hidden_states))\n",
    "\t\treturn hidden_states\n",
    "\n",
    "class GNMT_Decoder_Layer(nn.Module):\n",
    "\t\"\"\"\n",
    "\tGoogle Neural Machine Translation - Decoder\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, n_layer, hidden_size, dropout=0.1, device=None, **kwargs):\n",
    "\t\tsuper(GNMT_Decoder_Layer, self).__init__(**kwargs)\n",
    "\t\tassert n_layer>=3\n",
    "\n",
    "\t\tself.n_layer = n_layer\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.dropout = dropout\n",
    "\t\tself.device = device if device else torch.device('cpu')\n",
    "\n",
    "\t\tself.attention_calc = Additive_Attention_Layer(hidden_size)\n",
    "\t\tself.l1_lstm_cell = nn.LSTMCell(input_size=2*hidden_size, hidden_size=hidden_size, bias=True)\n",
    "\t\tself.l1_dropout = nn.Dropout(p=dropout)\n",
    "\t\tself.l2_lstm = nn.LSTM(input_size=2*hidden_size, hidden_size=hidden_size, batch_first=True, bias=True)\n",
    "\t\tself.l2_dropout = nn.Dropout(p=dropout)\n",
    "\t\tself.res_attn_lstm = Res_Attn_LSTM_Layer(n_layer-2, hidden_size, dropout=dropout)\n",
    "\n",
    "\tdef get_attention_mask(self, inp_len, batch_size, seq_len):\n",
    "\t\tmask = np.ones((batch_size, seq_len))\n",
    "\t\tfor index, l in enumerate(inp_len):\n",
    "\t\t\tmask[index,:l] = 0\n",
    "\t\tmask *= -1e9\n",
    "\t\treturn torch.from_numpy(mask).float().to(self.device)\n",
    "\n",
    "\tdef forward(self, enc_hidden_states, backward_hidden_state, backward_cell_state, inp_len):\n",
    "\t\tbatch_size, seq_len, _ = enc_hidden_states.shape\n",
    "\t\tattention_mask = self.get_attention_mask(inp_len, batch_size, seq_len)\n",
    "\t\tenc_hidden_states = enc_hidden_states.permute(1,0,2)                                                                                          # (seq_len, batch_size, hidden_size)\n",
    "\t\tdecoder_hidden_states_buf =  []\n",
    "\t\tdecoder_context_vectors_buf = []\n",
    "\t\tdecoder_h, decoder_c = backward_hidden_state, backward_cell_state\n",
    "\t\tfor step in range(seq_len):\n",
    "\t\t\tinp = enc_hidden_states[step]                        \n",
    "\t\t\tcontext_vector = self.attention_calc(inp, enc_hidden_states.permute(1,0,2), attention_mask)                                               # (batch_size, hidden_size)\n",
    "\t\t\tdecoder_context_vectors_buf.append(context_vector)\n",
    "\t\t\tinp = torch.cat((inp, context_vector), dim=1)                                                                                                    # (batch_size, 2*hidden_size)\n",
    "\t\t\tdecoder_h, decoder_c = self.l1_lstm_cell(inp, (decoder_c, decoder_h))\n",
    "\t\t\tdecoder_hidden_states_buf.append(decoder_h)\n",
    "\t\tdecoder_context_vectors = torch.stack(decoder_context_vectors_buf, dim=1)                                                                     # (batch_size, seq_len, hidden_size)\n",
    "\t\tdecoder_hidden_states = torch.stack(decoder_hidden_states_buf, dim=1)                                                                         # (batch_size, seq_len, hidden_size)\n",
    "\t\tdecoder_hidden_states = self.l1_dropout(torch.cat((decoder_hidden_states, decoder_context_vectors), dim=2))                                   # (batch_size, seq_len, 2*hidden_size)\n",
    "\t\tdecoder_hidden_states = nn.utils.rnn.pack_padded_sequence(decoder_hidden_states, batch_first=True, lengths=inp_len, enforce_sorted=False)\n",
    "\t\tdecoder_hidden_states, _ = self.l2_lstm(decoder_hidden_states)\n",
    "\t\tdecoder_hidden_states = nn.utils.rnn.pad_packed_sequence(decoder_hidden_states, batch_first=True, total_length=seq_len)[0]\n",
    "\t\tdecoder_hidden_states = self.l2_dropout(decoder_hidden_states)\n",
    "\t\tdecoder_hidden_states = self.res_attn_lstm(decoder_hidden_states, decoder_context_vectors, inp_len)                                                    # (batch_size, seq_len, hidden_size)\n",
    "\t\treturn decoder_hidden_states\n",
    "\n",
    "class GNMT_Extraction_Layer(nn.Module):\n",
    "\t\"\"\"\n",
    "\tSeq2Seq feature extration layer based on Google Neural Machine Translation.\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, embed_size, hidden_size, n_enc_layer, n_dec_layer, device=None, dropout=0.1, **kwargs):\n",
    "\t\tsuper(GNMT_Extraction_Layer, self).__init__(**kwargs)\n",
    "\t\tself.embed_size = embed_size\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.n_enc_layer = n_enc_layer\n",
    "\t\tself.n_dec_layer = n_dec_layer\n",
    "\t\tself.device = device if device else torch.device('cpu')\n",
    "\t\tself.dropout = dropout\n",
    "\n",
    "\t\tself.encoder = GNMT_Encoder_Layer(embed_size, n_enc_layer, hidden_size)\n",
    "\t\tself.decoder = GNMT_Decoder_Layer(n_dec_layer, hidden_size, device=self.device)\n",
    "\n",
    "\tdef forward(self, inp, inp_len):\n",
    "\t\tencoder_hidden_states, backward_hidden_state, backward_cell_state = self.encoder(inp, inp_len)\n",
    "\t\tdecoder_hidden_states = self.decoder(encoder_hidden_states, backward_hidden_state, backward_cell_state, inp_len)\n",
    "\t\treturn decoder_hidden_states\n",
    "\n",
    "class MLP_Classification_Layer(nn.Module):\n",
    "\t\"\"\"\n",
    "\tMultilayer Perception Classification Layer\n",
    "\t- Layer 1: Linear + Batchnorm + ReLU + Dropout\n",
    "\t- Layer 2: Linear + Batchnorm + ReLU + Dropout\n",
    "\t- Layer 3: Linear\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, inp_size, out_size, dropout=0.4, **kwargs):\n",
    "\t\tsuper(MLP_Classification_Layer, self).__init__(**kwargs)\n",
    "\t\tself.inp_size = inp_size\n",
    "\t\tself.out_size = out_size\n",
    "\t\tself.dropout = dropout\n",
    "\t\t\n",
    "\t\tself.mlp_1 = nn.Linear(inp_size, 2048)\n",
    "\t\tself.batchnorm_1 = nn.BatchNorm1d(2048)\n",
    "\t\tself.mlp_dropout_1 = nn.Dropout(p=dropout)\n",
    "\t\tself.mlp_2 = nn.Linear(2048, 1024)\n",
    "\t\tself.batchnorm_2 = nn.BatchNorm1d(1024)\n",
    "\t\tself.mlp_dropout_2 = nn.Dropout(p=dropout)\n",
    "\t\tself.mlp_3 = nn.Linear(1024, out_size)\n",
    "\t\t\n",
    "\tdef forward(self, inp):\n",
    "\t\tmlp_out = self.mlp_1(inp)                                                         # (batch_size, 1024)\n",
    "\t\tmlp_out = self.mlp_dropout_1(F.relu(self.batchnorm_1(mlp_out)))                   # (batch_size, 1024)\n",
    "\t\tmlp_out = self.mlp_2(mlp_out)                                                     # (batch_size, 512)\n",
    "\t\tmlp_out = self.mlp_dropout_2(F.relu(self.batchnorm_2(mlp_out)))                   # (batch_size, 512)\n",
    "\t\tmlp_out = self.mlp_3(mlp_out)                                                     # (batch_size, out_size)\n",
    "\t\treturn mlp_out   \n",
    "\n",
    "class Multi_Seq_GNMT_Classifier(nn.Module):\n",
    "\t\"\"\"\n",
    "\tUse GNMT for Seq2Seq feature extraction, apply max pooling & pick last state, then use multilayer perception for classification\n",
    "\t- Multi sequence input version\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, out_size, embed_size, hidden_size, n_enc_layer, n_dec_layer, device=None, rnn_dropout=0.1, dnn_dropout=0.4, **kwargs):\n",
    "\t\tsuper(Multi_Seq_GNMT_Classifier, self).__init__()\n",
    "\t\tassert isinstance(embed_size, list) and isinstance(hidden_size, list) and len(embed_size)==len(hidden_size)\n",
    "\t\tself.out_size = out_size\n",
    "\t\tself.embed_size = embed_size\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.n_enc_layer = n_enc_layer\n",
    "\t\tself.n_dec_layer = n_dec_layer\n",
    "\t\tself.device = device if device else torch.device('cpu')\n",
    "\t\tself.rnn_dropout = rnn_dropout\n",
    "\t\tself.dnn_dropout = dnn_dropout\n",
    "\n",
    "\t\tself.n_extraction = len(embed_size)\n",
    "\t\tself.mlp_inp_size = sum(map(lambda x:2*x, hidden_size))\n",
    "\n",
    "\t\tfor index, (e_size, h_size) in enumerate(zip(embed_size, hidden_size)):\n",
    "\t\t\tsetattr(self, 'GNMT_layer_{}'.format(index), GNMT_Extraction_Layer(e_size, h_size, n_enc_layer, n_dec_layer, device=device, dropout=rnn_dropout))\n",
    "\t\tself.mlp_layer = MLP_Classification_Layer(self.mlp_inp_size, out_size, dropout=dnn_dropout)\n",
    "\n",
    "\tdef forward(self, *args):\n",
    "\t\tassert len(args)==self.n_extraction+1\n",
    "\t\tout_buf, inp_len = [], args[-1]\n",
    "\t\tfor index, inp in enumerate(args[:-1]):\n",
    "\t\t\tinp = getattr(self, 'GNMT_layer_{}'.format(index))(inp, inp_len)  \n",
    "\t\t\tmax_pool_buf, last_buf = [], []\n",
    "\t\t\tfor batch_idx, l in enumerate(inp_len):\n",
    "\t\t\t\tmax_pool_buf.append(torch.max(inp[batch_idx,:l], dim=0)[0])\n",
    "\t\t\t\tlast_buf.append(inp[batch_idx, l-1])\n",
    "\t\t\tout_buf.append(torch.cat((torch.stack(max_pool_buf, dim=0), torch.stack(last_buf, dim=0)), dim=1)) \n",
    "\t\tout = torch.cat(out_buf, dim=1)\n",
    "\t\tout = self.mlp_layer(out)                                                     \n",
    "\t\treturn out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Multi_Seq_GNMT_Classifier(10,[256,256,128,128],[256,256,256,256],8,8)\n",
    "inp_1 = torch.ones(5, 10, 256).float()\n",
    "inp_2 = torch.ones(5, 10, 128).float()\n",
    "out = model(*[inp_1, inp_1, inp_2, inp_2, np.arange(1,6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51541002"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_torch_module_num_of_parameter(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Attention_Extraction_Layer(nn.Module):\n",
    "\t\"\"\"\n",
    "\tLSTM attention feature extration layer, idea from Google Neural Machine Translation\n",
    "\t- Layer 1: BiLSTM + Dropout + Layernorm\n",
    "\t- Layer 2: LSTM with Residual Connection + Dropout + Layernorm\n",
    "\t- Layer 3: LSTM + Batchnorm + ReLU + Dropout\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, embed_size, enc_hidden_size, dec_hidden_size, rnn_dropout=0.1, dnn_dropout=0.4, **kwargs):\n",
    "\t\tsuper(LSTM_Attention_Extraction_Layer, self).__init__(**kwargs)\n",
    "\t\tself.embed_size = embed_size\n",
    "\t\tself.enc_hidden_size = enc_hidden_size\n",
    "\t\tself.dec_hidden_size = dec_hidden_size\n",
    "\t\tself.rnn_dropout = rnn_dropout\n",
    "\t\tself.dnn_dropout = dnn_dropout\n",
    "\n",
    "\t\tself.enc_bilstm_1 = nn.LSTM(input_size=embed_size, hidden_size=enc_hidden_size, batch_first=True, bias=True, bidirectional=True)\n",
    "\t\tself.rnn_dropout_1 = nn.Dropout(p=rnn_dropout)\n",
    "\t\tself.enc_lstm_2 = nn.LSTM(input_size=enc_hidden_size*2, hidden_size=enc_hidden_size, bias=True)\n",
    "\t\tself.rnn_dropout_2 = nn.Dropout(p=rnn_dropout)\n",
    "\t\tself.enc_lstm_3 = nn.LSTM(input_size=enc_hidden_size, hidden_size=enc_hidden_size, bias=True)\n",
    "\t\tself.rnn_dropout_3 = nn.Dropout(p=rnn_dropout)\n",
    "\t\tself.enc_lstm_4 = nn.LSTM(input_size=enc_hidden_size, hidden_size=enc_hidden_size, bias=True)\n",
    "\t\tself.rnn_dropout_4 = nn.Dropout(p=rnn_dropout)\n",
    "\n",
    "\n",
    "\tdef forward(self, inp_embed, inp_last_idx):\n",
    "\t\tbatch_size = inp_embed.shape[0]\n",
    "\t\tinp_embed = torch.nn.utils.rnn.pack_padded_sequence(inp_embed, batch_first=True, lengths=inp_last_idx+1, enforce_sorted=False)\n",
    "\t\tout, (h, c) = self.enc_bilstm_1(inp_embed)\n",
    "\t\tbackward_hidden_state = h.view(1, 2, batch_size, self.enc_hidden_size)[:,1,:,:]                                                # (num_direction, batch_size, enc_hidden_size)\n",
    "\t\tbackward_cell_state = c.view(1, 2, batch_size, self.enc_hidden_size)[:,1,:,:]                                                  # (num_direction, batch_size, enc_hidden_size)\n",
    "\t\tout_unpacked = self.rnn_dropout_1(nn.utils.rnn.pad_packed_sequence(out, batch_first=True)[0])                                  # (batch_size, seq_len, 2*enc_hidden_size)\n",
    "\t\tout = torch.nn.utils.rnn.pack_padded_sequence(out_unpacked, batch_first=True, lengths=inp_last_idx+1, enforce_sorted=False)\n",
    "\t\tout, _ = self.enc_lstm_2(out)\n",
    "\t\tout_unpacked = self.rnn_dropout_2(nn.utils.rnn.pad_packed_sequence(out, batch_first=True)[0])                                  # (batch_size, seq_len, enc_hidden_size)\n",
    "\t\tout = torch.nn.utils.rnn.pack_padded_sequence(out_unpacked, batch_first=True, lengths=inp_last_idx+1, enforce_sorted=False)\n",
    "\t\tout, _ = self.enc_lstm_3(out)\n",
    "\t\tout_unpacked = self.rnn_dropout_3(torch.add(nn.utils.rnn.pad_packed_sequence(out, batch_first=True)[0], out_unpacked))         # (batch_size, seq_len, enc_hidden_size)\n",
    "\t\treturn out_unpacked  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10, 128])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTM_Attention_Extraction_Layer(128, 128, 128)\n",
    "inp = torch.ones(10, 10, 128).float()\n",
    "model(inp, np.arange(10)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Additive_Attention(nn.Module):\n",
    "\t\"\"\"\n",
    "\tAdditive attention used in GNMT\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, hidden_size, **kwargs):\n",
    "\t\tsuper(Additive_Attention, self).__init__(**kwargs)\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\n",
    "\t\tself.W = nn.Linear(hidden_size*2, hidden_size)\n",
    "\t\tself.tanh = nn.Tanh()\n",
    "\t\tself.V = nn.Parameter(torch.Tensor(1, hidden_size))\n",
    "\t\tself.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "\t\tnn.init.normal_(self.V, 0, 0.1)\n",
    "\n",
    "\tdef forward(self, query, values, mask):\n",
    "\t\t\"\"\"\n",
    "\t\t: query:  (batch_size, hidden_size)\n",
    "\t\t: values: (batch_size, seq_len, hidden_size)\n",
    "\t\t: mask:   (batch_size, seq_len)\n",
    "\t\t\"\"\"\n",
    "\t\tbatch_size, seq_len, hidden_size = values.shape\n",
    "\n",
    "\t\tquery = query.unsqueeze(1).expand(-1, seq_len, -1)\n",
    "\t\tscore = self.tanh(self.W(torch.cat((query, values), dim=2)))                              # (batch_size, seq_len, hidden_size)\n",
    "\t\tscore = torch.bmm(self.V.squeeze(1).expand(batch_size, -1, -1), score.permute(0,2,1))     # (batch_size, 1, seq_len)\n",
    "\t\tscore = self.softmax(torch.add(score, mask.unsqueeze(1)))                                 # (batch_size, 1, seq_len)\n",
    "\t\tcontext = torch.bmm(score, values).squeeze(1)                                             # (batch_size, hidden_size)\n",
    "\n",
    "\t\treturn context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = np.random.standard_normal((2, 10, 10))\n",
    "\n",
    "model = Additive_Attention(10)\n",
    "query = torch.from_numpy(np.random.standard_normal((2, 10))).float()\n",
    "values_1 = torch.from_numpy(np.concatenate([p1, np.random.standard_normal((2, 10, 10))], axis=1)).float()\n",
    "values_2 = torch.from_numpy(np.concatenate([p1, np.random.standard_normal((2, 10, 10))], axis=1)).float()\n",
    "mask = torch.from_numpy(np.concatenate([np.zeros((2,9)), np.ones((2,11))], axis=1)).float()*-1e9\n",
    "s1 = model(query, values_1, mask)    \n",
    "s2 = model(query, values_2, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function cat:\n",
      "\n",
      "cat(...)\n",
      "    cat(tensors, dim=0, out=None) -> Tensor\n",
      "    \n",
      "    Concatenates the given sequence of :attr:`seq` tensors in the given dimension.\n",
      "    All tensors must either have the same shape (except in the concatenating\n",
      "    dimension) or be empty.\n",
      "    \n",
      "    :func:`torch.cat` can be seen as an inverse operation for :func:`torch.split`\n",
      "    and :func:`torch.chunk`.\n",
      "    \n",
      "    :func:`torch.cat` can be best understood via examples.\n",
      "    \n",
      "    Args:\n",
      "        tensors (sequence of Tensors): any python sequence of tensors of the same type.\n",
      "            Non-empty tensors provided must have the same shape, except in the\n",
      "            cat dimension.\n",
      "        dim (int, optional): the dimension over which the tensors are concatenated\n",
      "        out (Tensor, optional): the output tensor.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> x = torch.randn(2, 3)\n",
      "        >>> x\n",
      "        tensor([[ 0.6580, -1.0969, -0.4614],\n",
      "                [-0.1034, -0.5790,  0.1497]])\n",
      "        >>> torch.cat((x, x, x), 0)\n",
      "        tensor([[ 0.6580, -1.0969, -0.4614],\n",
      "                [-0.1034, -0.5790,  0.1497],\n",
      "                [ 0.6580, -1.0969, -0.4614],\n",
      "                [-0.1034, -0.5790,  0.1497],\n",
      "                [ 0.6580, -1.0969, -0.4614],\n",
      "                [-0.1034, -0.5790,  0.1497]])\n",
      "        >>> torch.cat((x, x, x), 1)\n",
      "        tensor([[ 0.6580, -1.0969, -0.4614,  0.6580, -1.0969, -0.4614,  0.6580,\n",
      "                 -1.0969, -0.4614],\n",
      "                [-0.1034, -0.5790,  0.1497, -0.1034, -0.5790,  0.1497, -0.1034,\n",
      "                 -0.5790,  0.1497]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_attention_mask(np.arange(1,6),5,10).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Res_LSTM_Layer(nn.Module):\n",
    "\t\"\"\"\n",
    "\tMulti-layer unidirectional with residual connection.\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, n_layer, hidden_size, dropout=0.1, **kwargs):\n",
    "\t\tsuper(Res_LSTM_Layer, self).__init__(**kwargs)\n",
    "\t\tself.n_layer = n_layer\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.dropout = dropout\n",
    "\n",
    "\t\tfor index in range(n_layer):\n",
    "\t\t\tsetattr(self, 'lstm_{}'.format(index), nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, batch_first=True, bias=True))\n",
    "\t\t\tsetattr(self, 'dropout_{}'.format(index), nn.Dropout(p=dropout))\n",
    "\n",
    "\tdef forward(self, inp, inp_len):\n",
    "\t\t_, total_length, _ = inp.shape\n",
    "\t\tfor index in range(self.n_layer):\n",
    "\t\t\tout = nn.utils.rnn.pack_padded_sequence(inp, batch_first=True, lengths=inp_len, enforce_sorted=False)\n",
    "\t\t\tout, _ = getattr(self, 'lstm_{}'.format(index))(out)\n",
    "\t\t\tout = nn.utils.rnn.pad_packed_sequence(out, batch_first=True, total_length=total_length)[0]\n",
    "\t\t\tinp = getattr(self, 'dropout_{}'.format(index))(torch.add(out, inp))\n",
    "\t\treturn inp\n",
    "\n",
    "class GNMT_Encoder_Layer(nn.Module):\n",
    "\t\"\"\"\n",
    "\tGoogle Neural Machine Translation - Encoder\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, input_size, n_layer, hidden_size, dropout=0.1, **kwargs):\n",
    "\t\tsuper(GNMT_Encoder_Layer, self).__init__(**kwargs)\n",
    "\t\tassert n_layer >= 3\n",
    "\n",
    "\t\tself.input_size = input_size\n",
    "\t\tself.n_layer = n_layer\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.dropout = dropout\n",
    "\n",
    "\t\tself.l1_bilstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True, bias=True, bidirectional=True)\n",
    "\t\tself.l1_dropout = nn.Dropout(p=dropout)\n",
    "\t\tself.l2_lstm = nn.LSTM(input_size=input_size*2, hidden_size=hidden_size, bias=True)\n",
    "\t\tself.l2_dropout = nn.Dropout(p=dropout)\n",
    "\t\tself.res_lstm = Res_LSTM_Layer(n_layer-2, hidden_size, dropout=dropout)\n",
    "\n",
    "\tdef forward(self, inp, inp_len):\n",
    "\t\tbatch_size, total_length, _ = inp.shape\n",
    "\t\tinp = nn.utils.rnn.pack_padded_sequence(inp, batch_first=True, lengths=inp_len, enforce_sorted=False)\n",
    "\t\tout, (h, c) = self.l1_bilstm(inp)\n",
    "\t\tbackward_hidden_state = h.view(1, 2, batch_size, self.hidden_size)[:,1,:,:].squeeze(0)              # (num_direction, batch_size, enc_hidden_size)\n",
    "\t\tbackward_cell_state = c.view(1, 2, batch_size, self.hidden_size)[:,1,:,:].squeeze(0)                # (num_direction, batch_size, enc_hidden_size)\n",
    "\t\tout = self.l1_dropout(nn.utils.rnn.pad_packed_sequence(out, batch_first=True, total_length=total_length)[0])\n",
    "\t\tout = nn.utils.rnn.pack_padded_sequence(out, batch_first=True, lengths=inp_len, enforce_sorted=False)\n",
    "\t\tout, _ = self.l2_lstm(out)\n",
    "\t\tout = self.l2_dropout(nn.utils.rnn.pad_packed_sequence(out, batch_first=True, total_length=total_length)[0])\n",
    "\t\tout = self.res_lstm(out, inp_len)\n",
    "\t\treturn out, backward_hidden_state, backward_cell_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Res_LSTM_Layer(1,128)\n",
    "inp = torch.ones(5, 10, 128).float()\n",
    "out = model(inp, np.arange(1,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 128])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNMT_Encoder_Layer(128, 6, 128)\n",
    "inp = torch.ones(5, 10, 128).float()\n",
    "out = model(inp, np.arange(1,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 128])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 128])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].permute(1,0,2)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Additive_Attention_Layer(nn.Module):\n",
    "\t\"\"\"\n",
    "\tAdditive attention used in GNMT\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, hidden_size, **kwargs):\n",
    "\t\tsuper(Additive_Attention_Layer, self).__init__(**kwargs)\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\n",
    "\t\tself.W = nn.Linear(hidden_size*2, hidden_size)\n",
    "\t\tself.tanh = nn.Tanh()\n",
    "\t\tself.V = nn.Parameter(torch.Tensor(1, hidden_size))\n",
    "\t\tself.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "\t\tnn.init.normal_(self.V, 0, 0.1)\n",
    "\n",
    "\tdef forward(self, query, values, mask):\n",
    "\t\t\"\"\"\n",
    "\t\t: query:  (batch_size, hidden_size)\n",
    "\t\t: values: (batch_size, seq_len, hidden_size)\n",
    "\t\t: mask:   (batch_size, seq_len)\n",
    "\t\t\"\"\"\n",
    "\t\tbatch_size, seq_len, hidden_size = values.shape\n",
    "\n",
    "\t\tquery = query.unsqueeze(1).expand(-1, seq_len, -1)\n",
    "\t\tscore = self.tanh(self.W(torch.cat((query, values), dim=2)))                              # (batch_size, seq_len, hidden_size)\n",
    "\t\tscore = torch.bmm(self.V.squeeze(1).expand(batch_size, -1, -1), score.permute(0,2,1))     # (batch_size, 1, seq_len)\n",
    "\t\tscore = self.softmax(torch.add(score, mask.unsqueeze(1)))                                 # (batch_size, 1, seq_len)\n",
    "\t\tcontext = torch.bmm(score, values).squeeze(1)                                             # (batch_size, hidden_size)\n",
    "\n",
    "\t\treturn context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Res_Attn_LSTM_Layer(nn.Module):\n",
    "\t\"\"\"\n",
    "\tMulti-layer unidirectional LSTM with residual connection and attention.\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, n_layer, hidden_size, dropout=0.1, **kwargs):\n",
    "\t\tsuper(Res_Attn_LSTM_Layer, self).__init__(**kwargs)\n",
    "\t\tself.n_layer = n_layer\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.dropout = dropout\n",
    "\n",
    "\t\tfor index in range(n_layer):\n",
    "\t\t\tsetattr(self, 'lstm_{}'.format(index), nn.LSTM(input_size=2*hidden_size, hidden_size=hidden_size, batch_first=True, bias=True))\n",
    "\t\t\tsetattr(self, 'dropout_{}'.format(index), nn.Dropout(p=dropout))\n",
    "\n",
    "\tdef forward(self, hidden_states, context_vectors, inp_len):\n",
    "\t\t_, total_length, _ = hidden_states.shape\n",
    "\t\tfor index in range(self.n_layer):\n",
    "\t\t\tout = nn.utils.rnn.pack_padded_sequence(torch.cat((hidden_states, context_vectors), dim=2), batch_first=True, lengths=inp_len, enforce_sorted=False)\n",
    "\t\t\tout, _ = getattr(self, 'lstm_{}'.format(index))(out)\n",
    "\t\t\tout = nn.utils.rnn.pad_packed_sequence(out, batch_first=True, total_length=total_length)[0]\n",
    "\t\t\thidden_states = getattr(self, 'dropout_{}'.format(index))(torch.add(out, hidden_states))\n",
    "\t\treturn hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Res_Attn_LSTM_Layer(3, 128)\n",
    "inp = torch.ones(5,10,128).float()\n",
    "con = torch.ones(5,10,128).float()\n",
    "out = model(inp, con, np.arange(1,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "592896"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_torch_module_num_of_parameter(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Additive_Attention_Layer(nn.Module):\n",
    "\t\"\"\"\n",
    "\tAdditive attention used in GNMT\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, hidden_size, **kwargs):\n",
    "\t\tsuper(Additive_Attention_Layer, self).__init__(**kwargs)\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\n",
    "\t\tself.W = nn.Linear(hidden_size*2, hidden_size)\n",
    "\t\tself.tanh = nn.Tanh()\n",
    "\t\tself.V = nn.Parameter(torch.Tensor(1, hidden_size))\n",
    "\t\tself.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "\t\tnn.init.normal_(self.V, 0, 0.1)\n",
    "\n",
    "\tdef forward(self, query, values, mask):\n",
    "\t\t\"\"\"\n",
    "\t\t: query:  (batch_size, hidden_size)\n",
    "\t\t: values: (batch_size, seq_len, hidden_size)\n",
    "\t\t: mask:   (batch_size, seq_len)\n",
    "\t\t\"\"\"\n",
    "\t\tprint(query.shape, values.shape, mask.shape)\n",
    "\t\tbatch_size, seq_len, hidden_size = values.shape\n",
    "\n",
    "\t\tquery = query.unsqueeze(1).expand(-1, seq_len, -1)\n",
    "\t\tscore = self.tanh(self.W(torch.cat((query, values), dim=2)))                              # (batch_size, seq_len, hidden_size)\n",
    "\t\tscore = torch.bmm(self.V.squeeze(1).expand(batch_size, -1, -1), score.permute(0,2,1))     # (batch_size, 1, seq_len)\n",
    "\t\tscore = self.softmax(torch.add(score, mask.unsqueeze(1)))                                 # (batch_size, 1, seq_len)\n",
    "\t\tcontext = torch.bmm(score, values).squeeze(1)                                             # (batch_size, hidden_size)\n",
    "\n",
    "\t\treturn context\n",
    "\n",
    "class Res_Attn_LSTM_Layer(nn.Module):\n",
    "\t\"\"\"\n",
    "\tMulti-layer unidirectional LSTM with residual connection and attention.\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, n_layer, hidden_size, dropout=0.1, **kwargs):\n",
    "\t\tsuper(Res_Attn_LSTM_Layer, self).__init__(**kwargs)\n",
    "\t\tself.n_layer = n_layer\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.dropout = dropout\n",
    "\n",
    "\t\tfor index in range(n_layer):\n",
    "\t\t\tsetattr(self, 'lstm_{}'.format(index), nn.LSTM(input_size=2*hidden_size, hidden_size=hidden_size, batch_first=True, bias=True))\n",
    "\t\t\tsetattr(self, 'dropout_{}'.format(index), nn.Dropout(p=dropout))\n",
    "\n",
    "\tdef forward(self, hidden_states, context_vectors, inp_len):\n",
    "\t\t_, total_length, _ = hidden_states.shape\n",
    "\t\tfor index in range(self.n_layer):\n",
    "\t\t\tout = nn.utils.rnn.pack_padded_sequence(torch.cat((hidden_states, context_vectors), dim=2), batch_first=True, lengths=inp_len, enforce_sorted=False)\n",
    "\t\t\tout, _ = getattr(self, 'lstm_{}'.format(index))(out)\n",
    "\t\t\tout = nn.utils.rnn.pad_packed_sequence(out, batch_first=True, total_length=total_length)[0]\n",
    "\t\t\thidden_states = getattr(self, 'dropout_{}'.format(index))(torch.add(out, hidden_states))\n",
    "\t\treturn hidden_states\n",
    "\n",
    "class GNMT_Decoder_Layer(nn.Module):\n",
    "\t\"\"\"\n",
    "\tGoogle Neural Machine Translation - Decoder\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, n_layer, hidden_size, dropout=0.1, device=torch.device('cpu'), **kwargs):\n",
    "\t\tsuper(GNMT_Decoder_Layer, self).__init__(**kwargs)\n",
    "\t\tassert n_layer>=3\n",
    "\n",
    "\t\tself.n_layer = n_layer\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.dropout = dropout\n",
    "\t\tself.device = device\n",
    "\n",
    "\t\tself.attention_calc = Additive_Attention_Layer(hidden_size)\n",
    "\t\tself.l1_lstm_cell = nn.LSTMCell(input_size=2*hidden_size, hidden_size=hidden_size, bias=True)\n",
    "\t\tself.l1_dropout = nn.Dropout(p=dropout)\n",
    "\t\tself.l2_lstm = nn.LSTM(input_size=2*hidden_size, hidden_size=hidden_size, batch_first=True, bias=True)\n",
    "\t\tself.l2_dropout = nn.Dropout(p=dropout)\n",
    "\t\tself.res_attn_lstm = Res_Attn_LSTM_Layer(n_layer-2, hidden_size, dropout=dropout)\n",
    "\n",
    "\tdef get_attention_mask(self, inp_len, batch_size, seq_len):\n",
    "\t\tmask = np.ones((batch_size, seq_len))\n",
    "\t\tfor index, l in enumerate(inp_len):\n",
    "\t\t\tmask[index,:l] = 0\n",
    "\t\tmask *= -1e9\n",
    "\t\treturn torch.from_numpy(mask).float().to(self.device)\n",
    "\n",
    "\tdef forward(self, enc_hidden_states, backward_hidden_state, backward_cell_state, inp_len):\n",
    "\t\tbatch_size, seq_len, _ = enc_hidden_states.shape\n",
    "\t\tattention_mask = self.get_attention_mask(inp_len, batch_size, seq_len)\n",
    "\t\tenc_hidden_states = enc_hidden_states.permute(1,0,2)                                                                                          # (seq_len, batch_size, hidden_size)\n",
    "\t\tdecoder_hidden_states_buf =  []\n",
    "\t\tdecoder_context_vectors_buf = []\n",
    "\t\tdecoder_h, decoder_c = backward_hidden_state, backward_cell_state\n",
    "\t\tfor step in range(seq_len):\n",
    "\t\t\tinp = enc_hidden_states[step]                        \n",
    "\t\t\tcontext_vector = self.attention_calc(inp, enc_hidden_states.permute(1,0,2), attention_mask)                                               # (batch_size, hidden_size)\n",
    "\t\t\tdecoder_context_vectors_buf.append(context_vector)\n",
    "\t\t\tinp = torch.cat((inp, context_vector), dim=1)                                                                                                    # (batch_size, 2*hidden_size)\n",
    "\t\t\tdecoder_h, decoder_c = self.l1_lstm_cell(inp, (decoder_c, decoder_h))\n",
    "\t\t\tdecoder_hidden_states_buf.append(decoder_h)\n",
    "\t\tdecoder_context_vectors = torch.stack(decoder_context_vectors_buf, dim=1)                                                                     # (batch_size, seq_len, hidden_size)\n",
    "\t\tdecoder_hidden_states = torch.stack(decoder_hidden_states_buf, dim=1)                                                                         # (batch_size, seq_len, hidden_size)\n",
    "\t\tdecoder_hidden_states = self.l1_dropout(torch.cat((decoder_hidden_states, decoder_context_vectors), dim=2))                                   # (batch_size, seq_len, 2*hidden_size)\n",
    "\t\tdecoder_hidden_states = nn.utils.rnn.pack_padded_sequence(decoder_hidden_states, batch_first=True, lengths=inp_len, enforce_sorted=False)\n",
    "\t\tdecoder_hidden_states, _ = self.l2_lstm(decoder_hidden_states)\n",
    "\t\tdecoder_hidden_states = nn.utils.rnn.pad_packed_sequence(decoder_hidden_states, batch_first=True, total_length=seq_len)[0]\n",
    "\t\tdecoder_hidden_states = self.l2_dropout(decoder_hidden_states)\n",
    "\t\tdecoder_hidden_states = self.res_attn_lstm(decoder_hidden_states, decoder_context_vectors, inp_len)                                                    # (batch_size, seq_len, hidden_size)\n",
    "\t\treturn decoder_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 128]) torch.Size([5, 10, 128]) torch.Size([5, 10])\n",
      "torch.Size([5, 128]) torch.Size([5, 10, 128]) torch.Size([5, 10])\n",
      "torch.Size([5, 128]) torch.Size([5, 10, 128]) torch.Size([5, 10])\n",
      "torch.Size([5, 128]) torch.Size([5, 10, 128]) torch.Size([5, 10])\n",
      "torch.Size([5, 128]) torch.Size([5, 10, 128]) torch.Size([5, 10])\n",
      "torch.Size([5, 128]) torch.Size([5, 10, 128]) torch.Size([5, 10])\n",
      "torch.Size([5, 128]) torch.Size([5, 10, 128]) torch.Size([5, 10])\n",
      "torch.Size([5, 128]) torch.Size([5, 10, 128]) torch.Size([5, 10])\n",
      "torch.Size([5, 128]) torch.Size([5, 10, 128]) torch.Size([5, 10])\n",
      "torch.Size([5, 128]) torch.Size([5, 10, 128]) torch.Size([5, 10])\n"
     ]
    }
   ],
   "source": [
    "model = GNMT_Decoder_Layer(6, 128)\n",
    "inp = torch.ones(5, 10, 128).float()\n",
    "h = torch.ones(5,128).float()\n",
    "c = torch.ones(5,128).float()\n",
    "out = model(inp, h, c, np.arange(1,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 128])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1218816"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_torch_module_num_of_parameter(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Res_LSTM_Layer(nn.Module):\n",
    "\t\"\"\"\n",
    "\tMulti-layer unidirectional LSTM with residual connection.\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, n_layer, hidden_size, dropout=0.1, **kwargs):\n",
    "\t\tsuper(Res_LSTM_Layer, self).__init__(**kwargs)\n",
    "\t\tself.n_layer = n_layer\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.dropout = dropout\n",
    "\n",
    "\t\tfor index in range(n_layer):\n",
    "\t\t\tsetattr(self, 'lstm_{}'.format(index), nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, batch_first=True, bias=True))\n",
    "\t\t\tsetattr(self, 'dropout_{}'.format(index), nn.Dropout(p=dropout))\n",
    "\n",
    "\tdef forward(self, inp, inp_len):\n",
    "\t\t_, total_length, _ = inp.shape\n",
    "\t\tfor index in range(self.n_layer):\n",
    "\t\t\tout = nn.utils.rnn.pack_padded_sequence(inp, batch_first=True, lengths=inp_len, enforce_sorted=False)\n",
    "\t\t\tout, _ = getattr(self, 'lstm_{}'.format(index))(out)\n",
    "\t\t\tout = nn.utils.rnn.pad_packed_sequence(out, batch_first=True, total_length=total_length)[0]\n",
    "\t\t\tinp = getattr(self, 'dropout_{}'.format(index))(torch.add(out, inp))\n",
    "\t\treturn inp\n",
    "\n",
    "class GNMT_Encoder_Layer(nn.Module):\n",
    "\t\"\"\"\n",
    "\tGoogle Neural Machine Translation - Encoder\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, input_size, n_layer, hidden_size, dropout=0.1, **kwargs):\n",
    "\t\tsuper(GNMT_Encoder_Layer, self).__init__(**kwargs)\n",
    "\t\tassert n_layer >= 3\n",
    "\n",
    "\t\tself.input_size = input_size\n",
    "\t\tself.n_layer = n_layer\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.dropout = dropout\n",
    "\n",
    "\t\tself.l1_bilstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True, bias=True, bidirectional=True)\n",
    "\t\tself.l1_dropout = nn.Dropout(p=dropout)\n",
    "\t\tself.l2_lstm = nn.LSTM(input_size=hidden_size*2, hidden_size=hidden_size, bias=True)\n",
    "\t\tself.l2_dropout = nn.Dropout(p=dropout)\n",
    "\t\tself.res_lstm = Res_LSTM_Layer(n_layer-2, hidden_size, dropout=dropout)\n",
    "\n",
    "\tdef forward(self, inp, inp_len):\n",
    "\t\tbatch_size, total_length, _ = inp.shape\n",
    "\t\tinp = nn.utils.rnn.pack_padded_sequence(inp, batch_first=True, lengths=inp_len, enforce_sorted=False)\n",
    "\t\tout, (h, c) = self.l1_bilstm(inp)\n",
    "\t\tbackward_hidden_state = h.view(1, 2, batch_size, self.hidden_size)[:,1,:,:].squeeze(0)              # (num_direction, batch_size, enc_hidden_size)\n",
    "\t\tbackward_cell_state = c.view(1, 2, batch_size, self.hidden_size)[:,1,:,:].squeeze(0)                # (num_direction, batch_size, enc_hidden_size)\n",
    "\t\tout = self.l1_dropout(nn.utils.rnn.pad_packed_sequence(out, batch_first=True, total_length=total_length)[0])\n",
    "\t\tout = nn.utils.rnn.pack_padded_sequence(out, batch_first=True, lengths=inp_len, enforce_sorted=False)\n",
    "\t\tout, _ = self.l2_lstm(out)\n",
    "\t\tout = self.l2_dropout(nn.utils.rnn.pad_packed_sequence(out, batch_first=True, total_length=total_length)[0])\n",
    "\t\tout = self.res_lstm(out, inp_len)\n",
    "\t\treturn out, backward_hidden_state, backward_cell_state\n",
    "\n",
    "class Additive_Attention_Layer(nn.Module):\n",
    "\t\"\"\"\n",
    "\tAdditive attention used in GNMT\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, hidden_size, **kwargs):\n",
    "\t\tsuper(Additive_Attention_Layer, self).__init__(**kwargs)\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\n",
    "\t\tself.W = nn.Linear(hidden_size*2, hidden_size)\n",
    "\t\tself.tanh = nn.Tanh()\n",
    "\t\tself.V = nn.Parameter(torch.Tensor(1, hidden_size))\n",
    "\t\tself.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "\t\tnn.init.normal_(self.V, 0, 0.1)\n",
    "\n",
    "\tdef forward(self, query, values, mask):\n",
    "\t\t\"\"\"\n",
    "\t\t: query:  (batch_size, hidden_size)\n",
    "\t\t: values: (batch_size, seq_len, hidden_size)\n",
    "\t\t: mask:   (batch_size, seq_len)\n",
    "\t\t\"\"\"\n",
    "\t\tbatch_size, seq_len, hidden_size = values.shape\n",
    "\n",
    "\t\tquery = query.unsqueeze(1).expand(-1, seq_len, -1)\n",
    "\t\tscore = self.tanh(self.W(torch.cat((query, values), dim=2)))                              # (batch_size, seq_len, hidden_size)\n",
    "\t\tscore = torch.bmm(self.V.squeeze(1).expand(batch_size, -1, -1), score.permute(0,2,1))     # (batch_size, 1, seq_len)\n",
    "\t\tscore = self.softmax(torch.add(score, mask.unsqueeze(1)))                                 # (batch_size, 1, seq_len)\n",
    "\t\tcontext = torch.bmm(score, values).squeeze(1)                                             # (batch_size, hidden_size)\n",
    "\n",
    "\t\treturn context\n",
    "\n",
    "class Res_Attn_LSTM_Layer(nn.Module):\n",
    "\t\"\"\"\n",
    "\tMulti-layer unidirectional LSTM with residual connection and attention.\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, n_layer, hidden_size, dropout=0.1, **kwargs):\n",
    "\t\tsuper(Res_Attn_LSTM_Layer, self).__init__(**kwargs)\n",
    "\t\tself.n_layer = n_layer\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.dropout = dropout\n",
    "\n",
    "\t\tfor index in range(n_layer):\n",
    "\t\t\tsetattr(self, 'lstm_{}'.format(index), nn.LSTM(input_size=2*hidden_size, hidden_size=hidden_size, batch_first=True, bias=True))\n",
    "\t\t\tsetattr(self, 'dropout_{}'.format(index), nn.Dropout(p=dropout))\n",
    "\n",
    "\tdef forward(self, hidden_states, context_vectors, inp_len):\n",
    "\t\t_, total_length, _ = hidden_states.shape\n",
    "\t\tfor index in range(self.n_layer):\n",
    "\t\t\tout = nn.utils.rnn.pack_padded_sequence(torch.cat((hidden_states, context_vectors), dim=2), batch_first=True, lengths=inp_len, enforce_sorted=False)\n",
    "\t\t\tout, _ = getattr(self, 'lstm_{}'.format(index))(out)\n",
    "\t\t\tout = nn.utils.rnn.pad_packed_sequence(out, batch_first=True, total_length=total_length)[0]\n",
    "\t\t\thidden_states = getattr(self, 'dropout_{}'.format(index))(torch.add(out, hidden_states))\n",
    "\t\treturn hidden_states\n",
    "\n",
    "class GNMT_Decoder_Layer(nn.Module):\n",
    "\t\"\"\"\n",
    "\tGoogle Neural Machine Translation - Decoder\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, n_layer, hidden_size, dropout=0.1, device=None, **kwargs):\n",
    "\t\tsuper(GNMT_Decoder_Layer, self).__init__(**kwargs)\n",
    "\t\tassert n_layer>=3\n",
    "\n",
    "\t\tself.n_layer = n_layer\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.dropout = dropout\n",
    "\t\tself.device = device if device else torch.device('cpu')\n",
    "\n",
    "\t\tself.attention_calc = Additive_Attention_Layer(hidden_size)\n",
    "\t\tself.l1_lstm_cell = nn.LSTMCell(input_size=2*hidden_size, hidden_size=hidden_size, bias=True)\n",
    "\t\tself.l1_dropout = nn.Dropout(p=dropout)\n",
    "\t\tself.l2_lstm = nn.LSTM(input_size=2*hidden_size, hidden_size=hidden_size, batch_first=True, bias=True)\n",
    "\t\tself.l2_dropout = nn.Dropout(p=dropout)\n",
    "\t\tself.res_attn_lstm = Res_Attn_LSTM_Layer(n_layer-2, hidden_size, dropout=dropout)\n",
    "\n",
    "\tdef get_attention_mask(self, inp_len, batch_size, seq_len):\n",
    "\t\tmask = np.ones((batch_size, seq_len))\n",
    "\t\tfor index, l in enumerate(inp_len):\n",
    "\t\t\tmask[index,:l] = 0\n",
    "\t\tmask *= -1e9\n",
    "\t\treturn torch.from_numpy(mask).float().to(self.device)\n",
    "\n",
    "\tdef forward(self, enc_hidden_states, backward_hidden_state, backward_cell_state, inp_len):\n",
    "\t\tbatch_size, seq_len, _ = enc_hidden_states.shape\n",
    "\t\tattention_mask = self.get_attention_mask(inp_len, batch_size, seq_len)\n",
    "\t\tenc_hidden_states = enc_hidden_states.permute(1,0,2)                                                                                          # (seq_len, batch_size, hidden_size)\n",
    "\t\tdecoder_hidden_states_buf =  []\n",
    "\t\tdecoder_context_vectors_buf = []\n",
    "\t\tdecoder_h, decoder_c = backward_hidden_state, backward_cell_state\n",
    "\t\tfor step in range(seq_len):\n",
    "\t\t\tinp = enc_hidden_states[step]                        \n",
    "\t\t\tcontext_vector = self.attention_calc(inp, enc_hidden_states.permute(1,0,2), attention_mask)                                               # (batch_size, hidden_size)\n",
    "\t\t\tdecoder_context_vectors_buf.append(context_vector)\n",
    "\t\t\tinp = torch.cat((inp, context_vector), dim=1)                                                                                                    # (batch_size, 2*hidden_size)\n",
    "\t\t\tdecoder_h, decoder_c = self.l1_lstm_cell(inp, (decoder_c, decoder_h))\n",
    "\t\t\tdecoder_hidden_states_buf.append(decoder_h)\n",
    "\t\tdecoder_context_vectors = torch.stack(decoder_context_vectors_buf, dim=1)                                                                     # (batch_size, seq_len, hidden_size)\n",
    "\t\tdecoder_hidden_states = torch.stack(decoder_hidden_states_buf, dim=1)                                                                         # (batch_size, seq_len, hidden_size)\n",
    "\t\tdecoder_hidden_states = self.l1_dropout(torch.cat((decoder_hidden_states, decoder_context_vectors), dim=2))                                   # (batch_size, seq_len, 2*hidden_size)\n",
    "\t\tdecoder_hidden_states = nn.utils.rnn.pack_padded_sequence(decoder_hidden_states, batch_first=True, lengths=inp_len, enforce_sorted=False)\n",
    "\t\tdecoder_hidden_states, _ = self.l2_lstm(decoder_hidden_states)\n",
    "\t\tdecoder_hidden_states = nn.utils.rnn.pad_packed_sequence(decoder_hidden_states, batch_first=True, total_length=seq_len)[0]\n",
    "\t\tdecoder_hidden_states = self.l2_dropout(decoder_hidden_states)\n",
    "\t\tdecoder_hidden_states = self.res_attn_lstm(decoder_hidden_states, decoder_context_vectors, inp_len)                                                    # (batch_size, seq_len, hidden_size)\n",
    "\t\treturn decoder_hidden_states\n",
    "\n",
    "class GNMT_Extraction_Layer(nn.Module):\n",
    "\t\"\"\"\n",
    "\tSeq2Seq feature extration layer based on Google Neural Machine Translation.\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, embed_size, hidden_size, n_enc_layer, n_dec_layer, device=None, dropout=0.1, **kwargs):\n",
    "\t\tsuper(GNMT_Extraction_Layer, self).__init__(**kwargs)\n",
    "\t\tself.embed_size = embed_size\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.n_enc_layer = n_enc_layer\n",
    "\t\tself.n_dec_layer = n_dec_layer\n",
    "\t\tself.device = device if device else torch.device('cpu')\n",
    "\t\tself.dropout = dropout\n",
    "\n",
    "\t\tself.encoder = GNMT_Encoder_Layer(embed_size, n_enc_layer, hidden_size)\n",
    "\t\tself.decoder = GNMT_Decoder_Layer(n_dec_layer, hidden_size, device=self.device)\n",
    "\n",
    "\tdef forward(self, inp, inp_len):\n",
    "\t\tencoder_hidden_states, backward_hidden_state, backward_cell_state = self.encoder(inp, inp_len)\n",
    "\t\tdecoder_hidden_states = self.decoder(encoder_hidden_states, backward_hidden_state, backward_cell_state, inp_len)\n",
    "\t\treturn decoder_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNMT_Extraction_Layer(128, 128, 8,8)\n",
    "inp = torch.ones(5, 10, 128).float()\n",
    "out = model(inp, np.arange(1,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 128])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2868480"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_torch_module_num_of_parameter(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Res_LSTM_Layer(nn.Module):\n",
    "\t\"\"\"\n",
    "\tMulti-layer unidirectional LSTM with residual connection.\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, n_layer, hidden_size, dropout=0.1, **kwargs):\n",
    "\t\tsuper(Res_LSTM_Layer, self).__init__(**kwargs)\n",
    "\t\tself.n_layer = n_layer\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.dropout = dropout\n",
    "\n",
    "\t\tfor index in range(n_layer):\n",
    "\t\t\tsetattr(self, 'lstm_{}'.format(index), nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, batch_first=True, bias=True))\n",
    "\t\t\tsetattr(self, 'dropout_{}'.format(index), nn.Dropout(p=dropout))\n",
    "\n",
    "\tdef forward(self, inp, inp_len):\n",
    "\t\t_, total_length, _ = inp.shape\n",
    "\t\tfor index in range(self.n_layer):\n",
    "\t\t\tout = nn.utils.rnn.pack_padded_sequence(inp, batch_first=True, lengths=inp_len, enforce_sorted=False)\n",
    "\t\t\tout, _ = getattr(self, 'lstm_{}'.format(index))(out)\n",
    "\t\t\tout = nn.utils.rnn.pad_packed_sequence(out, batch_first=True, total_length=total_length)[0]\n",
    "\t\t\tinp = getattr(self, 'dropout_{}'.format(index))(torch.add(out, inp))\n",
    "\t\treturn inp\n",
    "\n",
    "class GNMT_Encoder_Layer(nn.Module):\n",
    "\t\"\"\"\n",
    "\tGoogle Neural Machine Translation - Encoder\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, input_size, n_layer, hidden_size, dropout=0.1, **kwargs):\n",
    "\t\tsuper(GNMT_Encoder_Layer, self).__init__(**kwargs)\n",
    "\t\tassert n_layer >= 3\n",
    "\n",
    "\t\tself.input_size = input_size\n",
    "\t\tself.n_layer = n_layer\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.dropout = dropout\n",
    "\n",
    "\t\tself.l1_bilstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True, bias=True, bidirectional=True)\n",
    "\t\tself.l1_dropout = nn.Dropout(p=dropout)\n",
    "\t\tself.l2_lstm = nn.LSTM(input_size=hidden_size*2, hidden_size=hidden_size, bias=True)\n",
    "\t\tself.l2_dropout = nn.Dropout(p=dropout)\n",
    "\t\tself.res_lstm = Res_LSTM_Layer(n_layer-2, hidden_size, dropout=dropout)\n",
    "\n",
    "\tdef forward(self, inp, inp_len):\n",
    "\t\tbatch_size, total_length, _ = inp.shape\n",
    "\t\tinp = nn.utils.rnn.pack_padded_sequence(inp, batch_first=True, lengths=inp_len, enforce_sorted=False)\n",
    "\t\tout, (h, c) = self.l1_bilstm(inp)\n",
    "\t\tbackward_hidden_state = h.view(1, 2, batch_size, self.hidden_size)[:,1,:,:].squeeze(0)              # (num_direction, batch_size, enc_hidden_size)\n",
    "\t\tbackward_cell_state = c.view(1, 2, batch_size, self.hidden_size)[:,1,:,:].squeeze(0)                # (num_direction, batch_size, enc_hidden_size)\n",
    "\t\tout = self.l1_dropout(nn.utils.rnn.pad_packed_sequence(out, batch_first=True, total_length=total_length)[0])\n",
    "\t\tout = nn.utils.rnn.pack_padded_sequence(out, batch_first=True, lengths=inp_len, enforce_sorted=False)\n",
    "\t\tout, _ = self.l2_lstm(out)\n",
    "\t\tout = self.l2_dropout(nn.utils.rnn.pad_packed_sequence(out, batch_first=True, total_length=total_length)[0])\n",
    "\t\tout = self.res_lstm(out, inp_len)\n",
    "\t\treturn out, backward_hidden_state, backward_cell_state\n",
    "\n",
    "class Additive_Attention_Layer(nn.Module):\n",
    "\t\"\"\"\n",
    "\tAdditive attention used in GNMT\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, hidden_size, **kwargs):\n",
    "\t\tsuper(Additive_Attention_Layer, self).__init__(**kwargs)\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\n",
    "\t\tself.W = nn.Linear(hidden_size*2, hidden_size)\n",
    "\t\tself.tanh = nn.Tanh()\n",
    "\t\tself.V = nn.Parameter(torch.Tensor(1, hidden_size))\n",
    "\t\tself.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "\t\tnn.init.normal_(self.V, 0, 0.1)\n",
    "\n",
    "\tdef forward(self, query, values, mask):\n",
    "\t\t\"\"\"\n",
    "\t\t: query:  (batch_size, hidden_size)\n",
    "\t\t: values: (batch_size, seq_len, hidden_size)\n",
    "\t\t: mask:   (batch_size, seq_len)\n",
    "\t\t\"\"\"\n",
    "\t\tbatch_size, seq_len, hidden_size = values.shape\n",
    "\n",
    "\t\tquery = query.unsqueeze(1).expand(-1, seq_len, -1)\n",
    "\t\tscore = self.tanh(self.W(torch.cat((query, values), dim=2)))                              # (batch_size, seq_len, hidden_size)\n",
    "\t\tscore = torch.bmm(self.V.squeeze(1).expand(batch_size, -1, -1), score.permute(0,2,1))     # (batch_size, 1, seq_len)\n",
    "\t\tscore = self.softmax(torch.add(score, mask.unsqueeze(1)))                                 # (batch_size, 1, seq_len)\n",
    "\t\tcontext = torch.bmm(score, values).squeeze(1)                                             # (batch_size, hidden_size)\n",
    "\n",
    "\t\treturn context\n",
    "\n",
    "class Res_Attn_LSTM_Layer(nn.Module):\n",
    "\t\"\"\"\n",
    "\tMulti-layer unidirectional LSTM with residual connection and attention.\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, n_layer, hidden_size, dropout=0.1, **kwargs):\n",
    "\t\tsuper(Res_Attn_LSTM_Layer, self).__init__(**kwargs)\n",
    "\t\tself.n_layer = n_layer\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.dropout = dropout\n",
    "\n",
    "\t\tfor index in range(n_layer):\n",
    "\t\t\tsetattr(self, 'lstm_{}'.format(index), nn.LSTM(input_size=2*hidden_size, hidden_size=hidden_size, batch_first=True, bias=True))\n",
    "\t\t\tsetattr(self, 'dropout_{}'.format(index), nn.Dropout(p=dropout))\n",
    "\n",
    "\tdef forward(self, hidden_states, context_vectors, inp_len):\n",
    "\t\t_, total_length, _ = hidden_states.shape\n",
    "\t\tfor index in range(self.n_layer):\n",
    "\t\t\tout = nn.utils.rnn.pack_padded_sequence(torch.cat((hidden_states, context_vectors), dim=2), batch_first=True, lengths=inp_len, enforce_sorted=False)\n",
    "\t\t\tout, _ = getattr(self, 'lstm_{}'.format(index))(out)\n",
    "\t\t\tout = nn.utils.rnn.pad_packed_sequence(out, batch_first=True, total_length=total_length)[0]\n",
    "\t\t\thidden_states = getattr(self, 'dropout_{}'.format(index))(torch.add(out, hidden_states))\n",
    "\t\treturn hidden_states\n",
    "\n",
    "class GNMT_Decoder_Layer(nn.Module):\n",
    "\t\"\"\"\n",
    "\tGoogle Neural Machine Translation - Decoder\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, n_layer, hidden_size, dropout=0.1, device=None, **kwargs):\n",
    "\t\tsuper(GNMT_Decoder_Layer, self).__init__(**kwargs)\n",
    "\t\tassert n_layer>=3\n",
    "\n",
    "\t\tself.n_layer = n_layer\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.dropout = dropout\n",
    "\t\tself.device = device if device else torch.device('cpu')\n",
    "\n",
    "\t\tself.attention_calc = Additive_Attention_Layer(hidden_size)\n",
    "\t\tself.l1_lstm_cell = nn.LSTMCell(input_size=2*hidden_size, hidden_size=hidden_size, bias=True)\n",
    "\t\tself.l1_dropout = nn.Dropout(p=dropout)\n",
    "\t\tself.l2_lstm = nn.LSTM(input_size=2*hidden_size, hidden_size=hidden_size, batch_first=True, bias=True)\n",
    "\t\tself.l2_dropout = nn.Dropout(p=dropout)\n",
    "\t\tself.res_attn_lstm = Res_Attn_LSTM_Layer(n_layer-2, hidden_size, dropout=dropout)\n",
    "\n",
    "\tdef get_attention_mask(self, inp_len, batch_size, seq_len):\n",
    "\t\tmask = np.ones((batch_size, seq_len))\n",
    "\t\tfor index, l in enumerate(inp_len):\n",
    "\t\t\tmask[index,:l] = 0\n",
    "\t\tmask *= -1e9\n",
    "\t\treturn torch.from_numpy(mask).float().to(self.device)\n",
    "\n",
    "\tdef forward(self, enc_hidden_states, backward_hidden_state, backward_cell_state, inp_len):\n",
    "\t\tbatch_size, seq_len, _ = enc_hidden_states.shape\n",
    "\t\tattention_mask = self.get_attention_mask(inp_len, batch_size, seq_len)\n",
    "\t\tenc_hidden_states = enc_hidden_states.permute(1,0,2)                                                                                          # (seq_len, batch_size, hidden_size)\n",
    "\t\tdecoder_hidden_states_buf =  []\n",
    "\t\tdecoder_context_vectors_buf = []\n",
    "\t\tdecoder_h, decoder_c = backward_hidden_state, backward_cell_state\n",
    "\t\tfor step in range(seq_len):\n",
    "\t\t\tinp = enc_hidden_states[step]                        \n",
    "\t\t\tcontext_vector = self.attention_calc(inp, enc_hidden_states.permute(1,0,2), attention_mask)                                               # (batch_size, hidden_size)\n",
    "\t\t\tdecoder_context_vectors_buf.append(context_vector)\n",
    "\t\t\tinp = torch.cat((inp, context_vector), dim=1)                                                                                                    # (batch_size, 2*hidden_size)\n",
    "\t\t\tdecoder_h, decoder_c = self.l1_lstm_cell(inp, (decoder_c, decoder_h))\n",
    "\t\t\tdecoder_hidden_states_buf.append(decoder_h)\n",
    "\t\tdecoder_context_vectors = torch.stack(decoder_context_vectors_buf, dim=1)                                                                     # (batch_size, seq_len, hidden_size)\n",
    "\t\tdecoder_hidden_states = torch.stack(decoder_hidden_states_buf, dim=1)                                                                         # (batch_size, seq_len, hidden_size)\n",
    "\t\tdecoder_hidden_states = self.l1_dropout(torch.cat((decoder_hidden_states, decoder_context_vectors), dim=2))                                   # (batch_size, seq_len, 2*hidden_size)\n",
    "\t\tdecoder_hidden_states = nn.utils.rnn.pack_padded_sequence(decoder_hidden_states, batch_first=True, lengths=inp_len, enforce_sorted=False)\n",
    "\t\tdecoder_hidden_states, _ = self.l2_lstm(decoder_hidden_states)\n",
    "\t\tdecoder_hidden_states = nn.utils.rnn.pad_packed_sequence(decoder_hidden_states, batch_first=True, total_length=seq_len)[0]\n",
    "\t\tdecoder_hidden_states = self.l2_dropout(decoder_hidden_states)\n",
    "\t\tdecoder_hidden_states = self.res_attn_lstm(decoder_hidden_states, decoder_context_vectors, inp_len)                                                    # (batch_size, seq_len, hidden_size)\n",
    "\t\treturn decoder_hidden_states\n",
    "\n",
    "class GNMT_Extraction_Layer(nn.Module):\n",
    "\t\"\"\"\n",
    "\tSeq2Seq feature extration layer based on Google Neural Machine Translation.\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, embed_size, hidden_size, n_enc_layer, n_dec_layer, device=None, dropout=0.1, **kwargs):\n",
    "\t\tsuper(GNMT_Extraction_Layer, self).__init__(**kwargs)\n",
    "\t\tself.embed_size = embed_size\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.n_enc_layer = n_enc_layer\n",
    "\t\tself.n_dec_layer = n_dec_layer\n",
    "\t\tself.device = device if device else torch.device('cpu')\n",
    "\t\tself.dropout = dropout\n",
    "\n",
    "\t\tself.encoder = GNMT_Encoder_Layer(embed_size, n_enc_layer, hidden_size)\n",
    "\t\tself.decoder = GNMT_Decoder_Layer(n_dec_layer, hidden_size, device=self.device)\n",
    "\n",
    "\tdef forward(self, inp, inp_len):\n",
    "\t\tencoder_hidden_states, backward_hidden_state, backward_cell_state = self.encoder(inp, inp_len)\n",
    "\t\tdecoder_hidden_states = self.decoder(encoder_hidden_states, backward_hidden_state, backward_cell_state, inp_len)\n",
    "\t\treturn decoder_hidden_states\n",
    "\n",
    "class MLP_Classification_Layer(nn.Module):\n",
    "\t\"\"\"\n",
    "\tMultilayer Perception Classification Layer\n",
    "\t- Layer 1: Linear + Batchnorm + ReLU + Dropout\n",
    "\t- Layer 2: Linear + Batchnorm + ReLU + Dropout\n",
    "\t- Layer 3: Linear\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, inp_size, out_size, dropout=0.4, **kwargs):\n",
    "\t\tsuper(MLP_Classification_Layer, self).__init__(**kwargs)\n",
    "\t\tself.inp_size = inp_size\n",
    "\t\tself.out_size = out_size\n",
    "\t\tself.dropout = dropout\n",
    "\t\t\n",
    "\t\tself.mlp_1 = nn.Linear(inp_size, 1024)\n",
    "\t\tself.batchnorm_1 = nn.BatchNorm1d(1024)\n",
    "\t\tself.mlp_dropout_1 = nn.Dropout(p=dropout)\n",
    "\t\tself.mlp_2 = nn.Linear(1024, 512)\n",
    "\t\tself.batchnorm_2 = nn.BatchNorm1d(512)\n",
    "\t\tself.mlp_dropout_2 = nn.Dropout(p=dropout)\n",
    "\t\tself.mlp_3 = nn.Linear(512, out_size)\n",
    "\t\t\n",
    "\tdef forward(self, inp):\n",
    "\t\tmlp_out = self.mlp_1(inp)                                                         # (batch_size, 1024)\n",
    "\t\tmlp_out = self.mlp_dropout_1(F.relu(self.batchnorm_1(mlp_out)))                   # (batch_size, 1024)\n",
    "\t\tmlp_out = self.mlp_2(mlp_out)                                                     # (batch_size, 512)\n",
    "\t\tmlp_out = self.mlp_dropout_2(F.relu(self.batchnorm_2(mlp_out)))                   # (batch_size, 512)\n",
    "\t\tmlp_out = self.mlp_3(mlp_out)                                                     # (batch_size, out_size)\n",
    "\t\treturn mlp_out   \n",
    "\n",
    "class GNMT_Classifier(nn.Module):\n",
    "\t\"\"\"\n",
    "\tUse GNMT for Seq2Seq feature extraction, apply max pooling & pick last state, then use multilayer perception for classification\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, out_size, embed_size, hidden_size, n_enc_layer, n_dec_layer, device=None, rnn_dropout=0.1, dnn_dropout=0.4, **kwargs):\n",
    "\t\tsuper(GNMT_Classifier, self).__init__()\n",
    "\t\tself.out_size = out_size\n",
    "\t\tself.embed_size = embed_size\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.n_enc_layer = n_enc_layer\n",
    "\t\tself.n_dec_layer = n_dec_layer\n",
    "\t\tself.device = device if device else torch.device('cpu')\n",
    "\t\tself.rnn_dropout = rnn_dropout\n",
    "\t\tself.dnn_dropout = dnn_dropout\n",
    "\n",
    "\t\tself.GNMT_layer = GNMT_Extraction_Layer(embed_size, hidden_size, n_enc_layer, n_dec_layer, device=device, dropout=rnn_dropout)\n",
    "\t\tself.mlp_layer = MLP_Classification_Layer(hidden_size*2, out_size, dropout=dnn_dropout)\n",
    "\n",
    "\tdef forward(self, inp, inp_len):\n",
    "\t\tinp = self.GNMT_layer(inp, inp_len)                                                        # (batch_size, seq_len, hidden_size)\n",
    "\t\tmax_pool_buf, last_buf = [], []\n",
    "\t\tfor batch_idx, l in enumerate(inp_len):\n",
    "\t\t\tmax_pool_buf.append(torch.max(inp[batch_idx,:l], dim=0)[0])\n",
    "\t\t\tlast_buf.append(inp[batch_idx, l-1])\n",
    "\t\tout = torch.cat((torch.stack(max_pool_buf, dim=0), torch.stack(last_buf, dim=0)), dim=1)   # (batch_size, hidden_size*2)\n",
    "\t\tout = self.mlp_layer(out)                                                                 # (batch_size, out_size)\n",
    "\t\treturn out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNMT_Classifier(10,256,256,8,8)\n",
    "inp = torch.ones(5, 10, 256).float()\n",
    "out = model(inp, np.arange(1,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12496906"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_torch_module_num_of_parameter(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_bert import BertConfig, BertEncoder, BertAttention,BertSelfAttention,BertLayer,BertPooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class Positional_Encoding_Layer(nn.Module):\n",
    "\t\"\"\"\n",
    "\tPositional encoding using sine and cosine as described in \"Attention is all you need\".\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, d_model, max_seq_len=512, dropout=0.1):\n",
    "\t\t\"\"\"\n",
    "\t\tFormula:\n",
    "\t\t| PE(pos,2i) = sin(pos/10000**(2*i/d_model))\n",
    "\t\t| PE(pos,2i+1) = cos(pos/10000**(2*i/d_model))\n",
    "\t\t\"\"\"\n",
    "\t\tsuper(Positional_Encoding_Layer, self).__init__()\n",
    "\t\tself.d_model = d_model\n",
    "\t\tself.dropout = dropout\n",
    "\t\tself.max_seq_len = max_seq_len\n",
    "\n",
    "\t\tself.dropout_layer = nn.Dropout(p=dropout)\n",
    "\t\tpe = torch.zeros(max_seq_len, d_model)                                                       # (max_seq_len, d_model)\n",
    "\t\tposition = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)                      # (max_seq_len, 1)\n",
    "\t\tdiv_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))     # (d_model/2)\n",
    "\t\tpe[:, 0::2] = torch.sin(position * div_term)\n",
    "\t\tpe[:, 1::2] = torch.cos(position * div_term) \n",
    "\t\tpe = pe.unsqueeze(0).transpose(0, 1)                                                         # (max_seq_len, 1, d_model)\n",
    "\t\tself.register_buffer('pe', pe)\n",
    "\n",
    "\tdef forward(self, inp):\n",
    "\t\tinp = inp + self.pe[:inp.size(0), :]                                                         # (n_step, batch_size, d_model)\n",
    "\t\treturn self.dropout_layer(inp)\n",
    "\n",
    "class Transformer_Encoder_Extraction_Layer(nn.Module):\n",
    "\t\"\"\"\n",
    "\tTransformer encoder as described in \"Attention is all you need\".\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, n_enc_layer, embed_size, n_head, intermediate_size, max_seq_len=512, dropout=0.1, **kwargs):\n",
    "\t\tsuper(Transformer_Encoder_Extraction_Layer, self).__init__(**kwargs)\n",
    "\t\tassert embed_size%n_head==0\n",
    "\n",
    "\t\tself.n_enc_layer = n_enc_layer\n",
    "\t\tself.embed_size = embed_size\n",
    "\t\tself.max_seq_len = max_seq_len\n",
    "\t\tself.n_head = n_head\n",
    "\t\tself.intermediate_size = intermediate_size\n",
    "\t\tself.dropout = dropout\n",
    "\n",
    "\t\tself.positional_encoder = Positional_Encoding_Layer(embed_size, max_seq_len=max_seq_len)\n",
    "\t\ttransformer_encoder_layer = TransformerEncoderLayer(embed_size, n_head, dim_feedforward=intermediate_size, dropout=dropout)\n",
    "\t\tself.transformer_encoder = TransformerEncoder(transformer_encoder_layer, n_enc_layer)\n",
    "\n",
    "\t\tself._init_weights()\n",
    "\n",
    "\tdef _init_weights(self):\n",
    "\t\tfor p in self.parameters():\n",
    "\t\t\tif p.dim() > 1:\n",
    "\t\t\t\txavier_uniform_(p)\n",
    "\n",
    "\tdef forward(self, inp, inp_padding_mask=None):\n",
    "\t\tinp = inp * np.sqrt(self.embed_size)                                           # (batch_size, n_step, embed_size)\n",
    "\t\tinp = self.positional_encoder(inp.permute(1, 0, 2))                            # (n_step, batch_size, embed_size)\n",
    "\t\tout = self.transformer_encoder(inp, src_key_padding_mask=inp_padding_mask)     # (n_step, batch_size, embed_size)\n",
    "\t\treturn out.permute(1, 0, 2)\n",
    "\n",
    "class MLP_Classification_Layer(nn.Module):\n",
    "\t\"\"\"\n",
    "\tMultilayer Perception Classification Layer\n",
    "\t- Layer 1: Linear + Batchnorm + ReLU + Dropout\n",
    "\t- Layer 2: Linear + Batchnorm + ReLU + Dropout\n",
    "\t- Layer 3: Linear\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, inp_size, out_size, dropout=0.4, **kwargs):\n",
    "\t\tsuper(MLP_Classification_Layer, self).__init__(**kwargs)\n",
    "\t\tself.inp_size = inp_size\n",
    "\t\tself.out_size = out_size\n",
    "\t\tself.dropout = dropout\n",
    "\t\t\n",
    "\t\tself.mlp_1 = nn.Linear(inp_size, 1024)\n",
    "\t\tself.batchnorm_1 = nn.BatchNorm1d(1024)\n",
    "\t\tself.mlp_dropout_1 = nn.Dropout(p=dropout)\n",
    "\t\tself.mlp_2 = nn.Linear(1024, 512)\n",
    "\t\tself.batchnorm_2 = nn.BatchNorm1d(512)\n",
    "\t\tself.mlp_dropout_2 = nn.Dropout(p=dropout)\n",
    "\t\tself.mlp_3 = nn.Linear(512, out_size)\n",
    "\t\n",
    "\tdef _init_weights(self):\n",
    "\t\tinitrange = 0.1\n",
    "\t\tself.mlp_1.weight.data.uniform_(-initrange, initrange)\n",
    "\t\tself.mlp_1.bias.data.zero_()\n",
    "\t\tself.mlp_2.weight.data.uniform_(-initrange, initrange)\n",
    "\t\tself.mlp_2.bias.data.zero_()\n",
    "\t\tself.mlp_3.weight.data.uniform_(-initrange, initrange)\n",
    "\t\tself.mlp_3.bias.data.zero_()\n",
    "\n",
    "\tdef forward(self, inp):\n",
    "\t\tmlp_out = self.mlp_1(inp)                                                         # (batch_size, 1024)\n",
    "\t\tmlp_out = self.mlp_dropout_1(F.relu(self.batchnorm_1(mlp_out)))                   # (batch_size, 1024)\n",
    "\t\tmlp_out = self.mlp_2(mlp_out)                                                     # (batch_size, 512)\n",
    "\t\tmlp_out = self.mlp_dropout_2(F.relu(self.batchnorm_2(mlp_out)))                   # (batch_size, 512)\n",
    "\t\tmlp_out = self.mlp_3(mlp_out)                                                     # (batch_size, out_size)\n",
    "\t\treturn mlp_out   \n",
    "\t\n",
    "class Transformer_Encoder_Classifier(nn.Module):\n",
    "\t\"\"\"\n",
    "\tTransformer Encoder + Multilayer Perception for Classification\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, embed_size, out_size, n_enc_layer, n_head, intermediate_size, device, transformer_dropout=0.1, mlp_dropout=0.4, **kwargs):\n",
    "\t\tsuper(Transformer_Encoder_Classifier, self).__init__(**kwargs)\n",
    "\t\t\n",
    "\t\tself.embed_size = embed_size\n",
    "\t\tself.out_size = out_size\n",
    "\t\tself.n_enc_layer = n_enc_layer\n",
    "\t\tself.n_head = n_head\n",
    "\t\tself.intermediate_size = intermediate_size\n",
    "\t\tself.device = device\n",
    "\t\tself.transformer_dropout = transformer_dropout\n",
    "\t\tself.mlp_dropout = mlp_dropout\n",
    "\n",
    "\t\tself.encoder_layer = Transformer_Encoder_Extraction_Layer(n_enc_layer, embed_size, n_head, intermediate_size, dropout=transformer_dropout)\n",
    "\t\tself.classification_layer = MLP_Classification_Layer(embed_size, out_size, dropout=mlp_dropout)\n",
    "\n",
    "\tdef get_padding_mask(self, batch_size, seq_len, inp_last_idx):\n",
    "\t\tpadding_mask = np.ones((batch_size, seq_len))\n",
    "\t\tfor index, last_idx in enumerate(inp_last_idx):\n",
    "\t\t\tpadding_mask[index,:last_idx+1] = 0\n",
    "\t\treturn torch.from_numpy(padding_mask).bool().to(self.device)\n",
    "\n",
    "\tdef forward(self, inp_embed, inp_last_idx):\n",
    "\t\tassert inp_embed.shape[0] == inp_last_idx.shape[0]\n",
    "\t\tbatch_size = inp_embed.shape[0]\n",
    "\t\tseq_len = inp_embed.shape[1]\n",
    "\t\tinp_padding_mask = self.get_padding_mask(batch_size, seq_len, inp_last_idx)\n",
    "\t\tout = self.encoder_layer(inp_embed, inp_padding_mask=inp_padding_mask)               # (batch_size, n_step, embed_size)\n",
    "\t\tpooled_buf = []\n",
    "\t\tfor index, last_idx in enumerate(inp_last_idx):\n",
    "\t\t\tpooled_buf.append(torch.max(out[index,:last_idx+1,:], dim=0)[0])\n",
    "\t\tout = torch.stack(pooled_buf)                                                        # (batch_size, embed_size)\n",
    "\t\tout = self.classification_layer(out)                                                 # (batch_size, out_size)\n",
    "\t\treturn out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer_Encoder_Classifier(256, 2, 6, 8, 2048, torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.random.standard_normal((100,6,256))\n",
    "inp = torch.from_numpy(np.concatenate([p, np.zeros((100, 4, 256))], axis=1)).float()\n",
    "inp_last_idx = np.array([5 for _ in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.4194e-01,  2.8054e-01],\n",
       "        [ 2.4362e-02,  8.0539e-01],\n",
       "        [-9.2631e-01,  2.7887e-01],\n",
       "        [-8.7398e-01, -2.1272e-01],\n",
       "        [-5.1911e-01,  6.4198e-01],\n",
       "        [ 3.7743e-01,  4.2349e-01],\n",
       "        [ 3.1638e-02,  1.1861e-01],\n",
       "        [-3.1806e-01,  2.7993e-01],\n",
       "        [ 8.0730e-01, -7.3080e-01],\n",
       "        [-5.1810e-01, -3.3092e-01],\n",
       "        [-1.1688e-02,  3.2414e-01],\n",
       "        [-1.6236e-01, -5.7130e-01],\n",
       "        [ 5.4187e-01, -7.0140e-01],\n",
       "        [-1.1773e+00, -8.6084e-02],\n",
       "        [-2.9260e-01,  1.8474e-01],\n",
       "        [ 5.6447e-01,  3.4316e-01],\n",
       "        [-1.6931e-01,  6.9761e-01],\n",
       "        [-5.2122e-01, -5.3940e-01],\n",
       "        [ 1.4284e+00, -3.9874e-02],\n",
       "        [-5.8163e-01, -1.2760e-01],\n",
       "        [ 1.0157e+00, -6.7378e-01],\n",
       "        [ 9.3978e-01, -4.7628e-01],\n",
       "        [ 2.1258e-01,  1.0124e+00],\n",
       "        [ 6.5808e-01, -3.0539e-02],\n",
       "        [ 3.1233e-01,  4.3065e-01],\n",
       "        [ 7.2707e-01,  1.5000e-01],\n",
       "        [ 4.3200e-01, -3.0156e-01],\n",
       "        [ 2.6709e-01,  6.5453e-02],\n",
       "        [-1.7455e-01,  1.9606e-01],\n",
       "        [ 1.1335e-01,  3.8149e-02],\n",
       "        [-1.1850e+00, -3.4550e-01],\n",
       "        [-7.2001e-01,  2.8458e-01],\n",
       "        [-3.2293e-01,  9.9903e-02],\n",
       "        [ 8.1855e-01,  3.1094e-01],\n",
       "        [ 7.2461e-01, -2.7705e-01],\n",
       "        [-4.2282e-01, -7.4102e-01],\n",
       "        [ 5.0382e-02,  5.0233e-01],\n",
       "        [-1.0803e+00,  6.2855e-01],\n",
       "        [-1.3559e-01, -4.2934e-01],\n",
       "        [ 7.8882e-01,  2.2693e-01],\n",
       "        [-7.3979e-02,  6.9879e-01],\n",
       "        [-1.4144e-01,  2.0818e-01],\n",
       "        [-1.4939e-01,  4.3142e-01],\n",
       "        [ 8.5547e-01, -8.2508e-01],\n",
       "        [-8.9211e-01, -4.4892e-01],\n",
       "        [-6.1936e-01, -2.0382e-03],\n",
       "        [-5.2739e-01,  6.2501e-01],\n",
       "        [-3.9158e-01,  1.5002e+00],\n",
       "        [-3.9382e-01,  7.5856e-01],\n",
       "        [ 4.3900e-01, -9.9002e-01],\n",
       "        [-3.3491e-01, -7.2632e-01],\n",
       "        [-4.9083e-02,  3.3897e-01],\n",
       "        [-4.2101e-01, -3.9805e-02],\n",
       "        [ 8.6413e-01,  5.7303e-01],\n",
       "        [-7.4531e-01, -1.6361e-01],\n",
       "        [-2.5259e-01,  2.7169e-01],\n",
       "        [ 3.9171e-01,  3.6578e-01],\n",
       "        [ 1.1217e-02, -3.1801e-01],\n",
       "        [-3.5138e-01, -5.6338e-01],\n",
       "        [-2.1765e-01, -1.2327e-01],\n",
       "        [-2.7186e-01,  3.7519e-01],\n",
       "        [ 1.1725e-01, -7.6917e-01],\n",
       "        [-1.8453e-01, -1.1196e-02],\n",
       "        [-1.5025e-01, -1.0848e-01],\n",
       "        [ 2.7590e-02, -4.8879e-01],\n",
       "        [-3.1410e-01, -1.7657e-01],\n",
       "        [-4.3743e-01,  5.9888e-01],\n",
       "        [-1.5965e-01, -6.8947e-02],\n",
       "        [ 1.8249e-01,  2.5539e-01],\n",
       "        [ 2.5776e-01,  1.6117e-01],\n",
       "        [-6.0152e-01, -2.4985e-01],\n",
       "        [ 6.4154e-04, -7.3043e-01],\n",
       "        [-7.4518e-01, -3.8710e-01],\n",
       "        [ 6.0488e-01, -8.1537e-02],\n",
       "        [ 2.5057e-01,  7.7861e-01],\n",
       "        [ 1.8404e-01, -2.7101e-01],\n",
       "        [-4.1052e-01,  4.9542e-01],\n",
       "        [ 5.8092e-01,  1.2579e+00],\n",
       "        [ 2.3286e-01, -1.0135e+00],\n",
       "        [ 9.3409e-01,  1.8422e-01],\n",
       "        [-2.9442e-01,  2.4716e-01],\n",
       "        [-5.1138e-01,  2.8391e-01],\n",
       "        [ 2.4200e-01, -4.0748e-01],\n",
       "        [ 1.2815e+00,  9.8498e-03],\n",
       "        [-2.8333e-01,  3.9535e-02],\n",
       "        [-6.5507e-01,  4.3177e-01],\n",
       "        [-5.9795e-01, -1.5498e-01],\n",
       "        [ 3.9184e-01,  2.3106e-01],\n",
       "        [-3.9720e-01,  1.1532e-01],\n",
       "        [-1.8655e-01,  1.8814e-01],\n",
       "        [-1.7116e-01,  7.8538e-01],\n",
       "        [ 3.5339e-01, -1.3229e-01],\n",
       "        [-9.9309e-01, -8.8625e-01],\n",
       "        [ 3.4673e-01,  2.2003e-02],\n",
       "        [ 4.0951e-01, -2.7282e-01],\n",
       "        [-3.4663e-01, -1.3855e+00],\n",
       "        [-3.3456e-01,  1.7942e-01],\n",
       "        [-5.7132e-01, -3.9619e-02],\n",
       "        [ 4.4180e-01,  4.8553e-01],\n",
       "        [ 5.7512e-02,  7.0541e-01]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(inp, inp_last_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ones(1,10).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(1,10).squeeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 20])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([torch.mean(torch.ones(1,10,20), dim=1).squeeze(0), torch.mean(torch.ones(1,10,20), dim=1).squeeze(0)]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0, 10, dtype=torch.float).unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 13\n",
    "torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class Positional_Encoding_Layer(nn.Module):\n",
    "\t\"\"\"\n",
    "\tPositional encoding using sine and cosine as described in \"Attention is all you need\".\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, d_model, max_seq_len=128, dropout=0.1):\n",
    "\t\t\"\"\"\n",
    "\t\tFormula:\n",
    "\t\t| PE(pos,2i) = sin(pos/10000**(2*i/d_model))\n",
    "\t\t| PE(pos,2i+1) = cos(pos/10000**(2*i/d_model))\n",
    "\t\t\"\"\"\n",
    "\t\tsuper(Positional_Encoding_Layer, self).__init__()\n",
    "\t\tself.d_model = d_model\n",
    "\t\tself.dropout = dropout\n",
    "\t\tself.max_seq_len = max_seq_len\n",
    "\n",
    "\t\tself.dropout_layer = nn.Dropout(p=dropout)\n",
    "\t\tpe = torch.zeros(max_seq_len, d_model)                                                       # (max_seq_len, d_model)\n",
    "\t\tposition = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)                      # (max_seq_len, 1)\n",
    "\t\tdiv_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))     # (d_model/2)\n",
    "\t\tpe[:, 0::2] = torch.sin(position * div_term)\n",
    "\t\tpe[:, 1::2] = torch.cos(position * div_term) \n",
    "\t\tpe = pe.unsqueeze(0).transpose(0, 1)                                                         # (max_seq_len, 1, d_model)\n",
    "\t\tself.register_buffer('pe', pe)\n",
    "\n",
    "\tdef forward(self, inp):\n",
    "\t\tinp = inp + self.pe[:inp.size(0), :]                                                         # (n_step, batch_size, d_model)\n",
    "\t\treturn self.dropout_layer(inp)\n",
    "\n",
    "class Transformer_Encoder_Extraction_Layer(nn.Module):\n",
    "\t\"\"\"\n",
    "\tTransformer encoder as described in \"Attention is all you need\".\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, n_enc_layer, embed_size, n_head, intermediate_size, max_seq_len=512, dropout=0.1, **kwargs):\n",
    "\t\tsuper(Transformer_Encoder_Extraction_Layer, self).__init__(**kwargs)\n",
    "\t\tassert embed_size%n_head==0\n",
    "\n",
    "\t\tself.n_enc_layer = n_enc_layer\n",
    "\t\tself.embed_size = embed_size\n",
    "\t\tself.max_seq_len = max_seq_len\n",
    "\t\tself.n_head = n_head\n",
    "\t\tself.intermediate_size = intermediate_size\n",
    "\t\tself.dropout = dropout\n",
    "\n",
    "\t\tself.positional_encoder = Positional_Encoding_Layer(embed_size, max_seq_len=max_seq_len)\n",
    "\t\ttransformer_encoder_layer = TransformerEncoderLayer(embed_size, n_head, dim_feedforward=intermediate_size, dropout=dropout)\n",
    "\t\tself.transformer_encoder = TransformerEncoder(transformer_encoder_layer, n_enc_layer)\n",
    "\n",
    "\t\tself._init_weights()\n",
    "\n",
    "\tdef _init_weights(self):\n",
    "\t\tfor p in self.parameters():\n",
    "\t\t\tif p.dim() > 1:\n",
    "\t\t\t\txavier_uniform_(p)\n",
    "\n",
    "\tdef forward(self, inp, inp_padding_mask=None):\n",
    "\t\tinp = inp * np.sqrt(self.embed_size)                                           # (batch_size, n_step, embed_size)\n",
    "\t\tinp = self.positional_encoder(inp.permute(1, 0, 2))                            # (n_step, batch_size, embed_size)\n",
    "\t\tout = self.transformer_encoder(inp, src_key_padding_mask=inp_padding_mask)     # (n_step, batch_size, embed_size)\n",
    "\t\treturn out.permute(1, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_torch_module_num_of_parameter(model):\n",
    "\t\"\"\"\n",
    "\tGet # of parameters in a torch module.\n",
    "\t\"\"\"\n",
    "\tmodel_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "\tparams = sum([np.prod(p.size()) for p in model_parameters])\n",
    "\treturn params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7890432"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer_Encoder_Extraction_Layer(6, 256, 8, 2048)\n",
    "get_torch_module_num_of_parameter(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.random.standard_normal((100,6,256))\n",
    "inp_1 = torch.from_numpy(np.concatenate([p, np.zeros((100, 4, 256))], axis=1)).float()\n",
    "inp_2 = torch.from_numpy(np.concatenate([p, np.ones((100, 4, 256))], axis=1)).float()\n",
    "pad_mask = torch.from_numpy(np.concatenate([np.zeros((100, 6)), np.ones((100,4))], axis=1)).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padding_mask(batch_size, seq_len, inp_last_idx):\n",
    "    padding_mask = np.ones((batch_size, seq_len))\n",
    "    for index, last_idx in enumerate(inp_last_idx):\n",
    "        padding_mask[index,:last_idx+1] = 0\n",
    "    return torch.from_numpy(padding_mask).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False,  True,  True]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_padding_mask(3,10,[5,6,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "out_1 = model(inp_1, inp_padding_mask=pad_mask)\n",
    "out_2 = model(inp_2, inp_padding_mask=pad_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = BertConfig(vocab_size=1, hidden_size=4, num_hidden_layers=8, num_attention_heads=2, intermediate_size=2048)\n",
    "model = BertModel(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.from_numpy(np.concatenate([np.random.standard_normal((1, 5, 4)), np.zeros((1, 5, 4))], axis=1)).float()\n",
    "mask = torch.from_numpy(np.concatenate([np.zeros((1, 5)), np.zeros((1, 5))], axis=1)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4829,  0.3867,  1.2085,  1.0879],\n",
       "         [-0.4869, -2.1700,  0.7577, -0.3682],\n",
       "         [-1.4087, -0.4176, -0.6754,  0.0763],\n",
       "         [-0.1583, -0.2029,  0.6745, -0.6823],\n",
       "         [ 0.4552,  0.5747, -0.3314, -1.9143],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000]]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "out= model(inputs_embeds=inp, attention_mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 4])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.5516, -0.2037,  0.9508,  0.8045],\n",
       "         [ 0.1553, -1.5379,  1.2636,  0.1190],\n",
       "         [-0.6950,  0.0802, -0.9813,  1.5961],\n",
       "         [-0.0377, -0.2082,  1.5238, -1.2779],\n",
       "         [ 0.7606,  0.8213,  0.0738, -1.6557],\n",
       "         [ 0.2063,  0.3378,  1.0872, -1.6313],\n",
       "         [ 0.4727,  0.4431,  0.7993, -1.7151],\n",
       "         [ 0.6379,  0.8671,  0.1718, -1.6768],\n",
       "         [ 0.5959,  0.3670,  0.7528, -1.7157],\n",
       "         [ 1.2472,  0.6829, -0.7249, -1.2052]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with torch padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AlbertModel, AlbertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = AlbertConfig()\n",
    "model = AlbertModel(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222595584"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_torch_module_num_of_parameter(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.from_numpy(np.concatenate([np.ones((10, 5, 128)), np.zeros((10, 5, 128))], axis=1)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "out= model(inputs_embeds=inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10, 4096])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.0415586 , -0.700448  , -0.23963146, ..., -0.34111258,\n",
       "        -0.88970816,  2.0453863 ],\n",
       "       [-1.0419453 , -0.7001726 , -0.24012905, ..., -0.34073353,\n",
       "        -0.8900057 ,  2.0456576 ],\n",
       "       [-1.0417546 , -0.7000878 , -0.24050954, ..., -0.34136513,\n",
       "        -0.8902673 ,  2.0448096 ],\n",
       "       ...,\n",
       "       [-1.0415502 , -0.70019853, -0.23981182, ..., -0.34107703,\n",
       "        -0.8901159 ,  2.0457187 ],\n",
       "       [-1.0413502 , -0.6995464 , -0.240383  , ..., -0.34042382,\n",
       "        -0.8905616 ,  2.0456984 ],\n",
       "       [-1.0413843 , -0.70032   , -0.23944165, ..., -0.34181798,\n",
       "        -0.89004594,  2.0447192 ]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0][1].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
