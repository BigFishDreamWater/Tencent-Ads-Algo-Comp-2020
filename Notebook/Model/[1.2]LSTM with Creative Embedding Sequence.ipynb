{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "train_path = '../../raw_train_artifact'\n",
    "test_path = '../../raw_test_artifact'\n",
    "embedding_path = '../../embedding_artifact'\n",
    "input_path = '../../input_artifact'\n",
    "input_split_path = '../../input_artifact/input_split'\n",
    "model_path = '../../model_artifact'\n",
    "output_path = '../../output_artifact'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gc\n",
    "gc.enable()\n",
    "import time\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns',120)\n",
    "pd.set_option('display.max_rows',2000)\n",
    "pd.set_option('precision',5)\n",
    "pd.set_option('float_format', '{:.5f}'.format)\n",
    "\n",
    "import tqdm\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from gensim.models import Word2Vec\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:37:50 INFO: Restart notebook\n",
      "==========================\n",
      "Wed Jun  3 08:37:50 2020\n",
      "==========================\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "log_path = '[1.2]LSTM with Creative Embedding Sequence.log'\n",
    "    \n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)-s: %(message)s', datefmt='%H:%M:%S')\n",
    "\n",
    "fh = logging.FileHandler(log_path)\n",
    "fh.setLevel(logging.INFO)\n",
    "fh.setFormatter(formatter)\n",
    "logger.addHandler(fh)\n",
    "\n",
    "sh = logging.StreamHandler(sys.stdout)\n",
    "sh.setLevel(logging.INFO)\n",
    "sh.setFormatter(formatter)\n",
    "logger.addHandler(sh)\n",
    "\n",
    "logger.info(f'Restart notebook\\n==========================\\n{time.ctime()}\\n==========================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:37:50 INFO: Device in Use: cuda\n",
      "08:37:50 INFO: CUDA Memory: Total 8.00 GB, Cached 0.00 GB, Allocated 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info('Device in Use: {}'.format(DEVICE))\n",
    "torch.cuda.empty_cache()\n",
    "t = torch.cuda.get_device_properties(DEVICE).total_memory/1024**3\n",
    "c = torch.cuda.memory_cached(DEVICE)/1024**3\n",
    "a = torch.cuda.memory_allocated(DEVICE)/1024**3\n",
    "logger.info('CUDA Memory: Total {:.2f} GB, Cached {:.2f} GB, Allocated {:.2f} GB'.format(t,c,a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "creative_embedding_path = r'C:\\JupyterNotebook\\Tencent-Ads-Algo-Comp-2020\\embedding_artifact\\creative_id_embed_s160_w64_cbow_38168zon'\n",
    "\n",
    "def prepare_data(split_id, max_seq=100, slient=False, logger=None):\n",
    "    global input_split_path, creative_embedding_path\n",
    "\n",
    "    start = time.time()\n",
    "    if not slient and logger: logger.info(f'Processing Split-{split_id}')\n",
    "    truth_path = os.path.join(input_split_path, f'train_truth_{split_id}.npy')\n",
    "    with open(truth_path, 'rb') as f:\n",
    "        truth = np.load(f)\n",
    "    inp_user = truth[:,0]\n",
    "    out_age = torch.from_numpy(truth[:,1]).long()\n",
    "    out_gender = torch.from_numpy(truth[:,2]).long()\n",
    "    if not slient and logger: logger.info(f'Target output ready after {time.time()-start:.2f}s')\n",
    "    del truth\n",
    "    _ = gc.collect()\n",
    "    \n",
    "    creative_embedding = Word2Vec.load(creative_embedding_path) \n",
    "    if not slient and logger: logger.info(f'Creative ID embedding artifact is loaded after {time.time()-start:.2f}s')\n",
    "    creative_path = os.path.join(input_split_path, f'train_creative_agg_user_{split_id}.json')\n",
    "    with open(creative_path, 'r') as f:\n",
    "        creative = json.load(f)\n",
    "    inp_creative = []\n",
    "    for user in inp_user:\n",
    "        inp_creative.append(torch.from_numpy(np.stack([creative_embedding.wv[key] for key in creative[str(user)][:max_seq]], axis=0)).float())\n",
    "    inp_last_idx = np.array([i.shape[0] for i in inp_creative])-1\n",
    "    if not slient and logger: logger.info(f'Creative embedding ready after {time.time()-start:.2f}s')\n",
    "    del creative_embedding, creative, inp_user\n",
    "    _ = gc.collect()\n",
    "        \n",
    "    return inp_creative, inp_last_idx, out_age, out_gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Classifier(nn.Module):\n",
    "    def __init__(self, embed_size, lstm_hidden_size, out_size, rnn_dropout=0.2, mlp_dropout=0.4, **kwargs):\n",
    "        super(LSTM_Classifier, self).__init__(**kwargs)\n",
    "        self.embed_size = embed_size\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.out_size = out_size\n",
    "        self.rnn_dropout = rnn_dropout\n",
    "        self.mlp_dropout = mlp_dropout\n",
    "        \n",
    "        self.bi_lstm = nn.LSTM(input_size=embed_size, hidden_size=lstm_hidden_size, bias=True, bidirectional=True)\n",
    "        self.rnn_dropout_1 = nn.Dropout(p=rnn_dropout)\n",
    "        self.layernorm_1 = nn.LayerNorm(2*lstm_hidden_size)\n",
    "        self.lstm_1 = nn.LSTM(input_size=2*lstm_hidden_size, hidden_size=2*lstm_hidden_size)\n",
    "        self.rnn_dropout_2 = nn.Dropout(p=rnn_dropout)\n",
    "        self.layernorm_2 = nn.LayerNorm(2*lstm_hidden_size)\n",
    "        self.lstm_2 = nn.LSTM(input_size=2*lstm_hidden_size, hidden_size=2*lstm_hidden_size)\n",
    "        self.batchnorm_1 = nn.BatchNorm1d(2*lstm_hidden_size)\n",
    "        self.mlp_dropout_1 = nn.Dropout(p=mlp_dropout)\n",
    "        self.mlp_1 = nn.Linear(2*lstm_hidden_size, 1024)\n",
    "        self.batchnorm_2 = nn.BatchNorm1d(1024)\n",
    "        self.mlp_dropout_2 = nn.Dropout(p=mlp_dropout)\n",
    "        self.mlp_2 = nn.Linear(1024, 512)\n",
    "        self.batchnorm_3 = nn.BatchNorm1d(512)\n",
    "        self.mlp_dropout_3 = nn.Dropout(p=mlp_dropout)\n",
    "        self.mlp_3 = nn.Linear(512, out_size)\n",
    "        \n",
    "    def forward(self, inp_embed, inp_last_idx):\n",
    "        bilstm_out, _ = self.bi_lstm(inp_embed.permute(1,0,2))                            # (max_seq_length, batch_size, embed_size) -> (max_seq_length, batch_size, 2*lstm_hidden_size)\n",
    "        bilstm_out = self.rnn_dropout_1(bilstm_out)                                       # (max_seq_length, batch_size, 2*lstm_hidden_size)\n",
    "        bilstm_out = self.layernorm_1(bilstm_out)                                         # (max_seq_length, batch_size, 2*lstm_hidden_size)\n",
    "        lstm_out, _ = self.lstm_1(bilstm_out)                                             # (max_seq_length, batch_size, 2*lstm_hidden_size)\n",
    "        lstm_out = self.rnn_dropout_2(lstm_out)                                           # (max_seq_length, batch_size, 2*lstm_hidden_size)\n",
    "        lstm_out = self.layernorm_2(lstm_out+bilstm_out)                                  # (max_seq_length, batch_size, 2*lstm_hidden_size)\n",
    "        lstm_out, _ = self.lstm_2(lstm_out)                                               # (max_seq_length, batch_size, 2*lstm_hidden_size)\n",
    "        lstm_out = lstm_out.permute(1,0,2)[np.arange(len(inp_last_idx)), inp_last_idx,:]  # (batch_size, 2*lstm_hidden_size)\n",
    "        lstm_out = self.mlp_dropout_1(F.relu(self.batchnorm_1(lstm_out)))                 # (batch_size, 2*lstm_hidden_size)\n",
    "        mlp_out = self.mlp_1(lstm_out)                                                    # (batch_size, 1024)\n",
    "        mlp_out = self.mlp_dropout_2(F.relu(self.batchnorm_2(mlp_out)))                   # (batch_size, 1024)\n",
    "        mlp_out = self.mlp_2(mlp_out)                                                     # (batch_size, 512)\n",
    "        mlp_out = self.mlp_dropout_3(F.relu(self.batchnorm_3(mlp_out)))                   # (batch_size, 512)\n",
    "        mlp_out = self.mlp_3(mlp_out)                                                     # (batch_size, out_size)\n",
    "        return mlp_out   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gender Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHES = 5\n",
    "BATCH_SIZE = 512\n",
    "N_BATCH = 90000//BATCH_SIZE-1\n",
    "TEST_SIZE = 90000%BATCH_SIZE + BATCH_SIZE\n",
    "\n",
    "def train_gender(model, loss_fn, optimizer, device, checkpoint_dir, checkpoint_prefix, logger=None, epoch_start=0):\n",
    "    global EPOCHES, BATCH_SIZE, N_BATCH, TEST_SIZE\n",
    "    if not os.path.isdir(checkpoint_dir):\n",
    "        os.mkdir(checkpoint_dir)\n",
    "    \n",
    "    for epoch in range(1+epoch_start, EPOCHES+1+epoch_start):\n",
    "        if logger: \n",
    "            logger.info('=========================')\n",
    "            logger.info(f'Processing Epoch {epoch}/{EPOCHES+epoch_start}')\n",
    "            logger.info('=========================')\n",
    "            \n",
    "        train_file = [1,2,3,4,5,6,7,8,9]\n",
    "        test_file = [10]\n",
    "            \n",
    "        train_running_loss, train_n_batch = 0, 0\n",
    "        pred_y, true_y = [], []\n",
    "        for index, split_id in enumerate(train_file, start=1):\n",
    "            inp_creative, inp_last_idx, out_age, out_gender = prepare_data(split_id)\n",
    "            train_creative, test_creative = inp_creative[:-TEST_SIZE], inp_creative[-TEST_SIZE:]\n",
    "            train_last_idx, test_last_idx = inp_last_idx[:-TEST_SIZE], inp_last_idx[-TEST_SIZE:]\n",
    "            train_gender, test_gender = out_gender[:-TEST_SIZE], out_gender[-TEST_SIZE:]\n",
    "            \n",
    "            model.train()\n",
    "            \n",
    "            for batch_index in range(N_BATCH):\n",
    "                x1 = torch.nn.utils.rnn.pad_sequence(train_creative[batch_index*BATCH_SIZE:(batch_index+1)*BATCH_SIZE], batch_first=True, padding_value=0).to(device)\n",
    "                x2 = train_last_idx[batch_index*BATCH_SIZE:(batch_index+1)*BATCH_SIZE]\n",
    "                y = train_gender[batch_index*BATCH_SIZE:(batch_index+1)*BATCH_SIZE].to(device)\n",
    "                optimizer.zero_grad()\n",
    "                yp = F.softmax(model(x1, x2), 1)\n",
    "                loss = loss_fn(yp, y)\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=100)\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_running_loss += loss.item()\n",
    "                train_n_batch += 1\n",
    "                \n",
    "                del x1, x2, y, yp\n",
    "                _ = gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            model.eval()\n",
    "            \n",
    "            x1 = torch.nn.utils.rnn.pad_sequence(test_creative, batch_first=True, padding_value=0).to(device)\n",
    "            x2 = test_last_idx\n",
    "            y = test_gender.to(device)\n",
    "            yp = F.softmax(model(x1, x2), 1)\n",
    "            loss = loss_fn(yp, y)\n",
    "            \n",
    "            pred_y.extend(list(yp.cpu().detach().numpy()))\n",
    "            true_y.extend(list(y.cpu().detach().numpy()))\n",
    "            \n",
    "            del x1, x2, y, yp\n",
    "            _ = gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            prob = np.array(pred_y)[:,1]\n",
    "            pred = np.argmax(np.array(pred_y), 1)\n",
    "            true = np.array(true_y).reshape((-1,))\n",
    "            roc_score = roc_auc_score(true, prob)\n",
    "            acc_score = accuracy_score(true, pred)\n",
    "            \n",
    "            if logger:\n",
    "                logger.info(f'Epoch {epoch}/{EPOCHES+epoch_start} - Training Split {index}/{len(train_file)} Done - Train Loss: {train_running_loss/train_n_batch:.6f}, Val Loss: {loss.item():.6f}, Val AUC: {roc_score:.6f}, Val Accuracy: {acc_score:.6f}')\n",
    "            \n",
    "            del inp_creative, inp_last_idx, out_age, out_gender, train_creative, test_creative, train_last_idx, test_last_idx, train_gender, test_gender\n",
    "            _ = gc.collect()\n",
    "            torch.cuda.empty_cache()   \n",
    "        \n",
    "        model.eval()\n",
    "        test_running_loss, test_n_batch = 0, 0\n",
    "        true_y, pred_y = [], []\n",
    "        \n",
    "        for index, split_id in enumerate(test_file, start=1):\n",
    "            inp_creative, inp_last_idx, out_age, out_gender = prepare_data(split_id)\n",
    "            for batch_index in range(N_BATCH+2):\n",
    "                x1 = torch.nn.utils.rnn.pad_sequence(inp_creative[batch_index*BATCH_SIZE:(batch_index+1)*BATCH_SIZE], batch_first=True, padding_value=0).to(device)\n",
    "                x2 = inp_last_idx[batch_index*BATCH_SIZE:(batch_index+1)*BATCH_SIZE]\n",
    "                y = out_gender[batch_index*BATCH_SIZE:(batch_index+1)*BATCH_SIZE].to(device)\n",
    "                yp = F.softmax(model(x1, x2), 1)\n",
    "                loss = loss_fn(yp, y)\n",
    "            \n",
    "                test_running_loss += loss.item()\n",
    "                test_n_batch += 1\n",
    "            \n",
    "                pred_y.extend(list(yp.cpu().detach().numpy()))\n",
    "                true_y.extend(list(y.cpu().detach().numpy()))\n",
    "            \n",
    "                del x1, x2, y, yp\n",
    "                _ = gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            del inp_creative, inp_last_idx, out_age, out_gender\n",
    "            _ = gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        prob = np.array(pred_y)[:,1]\n",
    "        pred = np.argmax(np.array(pred_y), 1)\n",
    "        true = np.array(true_y).reshape((-1,))\n",
    "        roc_score = roc_auc_score(true, prob)\n",
    "        acc_score = accuracy_score(true, pred)\n",
    "        \n",
    "        if logger:\n",
    "            logger.info(f'Epoch {epoch}/{EPOCHES+epoch_start} Done - Test Loss: {test_running_loss/test_n_batch:.6f}, Test AUC: {roc_score:.6f}, Test Accuracy: {acc_score:.6f}')\n",
    "            \n",
    "        ck_file_name = f'{checkpoint_prefix}_{epoch}.pth'\n",
    "        ck_file_path = os.path.join(checkpoint_dir, ck_file_name)\n",
    "        \n",
    "        torch.save(model.state_dict(), ck_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21:21:42 INFO: =========================\n",
      "21:21:42 INFO: Processing Epoch 1/5\n",
      "21:21:42 INFO: =========================\n",
      "21:23:53 INFO: Epoch 1/5 - Training Split 1/9 Done - Train Loss: 0.411199, Val Loss: 0.379032, Val AUC: 0.971454, Val Accuracy: 0.929825\n",
      "21:26:07 INFO: Epoch 1/5 - Training Split 2/9 Done - Train Loss: 0.401921, Val Loss: 0.402942, Val AUC: 0.963301, Val Accuracy: 0.917215\n",
      "21:28:22 INFO: Epoch 1/5 - Training Split 3/9 Done - Train Loss: 0.396658, Val Loss: 0.387113, Val AUC: 0.965022, Val Accuracy: 0.919225\n",
      "21:30:35 INFO: Epoch 1/5 - Training Split 4/9 Done - Train Loss: 0.392903, Val Loss: 0.371584, Val AUC: 0.968793, Val Accuracy: 0.924068\n",
      "21:32:46 INFO: Epoch 1/5 - Training Split 5/9 Done - Train Loss: 0.390318, Val Loss: 0.380074, Val AUC: 0.968102, Val Accuracy: 0.924781\n",
      "21:34:58 INFO: Epoch 1/5 - Training Split 6/9 Done - Train Loss: 0.388471, Val Loss: 0.377876, Val AUC: 0.968840, Val Accuracy: 0.926352\n",
      "21:37:11 INFO: Epoch 1/5 - Training Split 7/9 Done - Train Loss: 0.387017, Val Loss: 0.377145, Val AUC: 0.967544, Val Accuracy: 0.927475\n",
      "21:39:24 INFO: Epoch 1/5 - Training Split 8/9 Done - Train Loss: 0.385867, Val Loss: 0.375021, Val AUC: 0.966530, Val Accuracy: 0.928865\n",
      "21:41:36 INFO: Epoch 1/5 - Training Split 9/9 Done - Train Loss: 0.384892, Val Loss: 0.375259, Val AUC: 0.967639, Val Accuracy: 0.929337\n",
      "21:42:50 INFO: Epoch 1/5 Done - Test Loss: 0.376814, Test AUC: 0.972552, Test Accuracy: 0.934011\n",
      "21:42:50 INFO: =========================\n",
      "21:42:50 INFO: Processing Epoch 2/5\n",
      "21:42:50 INFO: =========================\n",
      "21:44:58 INFO: Epoch 2/5 - Training Split 1/9 Done - Train Loss: 0.377377, Val Loss: 0.371034, Val AUC: 0.975386, Val Accuracy: 0.938596\n",
      "21:47:10 INFO: Epoch 2/5 - Training Split 2/9 Done - Train Loss: 0.377170, Val Loss: 0.390131, Val AUC: 0.963474, Val Accuracy: 0.929825\n",
      "21:49:21 INFO: Epoch 2/5 - Training Split 3/9 Done - Train Loss: 0.376780, Val Loss: 0.372843, Val AUC: 0.967721, Val Accuracy: 0.931652\n",
      "21:51:31 INFO: Epoch 2/5 - Training Split 4/9 Done - Train Loss: 0.376171, Val Loss: 0.368511, Val AUC: 0.970731, Val Accuracy: 0.933936\n",
      "21:53:41 INFO: Epoch 2/5 - Training Split 5/9 Done - Train Loss: 0.375892, Val Loss: 0.376598, Val AUC: 0.970508, Val Accuracy: 0.934430\n",
      "21:55:52 INFO: Epoch 2/5 - Training Split 6/9 Done - Train Loss: 0.375552, Val Loss: 0.376898, Val AUC: 0.970558, Val Accuracy: 0.934576\n",
      "21:58:01 INFO: Epoch 2/5 - Training Split 7/9 Done - Train Loss: 0.375395, Val Loss: 0.379547, Val AUC: 0.970071, Val Accuracy: 0.934211\n",
      "22:00:11 INFO: Epoch 2/5 - Training Split 8/9 Done - Train Loss: 0.375212, Val Loss: 0.371123, Val AUC: 0.969106, Val Accuracy: 0.935170\n",
      "22:02:22 INFO: Epoch 2/5 - Training Split 9/9 Done - Train Loss: 0.375022, Val Loss: 0.372207, Val AUC: 0.969907, Val Accuracy: 0.935063\n",
      "22:03:37 INFO: Epoch 2/5 Done - Test Loss: 0.374555, Test AUC: 0.971980, Test Accuracy: 0.935800\n",
      "22:03:37 INFO: =========================\n",
      "22:03:37 INFO: Processing Epoch 3/5\n",
      "22:03:37 INFO: =========================\n",
      "22:05:48 INFO: Epoch 3/5 - Training Split 1/9 Done - Train Loss: 0.373379, Val Loss: 0.369779, Val AUC: 0.974639, Val Accuracy: 0.941886\n",
      "22:07:58 INFO: Epoch 3/5 - Training Split 2/9 Done - Train Loss: 0.373856, Val Loss: 0.393462, Val AUC: 0.959770, Val Accuracy: 0.929825\n",
      "22:10:07 INFO: Epoch 3/5 - Training Split 3/9 Done - Train Loss: 0.373681, Val Loss: 0.377072, Val AUC: 0.963805, Val Accuracy: 0.930556\n",
      "22:12:18 INFO: Epoch 3/5 - Training Split 4/9 Done - Train Loss: 0.373190, Val Loss: 0.363372, Val AUC: 0.966482, Val Accuracy: 0.934759\n",
      "22:14:30 INFO: Epoch 3/5 - Training Split 5/9 Done - Train Loss: 0.372972, Val Loss: 0.377836, Val AUC: 0.966227, Val Accuracy: 0.934868\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_EXECUTION_FAILED (_cudnn_rnn_backward_input at ..\\aten\\src\\ATen\\native\\cudnn\\RNN.cpp:931)\n(no backtrace available)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-35610118d749>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mcheckpoint_prefix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'LSTM_Classifier'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint_prefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-dc5312267606>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, loss_fn, optimizer, device, checkpoint_dir, checkpoint_prefix, logger, epoch_start)\u001b[0m\n\u001b[0;32m     36\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yifan wu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \"\"\"\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yifan wu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED (_cudnn_rnn_backward_input at ..\\aten\\src\\ATen\\native\\cudnn\\RNN.cpp:931)\n(no backtrace available)"
     ]
    }
   ],
   "source": [
    "model = LSTM_Classifier(160, 256, 2).to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "device = DEVICE\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "checkpoint_dir = os.path.join(model_path, 'LSTM_Classifier_Creative_Gender')\n",
    "checkpoint_prefix = 'LSTM_Classifier_Creative_Gender'\n",
    "\n",
    "train_gender(model, loss_fn, optimizer, device, checkpoint_dir, checkpoint_prefix, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:27:42 INFO: =========================\n",
      "22:27:42 INFO: Processing Epoch 3/7\n",
      "22:27:42 INFO: =========================\n",
      "22:29:54 INFO: Epoch 3/7 - Training Split 1/9 Done - Train Loss: 0.374186, Val Loss: 0.368580, Val AUC: 0.973175, Val Accuracy: 0.941886\n",
      "22:32:08 INFO: Epoch 3/7 - Training Split 2/9 Done - Train Loss: 0.374155, Val Loss: 0.396829, Val AUC: 0.967264, Val Accuracy: 0.927632\n",
      "22:34:23 INFO: Epoch 3/7 - Training Split 3/9 Done - Train Loss: 0.373919, Val Loss: 0.376800, Val AUC: 0.970505, Val Accuracy: 0.929825\n",
      "22:36:40 INFO: Epoch 3/7 - Training Split 4/9 Done - Train Loss: 0.373435, Val Loss: 0.368807, Val AUC: 0.972180, Val Accuracy: 0.933114\n",
      "22:38:54 INFO: Epoch 3/7 - Training Split 5/9 Done - Train Loss: 0.373301, Val Loss: 0.375425, Val AUC: 0.971957, Val Accuracy: 0.933772\n",
      "22:41:06 INFO: Epoch 3/7 - Training Split 6/9 Done - Train Loss: 0.373066, Val Loss: 0.376274, Val AUC: 0.971379, Val Accuracy: 0.933845\n",
      "22:43:19 INFO: Epoch 3/7 - Training Split 7/9 Done - Train Loss: 0.372876, Val Loss: 0.376838, Val AUC: 0.970724, Val Accuracy: 0.933427\n",
      "22:45:32 INFO: Epoch 3/7 - Training Split 8/9 Done - Train Loss: 0.372745, Val Loss: 0.373429, Val AUC: 0.969588, Val Accuracy: 0.933388\n",
      "22:47:47 INFO: Epoch 3/7 - Training Split 9/9 Done - Train Loss: 0.372620, Val Loss: 0.367986, Val AUC: 0.969490, Val Accuracy: 0.934576\n",
      "22:49:04 INFO: Epoch 3/7 Done - Test Loss: 0.374317, Test AUC: 0.969682, Test Accuracy: 0.936322\n",
      "22:49:04 INFO: =========================\n",
      "22:49:04 INFO: Processing Epoch 4/7\n",
      "22:49:04 INFO: =========================\n",
      "22:51:16 INFO: Epoch 4/7 - Training Split 1/9 Done - Train Loss: 0.371197, Val Loss: 0.366608, Val AUC: 0.971120, Val Accuracy: 0.946272\n",
      "22:53:27 INFO: Epoch 4/7 - Training Split 2/9 Done - Train Loss: 0.371717, Val Loss: 0.392243, Val AUC: 0.961415, Val Accuracy: 0.932566\n",
      "22:55:39 INFO: Epoch 4/7 - Training Split 3/9 Done - Train Loss: 0.371524, Val Loss: 0.374772, Val AUC: 0.966302, Val Accuracy: 0.932018\n",
      "22:57:51 INFO: Epoch 4/7 - Training Split 4/9 Done - Train Loss: 0.371120, Val Loss: 0.366071, Val AUC: 0.969229, Val Accuracy: 0.933936\n",
      "23:00:03 INFO: Epoch 4/7 - Training Split 5/9 Done - Train Loss: 0.371122, Val Loss: 0.371525, Val AUC: 0.969061, Val Accuracy: 0.935526\n",
      "23:02:15 INFO: Epoch 4/7 - Training Split 6/9 Done - Train Loss: 0.370906, Val Loss: 0.376715, Val AUC: 0.969416, Val Accuracy: 0.935124\n",
      "23:04:27 INFO: Epoch 4/7 - Training Split 7/9 Done - Train Loss: 0.370724, Val Loss: 0.377842, Val AUC: 0.967956, Val Accuracy: 0.934837\n",
      "23:06:40 INFO: Epoch 4/7 - Training Split 8/9 Done - Train Loss: 0.370652, Val Loss: 0.370586, Val AUC: 0.965746, Val Accuracy: 0.935170\n",
      "23:08:50 INFO: Epoch 4/7 - Training Split 9/9 Done - Train Loss: 0.370538, Val Loss: 0.369951, Val AUC: 0.966507, Val Accuracy: 0.935794\n",
      "23:10:02 INFO: Epoch 4/7 Done - Test Loss: 0.373557, Test AUC: 0.965373, Test Accuracy: 0.937067\n",
      "23:10:02 INFO: =========================\n",
      "23:10:02 INFO: Processing Epoch 5/7\n",
      "23:10:02 INFO: =========================\n",
      "23:12:13 INFO: Epoch 5/7 - Training Split 1/9 Done - Train Loss: 0.369329, Val Loss: 0.368442, Val AUC: 0.969659, Val Accuracy: 0.940789\n",
      "23:14:25 INFO: Epoch 5/7 - Training Split 2/9 Done - Train Loss: 0.369636, Val Loss: 0.390653, Val AUC: 0.963293, Val Accuracy: 0.930373\n",
      "23:16:38 INFO: Epoch 5/7 - Training Split 3/9 Done - Train Loss: 0.369453, Val Loss: 0.373250, Val AUC: 0.963465, Val Accuracy: 0.932383\n",
      "23:18:48 INFO: Epoch 5/7 - Training Split 4/9 Done - Train Loss: 0.369195, Val Loss: 0.360455, Val AUC: 0.965675, Val Accuracy: 0.936129\n",
      "23:20:58 INFO: Epoch 5/7 - Training Split 5/9 Done - Train Loss: 0.369133, Val Loss: 0.375034, Val AUC: 0.965855, Val Accuracy: 0.935746\n",
      "23:23:10 INFO: Epoch 5/7 - Training Split 6/9 Done - Train Loss: 0.369015, Val Loss: 0.376834, Val AUC: 0.967334, Val Accuracy: 0.935490\n",
      "23:25:23 INFO: Epoch 5/7 - Training Split 7/9 Done - Train Loss: 0.368821, Val Loss: 0.376673, Val AUC: 0.966421, Val Accuracy: 0.935307\n",
      "23:27:33 INFO: Epoch 5/7 - Training Split 8/9 Done - Train Loss: 0.368751, Val Loss: 0.375803, Val AUC: 0.963738, Val Accuracy: 0.935444\n",
      "23:29:43 INFO: Epoch 5/7 - Training Split 9/9 Done - Train Loss: 0.368684, Val Loss: 0.367965, Val AUC: 0.964732, Val Accuracy: 0.936038\n",
      "23:30:59 INFO: Epoch 5/7 Done - Test Loss: 0.373080, Test AUC: 0.969174, Test Accuracy: 0.937689\n",
      "23:30:59 INFO: =========================\n",
      "23:30:59 INFO: Processing Epoch 6/7\n",
      "23:30:59 INFO: =========================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-dd5e334692a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mtrain_gender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint_prefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-c36ea2e81a46>\u001b[0m in \u001b[0;36mtrain_gender\u001b[1;34m(model, loss_fn, optimizer, device, checkpoint_dir, checkpoint_prefix, logger, epoch_start)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mpred_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[0minp_creative\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minp_last_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_age\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_gender\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[0mtrain_creative\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_creative\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minp_creative\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mTEST_SIZE\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minp_creative\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mTEST_SIZE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mtrain_last_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_last_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minp_last_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mTEST_SIZE\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minp_last_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mTEST_SIZE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-c933ec1d0e15>\u001b[0m in \u001b[0;36mprepare_data\u001b[1;34m(split_id, max_seq, slient, logger)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mcreative_embedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcreative_embedding_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mslient\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlogger\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Creative ID embedding artifact is loaded after {time.time()-start:.2f}s'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mcreative_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_split_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf'train_creative_agg_user_{split_id}.json'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yifan wu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1139\u001b[0m         \"\"\"\n\u001b[0;32m   1140\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1141\u001b[1;33m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1143\u001b[0m             \u001b[1;31m# for backward compatibility for `max_final_vocab` feature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yifan wu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1229\u001b[0m         \"\"\"\n\u001b[1;32m-> 1230\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseWordEmbeddingsModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1231\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ns_exponent'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1232\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mns_exponent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.75\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yifan wu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(cls, fname_or_handle, **kwargs)\u001b[0m\n\u001b[0;32m    600\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m         \"\"\"\n\u001b[1;32m--> 602\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseAny2VecModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    603\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname_or_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yifan wu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(cls, fname, mmap)\u001b[0m\n\u001b[0;32m    433\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m         \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loaded %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yifan wu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36munpickle\u001b[1;34m(fname)\u001b[0m\n\u001b[0;32m   1396\u001b[0m         \u001b[1;31m# Because of loading from S3 load can't be used (missing readline in smart_open)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1397\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1398\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'latin1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1399\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1400\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yifan wu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\numpy\\random\\_pickle.py\u001b[0m in \u001b[0;36m__randomstate_ctor\u001b[1;34m(bit_generator_name)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0m__randomstate_ctor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbit_generator_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'MT19937'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m     \"\"\"\n\u001b[0;32m     64\u001b[0m     \u001b[0mPickling\u001b[0m \u001b[0mhelper\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlegacy\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlike\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = LSTM_Classifier(160, 256, 2)\n",
    "checkpoint_dir = os.path.join(model_path, 'LSTM_Classifier_Creative_Gender')\n",
    "checkpoint_prefix = 'LSTM_Classifier_Creative_Gender'\n",
    "model.load_state_dict(torch.load(os.path.join(checkpoint_dir, f'{checkpoint_prefix}_2.pth')))\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "device = DEVICE\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "train_gender(model, loss_fn, optimizer, device, checkpoint_dir, checkpoint_prefix, logger=logger, epoch_start=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07:11:35 INFO: =========================\n",
      "07:11:35 INFO: Processing Epoch 6/10\n",
      "07:11:35 INFO: =========================\n",
      "07:13:46 INFO: Epoch 6/10 - Training Split 1/9 Done - Train Loss: 0.367703, Val Loss: 0.366074, Val AUC: 0.972564, Val Accuracy: 0.944079\n",
      "07:15:58 INFO: Epoch 6/10 - Training Split 2/9 Done - Train Loss: 0.367952, Val Loss: 0.388708, Val AUC: 0.963354, Val Accuracy: 0.934211\n",
      "07:18:10 INFO: Epoch 6/10 - Training Split 3/9 Done - Train Loss: 0.367923, Val Loss: 0.376057, Val AUC: 0.963560, Val Accuracy: 0.934942\n",
      "07:20:21 INFO: Epoch 6/10 - Training Split 4/9 Done - Train Loss: 0.367549, Val Loss: 0.362159, Val AUC: 0.968822, Val Accuracy: 0.938596\n",
      "07:22:32 INFO: Epoch 6/10 - Training Split 5/9 Done - Train Loss: 0.367529, Val Loss: 0.376939, Val AUC: 0.968141, Val Accuracy: 0.938158\n",
      "07:24:41 INFO: Epoch 6/10 - Training Split 6/9 Done - Train Loss: 0.367303, Val Loss: 0.374775, Val AUC: 0.968516, Val Accuracy: 0.938048\n",
      "07:26:50 INFO: Epoch 6/10 - Training Split 7/9 Done - Train Loss: 0.367093, Val Loss: 0.376527, Val AUC: 0.967549, Val Accuracy: 0.937500\n",
      "07:29:00 INFO: Epoch 6/10 - Training Split 8/9 Done - Train Loss: 0.367016, Val Loss: 0.374607, Val AUC: 0.966299, Val Accuracy: 0.937363\n",
      "07:31:10 INFO: Epoch 6/10 - Training Split 9/9 Done - Train Loss: 0.366892, Val Loss: 0.362895, Val AUC: 0.966978, Val Accuracy: 0.938596\n",
      "07:32:24 INFO: Epoch 6/10 Done - Test Loss: 0.374103, Test AUC: 0.966053, Test Accuracy: 0.936822\n",
      "07:32:24 INFO: =========================\n",
      "07:32:24 INFO: Processing Epoch 7/10\n",
      "07:32:24 INFO: =========================\n",
      "07:34:31 INFO: Epoch 7/10 - Training Split 1/9 Done - Train Loss: 0.365962, Val Loss: 0.369601, Val AUC: 0.974977, Val Accuracy: 0.945175\n",
      "07:36:39 INFO: Epoch 7/10 - Training Split 2/9 Done - Train Loss: 0.366253, Val Loss: 0.394024, Val AUC: 0.964866, Val Accuracy: 0.930921\n",
      "07:38:48 INFO: Epoch 7/10 - Training Split 3/9 Done - Train Loss: 0.366213, Val Loss: 0.371799, Val AUC: 0.966889, Val Accuracy: 0.933114\n",
      "07:40:56 INFO: Epoch 7/10 - Training Split 4/9 Done - Train Loss: 0.365894, Val Loss: 0.365591, Val AUC: 0.968359, Val Accuracy: 0.936129\n",
      "07:43:04 INFO: Epoch 7/10 - Training Split 5/9 Done - Train Loss: 0.365980, Val Loss: 0.377705, Val AUC: 0.966158, Val Accuracy: 0.935746\n",
      "07:45:12 INFO: Epoch 7/10 - Training Split 6/9 Done - Train Loss: 0.365890, Val Loss: 0.371809, Val AUC: 0.967241, Val Accuracy: 0.935855\n",
      "07:47:20 INFO: Epoch 7/10 - Training Split 7/9 Done - Train Loss: 0.365702, Val Loss: 0.376031, Val AUC: 0.965941, Val Accuracy: 0.936090\n",
      "07:49:28 INFO: Epoch 7/10 - Training Split 8/9 Done - Train Loss: 0.365634, Val Loss: 0.374945, Val AUC: 0.964625, Val Accuracy: 0.936129\n",
      "07:51:37 INFO: Epoch 7/10 - Training Split 9/9 Done - Train Loss: 0.365532, Val Loss: 0.366751, Val AUC: 0.964642, Val Accuracy: 0.937256\n",
      "07:52:50 INFO: Epoch 7/10 Done - Test Loss: 0.373979, Test AUC: 0.959674, Test Accuracy: 0.937400\n",
      "07:52:50 INFO: =========================\n",
      "07:52:50 INFO: Processing Epoch 8/10\n",
      "07:52:50 INFO: =========================\n",
      "07:55:00 INFO: Epoch 8/10 - Training Split 1/9 Done - Train Loss: 0.365085, Val Loss: 0.372399, Val AUC: 0.959917, Val Accuracy: 0.940789\n",
      "07:57:12 INFO: Epoch 8/10 - Training Split 2/9 Done - Train Loss: 0.365652, Val Loss: 0.398669, Val AUC: 0.954910, Val Accuracy: 0.924342\n",
      "07:59:23 INFO: Epoch 8/10 - Training Split 3/9 Done - Train Loss: 0.365394, Val Loss: 0.371852, Val AUC: 0.956458, Val Accuracy: 0.929825\n",
      "08:01:34 INFO: Epoch 8/10 - Training Split 4/9 Done - Train Loss: 0.364907, Val Loss: 0.367468, Val AUC: 0.960492, Val Accuracy: 0.932840\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_EXECUTION_FAILED (_cudnn_rnn_backward_input at ..\\aten\\src\\ATen\\native\\cudnn\\RNN.cpp:931)\n(no backtrace available)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-5d643bcd6c24>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mtrain_gender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint_prefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-c36ea2e81a46>\u001b[0m in \u001b[0;36mtrain_gender\u001b[1;34m(model, loss_fn, optimizer, device, checkpoint_dir, checkpoint_prefix, logger, epoch_start)\u001b[0m\n\u001b[0;32m     36\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yifan wu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \"\"\"\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yifan wu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED (_cudnn_rnn_backward_input at ..\\aten\\src\\ATen\\native\\cudnn\\RNN.cpp:931)\n(no backtrace available)"
     ]
    }
   ],
   "source": [
    "model = LSTM_Classifier(160, 256, 2)\n",
    "checkpoint_dir = os.path.join(model_path, 'LSTM_Classifier_Creative_Gender')\n",
    "checkpoint_prefix = 'LSTM_Classifier_Creative_Gender'\n",
    "model.load_state_dict(torch.load(os.path.join(checkpoint_dir, f'{checkpoint_prefix}_5.pth')))\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "device = DEVICE\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "train_gender(model, loss_fn, optimizer, device, checkpoint_dir, checkpoint_prefix, logger=logger, epoch_start=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:05:00 INFO: =========================\n",
      "08:05:00 INFO: Processing Epoch 8/10\n",
      "08:05:00 INFO: =========================\n",
      "08:07:07 INFO: Epoch 8/10 - Training Split 1/9 Done - Train Loss: 0.364662, Val Loss: 0.368567, Val AUC: 0.962465, Val Accuracy: 0.944079\n",
      "08:09:17 INFO: Epoch 8/10 - Training Split 2/9 Done - Train Loss: 0.365105, Val Loss: 0.394047, Val AUC: 0.956590, Val Accuracy: 0.931469\n",
      "08:11:27 INFO: Epoch 8/10 - Training Split 3/9 Done - Train Loss: 0.365040, Val Loss: 0.372571, Val AUC: 0.959230, Val Accuracy: 0.933114\n",
      "08:13:38 INFO: Epoch 8/10 - Training Split 4/9 Done - Train Loss: 0.364682, Val Loss: 0.366505, Val AUC: 0.961847, Val Accuracy: 0.935855\n",
      "08:15:46 INFO: Epoch 8/10 - Training Split 5/9 Done - Train Loss: 0.364662, Val Loss: 0.372614, Val AUC: 0.962568, Val Accuracy: 0.937061\n",
      "08:17:55 INFO: Epoch 8/10 - Training Split 6/9 Done - Train Loss: 0.364575, Val Loss: 0.376573, Val AUC: 0.962644, Val Accuracy: 0.936952\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_EXECUTION_FAILED (_cudnn_rnn_backward_input at ..\\aten\\src\\ATen\\native\\cudnn\\RNN.cpp:931)\n(no backtrace available)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-526dfe662ce1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mEPOCHES\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mtrain_gender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint_prefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-c36ea2e81a46>\u001b[0m in \u001b[0;36mtrain_gender\u001b[1;34m(model, loss_fn, optimizer, device, checkpoint_dir, checkpoint_prefix, logger, epoch_start)\u001b[0m\n\u001b[0;32m     36\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yifan wu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \"\"\"\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yifan wu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED (_cudnn_rnn_backward_input at ..\\aten\\src\\ATen\\native\\cudnn\\RNN.cpp:931)\n(no backtrace available)"
     ]
    }
   ],
   "source": [
    "model = LSTM_Classifier(160, 256, 2)\n",
    "checkpoint_dir = os.path.join(model_path, 'LSTM_Classifier_Creative_Gender')\n",
    "checkpoint_prefix = 'LSTM_Classifier_Creative_Gender'\n",
    "model.load_state_dict(torch.load(os.path.join(checkpoint_dir, f'{checkpoint_prefix}_7.pth')))\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "device = DEVICE\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "EPOCHES = 3\n",
    "\n",
    "train_gender(model, loss_fn, optimizer, device, checkpoint_dir, checkpoint_prefix, logger=logger, epoch_start=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Age Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHES = 5\n",
    "BATCH_SIZE = 512\n",
    "N_BATCH = 90000//BATCH_SIZE-1\n",
    "TEST_SIZE = 90000%BATCH_SIZE + BATCH_SIZE\n",
    "\n",
    "def train_age(model, loss_fn, optimizer, device, checkpoint_dir, checkpoint_prefix, logger=None, epoch_start=0):\n",
    "    global EPOCHES, BATCH_SIZE, N_BATCH, TEST_SIZE\n",
    "    if not os.path.isdir(checkpoint_dir):\n",
    "        os.mkdir(checkpoint_dir)\n",
    "    \n",
    "    for epoch in range(1+epoch_start, EPOCHES+1+epoch_start):\n",
    "        if logger: \n",
    "            logger.info('=========================')\n",
    "            logger.info(f'Processing Epoch {epoch}/{EPOCHES+epoch_start}')\n",
    "            logger.info('=========================')\n",
    "            \n",
    "        train_file = [1,2,3,4,5,6,7,8,9]\n",
    "        test_file = [10]\n",
    "            \n",
    "        train_running_loss, train_n_batch = 0, 0\n",
    "        pred_y, true_y = [], []\n",
    "        for index, split_id in enumerate(train_file, start=1):\n",
    "            inp_creative, inp_last_idx, out_age, out_gender = prepare_data(split_id)\n",
    "            train_creative, test_creative = inp_creative[:-TEST_SIZE], inp_creative[-TEST_SIZE:]\n",
    "            train_last_idx, test_last_idx = inp_last_idx[:-TEST_SIZE], inp_last_idx[-TEST_SIZE:]\n",
    "            train_age, test_age = out_age[:-TEST_SIZE], out_age[-TEST_SIZE:]\n",
    "            \n",
    "            model.train()\n",
    "            \n",
    "            for batch_index in range(N_BATCH):\n",
    "                x1 = torch.nn.utils.rnn.pad_sequence(train_creative[batch_index*BATCH_SIZE:(batch_index+1)*BATCH_SIZE], batch_first=True, padding_value=0).to(device)\n",
    "                x2 = train_last_idx[batch_index*BATCH_SIZE:(batch_index+1)*BATCH_SIZE]\n",
    "                y = train_age[batch_index*BATCH_SIZE:(batch_index+1)*BATCH_SIZE].to(device)\n",
    "                optimizer.zero_grad()\n",
    "                yp = F.softmax(model(x1, x2), 1)\n",
    "                loss = loss_fn(yp, y)\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=100)\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_running_loss += loss.item()\n",
    "                train_n_batch += 1\n",
    "                \n",
    "                del x1, x2, y, yp\n",
    "                _ = gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            model.eval()\n",
    "            \n",
    "            x1 = torch.nn.utils.rnn.pad_sequence(test_creative, batch_first=True, padding_value=0).to(device)\n",
    "            x2 = test_last_idx\n",
    "            y = test_age.to(device)\n",
    "            yp = F.softmax(model(x1, x2), 1)\n",
    "            loss = loss_fn(yp, y)\n",
    "            \n",
    "            pred_y.extend(list(yp.cpu().detach().numpy()))\n",
    "            true_y.extend(list(y.cpu().detach().numpy()))\n",
    "            \n",
    "            del x1, x2, y, yp\n",
    "            _ = gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            pred = np.argmax(np.array(pred_y), 1)\n",
    "            true = np.array(true_y).reshape((-1,))\n",
    "            acc_score = accuracy_score(true, pred)\n",
    "            \n",
    "            if logger:\n",
    "                logger.info(f'Epoch {epoch}/{EPOCHES+epoch_start} - Training Split {index}/{len(train_file)} Done - Train Loss: {train_running_loss/train_n_batch:.6f}, Val Loss: {loss.item():.6f}, Val Accuracy: {acc_score:.6f}')\n",
    "            \n",
    "            del inp_creative, inp_last_idx, out_age, out_gender, train_creative, test_creative, train_last_idx, test_last_idx, train_age, test_age\n",
    "            _ = gc.collect()\n",
    "            torch.cuda.empty_cache()   \n",
    "        \n",
    "        model.eval()\n",
    "        test_running_loss, test_n_batch = 0, 0\n",
    "        true_y, pred_y = [], []\n",
    "        \n",
    "        for index, split_id in enumerate(test_file, start=1):\n",
    "            inp_creative, inp_last_idx, out_age, out_gender = prepare_data(split_id)\n",
    "            for batch_index in range(N_BATCH+2):\n",
    "                x1 = torch.nn.utils.rnn.pad_sequence(inp_creative[batch_index*BATCH_SIZE:(batch_index+1)*BATCH_SIZE], batch_first=True, padding_value=0).to(device)\n",
    "                x2 = inp_last_idx[batch_index*BATCH_SIZE:(batch_index+1)*BATCH_SIZE]\n",
    "                y = out_age[batch_index*BATCH_SIZE:(batch_index+1)*BATCH_SIZE].to(device)\n",
    "                yp = F.softmax(model(x1, x2), 1)\n",
    "                loss = loss_fn(yp, y)\n",
    "            \n",
    "                test_running_loss += loss.item()\n",
    "                test_n_batch += 1\n",
    "            \n",
    "                pred_y.extend(list(yp.cpu().detach().numpy()))\n",
    "                true_y.extend(list(y.cpu().detach().numpy()))\n",
    "            \n",
    "                del x1, x2, y, yp\n",
    "                _ = gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            del inp_creative, inp_last_idx, out_age, out_gender\n",
    "            _ = gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        pred = np.argmax(np.array(pred_y), 1)\n",
    "        true = np.array(true_y).reshape((-1,))\n",
    "        acc_score = accuracy_score(true, pred)\n",
    "        \n",
    "        if logger:\n",
    "            logger.info(f'Epoch {epoch}/{EPOCHES+epoch_start} Done - Test Loss: {test_running_loss/test_n_batch:.6f}, Test Accuracy: {acc_score:.6f}')\n",
    "            \n",
    "        ck_file_name = f'{checkpoint_prefix}_{epoch}.pth'\n",
    "        ck_file_path = os.path.join(checkpoint_dir, ck_file_name)\n",
    "        \n",
    "        torch.save(model.state_dict(), ck_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:38:12 INFO: =========================\n",
      "08:38:12 INFO: Processing Epoch 1/5\n",
      "08:38:12 INFO: =========================\n",
      "08:40:21 INFO: Epoch 1/5 - Training Split 1/9 Done - Train Loss: 2.143261, Val Loss: 2.099092, Val Accuracy: 0.353070\n",
      "08:42:32 INFO: Epoch 1/5 - Training Split 2/9 Done - Train Loss: 2.130610, Val Loss: 2.117119, Val Accuracy: 0.345943\n",
      "08:44:43 INFO: Epoch 1/5 - Training Split 3/9 Done - Train Loss: 2.123347, Val Loss: 2.122414, Val Accuracy: 0.339912\n",
      "08:46:55 INFO: Epoch 1/5 - Training Split 4/9 Done - Train Loss: 2.118455, Val Loss: 2.112529, Val Accuracy: 0.335800\n",
      "08:49:05 INFO: Epoch 1/5 - Training Split 5/9 Done - Train Loss: 2.113458, Val Loss: 2.080825, Val Accuracy: 0.343421\n",
      "08:51:14 INFO: Epoch 1/5 - Training Split 6/9 Done - Train Loss: 2.109293, Val Loss: 2.093206, Val Accuracy: 0.344846\n",
      "08:53:22 INFO: Epoch 1/5 - Training Split 7/9 Done - Train Loss: 2.105581, Val Loss: 2.084407, Val Accuracy: 0.347744\n",
      "08:55:30 INFO: Epoch 1/5 - Training Split 8/9 Done - Train Loss: 2.101984, Val Loss: 2.045729, Val Accuracy: 0.354989\n",
      "08:57:39 INFO: Epoch 1/5 - Training Split 9/9 Done - Train Loss: 2.099058, Val Loss: 2.071262, Val Accuracy: 0.357212\n",
      "08:58:52 INFO: Epoch 1/5 Done - Test Loss: 2.066989, Test Accuracy: 0.382289\n",
      "08:58:52 INFO: =========================\n",
      "08:58:52 INFO: Processing Epoch 2/5\n",
      "08:58:52 INFO: =========================\n",
      "09:01:01 INFO: Epoch 2/5 - Training Split 1/9 Done - Train Loss: 2.074593, Val Loss: 2.061053, Val Accuracy: 0.379386\n",
      "09:03:11 INFO: Epoch 2/5 - Training Split 2/9 Done - Train Loss: 2.072734, Val Loss: 2.088557, Val Accuracy: 0.368421\n",
      "09:05:21 INFO: Epoch 2/5 - Training Split 3/9 Done - Train Loss: 2.071254, Val Loss: 2.066597, Val Accuracy: 0.374635\n",
      "09:07:31 INFO: Epoch 2/5 - Training Split 4/9 Done - Train Loss: 2.069754, Val Loss: 2.092057, Val Accuracy: 0.369792\n",
      "09:09:39 INFO: Epoch 2/5 - Training Split 5/9 Done - Train Loss: 2.067662, Val Loss: 2.053303, Val Accuracy: 0.376535\n",
      "09:11:47 INFO: Epoch 2/5 - Training Split 6/9 Done - Train Loss: 2.066299, Val Loss: 2.070652, Val Accuracy: 0.379569\n",
      "09:13:57 INFO: Epoch 2/5 - Training Split 7/9 Done - Train Loss: 2.064501, Val Loss: 2.058727, Val Accuracy: 0.381736\n",
      "09:16:07 INFO: Epoch 2/5 - Training Split 8/9 Done - Train Loss: 2.063217, Val Loss: 2.053853, Val Accuracy: 0.384457\n",
      "09:18:18 INFO: Epoch 2/5 - Training Split 9/9 Done - Train Loss: 2.062026, Val Loss: 2.073239, Val Accuracy: 0.383772\n",
      "09:19:29 INFO: Epoch 2/5 Done - Test Loss: 2.051601, Test Accuracy: 0.398767\n",
      "09:19:29 INFO: =========================\n",
      "09:19:29 INFO: Processing Epoch 3/5\n",
      "09:19:29 INFO: =========================\n",
      "09:21:37 INFO: Epoch 3/5 - Training Split 1/9 Done - Train Loss: 2.052587, Val Loss: 2.036031, Val Accuracy: 0.426535\n",
      "09:23:45 INFO: Epoch 3/5 - Training Split 2/9 Done - Train Loss: 2.052839, Val Loss: 2.085386, Val Accuracy: 0.389254\n",
      "09:25:56 INFO: Epoch 3/5 - Training Split 3/9 Done - Train Loss: 2.051581, Val Loss: 2.057141, Val Accuracy: 0.387427\n",
      "09:28:06 INFO: Epoch 3/5 - Training Split 4/9 Done - Train Loss: 2.050618, Val Loss: 2.078474, Val Accuracy: 0.382401\n",
      "09:30:15 INFO: Epoch 3/5 - Training Split 5/9 Done - Train Loss: 2.049475, Val Loss: 2.043304, Val Accuracy: 0.385965\n",
      "09:32:24 INFO: Epoch 3/5 - Training Split 6/9 Done - Train Loss: 2.049223, Val Loss: 2.055978, Val Accuracy: 0.387975\n",
      "09:34:33 INFO: Epoch 3/5 - Training Split 7/9 Done - Train Loss: 2.048132, Val Loss: 2.055185, Val Accuracy: 0.390038\n",
      "09:36:41 INFO: Epoch 3/5 - Training Split 8/9 Done - Train Loss: 2.047401, Val Loss: 2.038610, Val Accuracy: 0.391996\n",
      "09:38:50 INFO: Epoch 3/5 - Training Split 9/9 Done - Train Loss: 2.046930, Val Loss: 2.074986, Val Accuracy: 0.389254\n",
      "09:40:02 INFO: Epoch 3/5 Done - Test Loss: 2.044268, Test Accuracy: 0.406167\n",
      "09:40:02 INFO: =========================\n",
      "09:40:02 INFO: Processing Epoch 4/5\n",
      "09:40:02 INFO: =========================\n",
      "09:42:10 INFO: Epoch 4/5 - Training Split 1/9 Done - Train Loss: 2.041789, Val Loss: 2.040714, Val Accuracy: 0.413377\n",
      "09:44:18 INFO: Epoch 4/5 - Training Split 2/9 Done - Train Loss: 2.041848, Val Loss: 2.068742, Val Accuracy: 0.398575\n",
      "09:46:27 INFO: Epoch 4/5 - Training Split 3/9 Done - Train Loss: 2.041160, Val Loss: 2.051262, Val Accuracy: 0.400219\n",
      "09:48:36 INFO: Epoch 4/5 - Training Split 4/9 Done - Train Loss: 2.041129, Val Loss: 2.065161, Val Accuracy: 0.395833\n",
      "09:50:44 INFO: Epoch 4/5 - Training Split 5/9 Done - Train Loss: 2.040497, Val Loss: 2.043268, Val Accuracy: 0.398246\n",
      "09:52:53 INFO: Epoch 4/5 - Training Split 6/9 Done - Train Loss: 2.040408, Val Loss: 2.046109, Val Accuracy: 0.398026\n",
      "09:55:02 INFO: Epoch 4/5 - Training Split 7/9 Done - Train Loss: 2.039826, Val Loss: 2.057589, Val Accuracy: 0.396147\n",
      "09:57:10 INFO: Epoch 4/5 - Training Split 8/9 Done - Train Loss: 2.039329, Val Loss: 2.036381, Val Accuracy: 0.398163\n",
      "09:59:19 INFO: Epoch 4/5 - Training Split 9/9 Done - Train Loss: 2.039002, Val Loss: 2.071695, Val Accuracy: 0.396199\n",
      "10:00:31 INFO: Epoch 4/5 Done - Test Loss: 2.041790, Test Accuracy: 0.409411\n",
      "10:00:31 INFO: =========================\n",
      "10:00:31 INFO: Processing Epoch 5/5\n",
      "10:00:31 INFO: =========================\n",
      "10:02:39 INFO: Epoch 5/5 - Training Split 1/9 Done - Train Loss: 2.036142, Val Loss: 2.026661, Val Accuracy: 0.419956\n",
      "10:04:48 INFO: Epoch 5/5 - Training Split 2/9 Done - Train Loss: 2.036291, Val Loss: 2.072807, Val Accuracy: 0.398575\n",
      "10:06:58 INFO: Epoch 5/5 - Training Split 3/9 Done - Train Loss: 2.035759, Val Loss: 2.059260, Val Accuracy: 0.393275\n",
      "10:09:09 INFO: Epoch 5/5 - Training Split 4/9 Done - Train Loss: 2.035682, Val Loss: 2.075045, Val Accuracy: 0.387336\n",
      "10:11:21 INFO: Epoch 5/5 - Training Split 5/9 Done - Train Loss: 2.034987, Val Loss: 2.038044, Val Accuracy: 0.391447\n",
      "10:13:33 INFO: Epoch 5/5 - Training Split 6/9 Done - Train Loss: 2.035011, Val Loss: 2.050046, Val Accuracy: 0.393458\n",
      "10:15:42 INFO: Epoch 5/5 - Training Split 7/9 Done - Train Loss: 2.034497, Val Loss: 2.055528, Val Accuracy: 0.394580\n",
      "10:17:52 INFO: Epoch 5/5 - Training Split 8/9 Done - Train Loss: 2.033886, Val Loss: 2.029827, Val Accuracy: 0.398575\n",
      "10:20:03 INFO: Epoch 5/5 - Training Split 9/9 Done - Train Loss: 2.033427, Val Loss: 2.064405, Val Accuracy: 0.397417\n",
      "10:21:18 INFO: Epoch 5/5 Done - Test Loss: 2.039317, Test Accuracy: 0.412544\n"
     ]
    }
   ],
   "source": [
    "model = LSTM_Classifier(160, 256, 10).to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "device = DEVICE\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "checkpoint_dir = os.path.join(model_path, 'LSTM_Classifier_Creative_Age')\n",
    "checkpoint_prefix = 'LSTM_Classifier_Creative_Age'\n",
    "\n",
    "train_age(model, loss_fn, optimizer, device, checkpoint_dir, checkpoint_prefix, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:24:46 INFO: =========================\n",
      "10:24:46 INFO: Processing Epoch 6/10\n",
      "10:24:46 INFO: =========================\n",
      "10:26:53 INFO: Epoch 6/10 - Training Split 1/9 Done - Train Loss: 2.031641, Val Loss: 2.028656, Val Accuracy: 0.429825\n",
      "10:29:03 INFO: Epoch 6/10 - Training Split 2/9 Done - Train Loss: 2.031851, Val Loss: 2.082080, Val Accuracy: 0.398026\n",
      "10:31:14 INFO: Epoch 6/10 - Training Split 3/9 Done - Train Loss: 2.031182, Val Loss: 2.046174, Val Accuracy: 0.401316\n",
      "10:33:25 INFO: Epoch 6/10 - Training Split 4/9 Done - Train Loss: 2.030634, Val Loss: 2.080975, Val Accuracy: 0.391721\n",
      "10:35:36 INFO: Epoch 6/10 - Training Split 5/9 Done - Train Loss: 2.030126, Val Loss: 2.035072, Val Accuracy: 0.396930\n",
      "10:37:47 INFO: Epoch 6/10 - Training Split 6/9 Done - Train Loss: 2.029887, Val Loss: 2.033610, Val Accuracy: 0.400037\n",
      "10:39:58 INFO: Epoch 6/10 - Training Split 7/9 Done - Train Loss: 2.028962, Val Loss: 2.051966, Val Accuracy: 0.401003\n",
      "10:42:08 INFO: Epoch 6/10 - Training Split 8/9 Done - Train Loss: 2.027995, Val Loss: 2.021664, Val Accuracy: 0.406113\n",
      "10:44:16 INFO: Epoch 6/10 - Training Split 9/9 Done - Train Loss: 2.027123, Val Loss: 2.056844, Val Accuracy: 0.404118\n",
      "10:45:28 INFO: Epoch 6/10 Done - Test Loss: 2.028480, Test Accuracy: 0.424267\n",
      "10:45:28 INFO: =========================\n",
      "10:45:28 INFO: Processing Epoch 7/10\n",
      "10:45:28 INFO: =========================\n",
      "10:47:35 INFO: Epoch 7/10 - Training Split 1/9 Done - Train Loss: 2.020566, Val Loss: 2.014022, Val Accuracy: 0.439693\n",
      "10:49:46 INFO: Epoch 7/10 - Training Split 2/9 Done - Train Loss: 2.020241, Val Loss: 2.055614, Val Accuracy: 0.423246\n",
      "10:51:56 INFO: Epoch 7/10 - Training Split 3/9 Done - Train Loss: 2.019206, Val Loss: 2.039567, Val Accuracy: 0.419225\n",
      "10:54:07 INFO: Epoch 7/10 - Training Split 4/9 Done - Train Loss: 2.018255, Val Loss: 2.059819, Val Accuracy: 0.413925\n",
      "10:56:18 INFO: Epoch 7/10 - Training Split 5/9 Done - Train Loss: 2.017221, Val Loss: 2.020290, Val Accuracy: 0.417763\n"
     ]
    }
   ],
   "source": [
    "model = LSTM_Classifier(160, 256, 10)\n",
    "checkpoint_dir = os.path.join(model_path, 'LSTM_Classifier_Creative_Age')\n",
    "checkpoint_prefix = 'LSTM_Classifier_Creative_Age'\n",
    "model.load_state_dict(torch.load(os.path.join(checkpoint_dir, f'{checkpoint_prefix}_5.pth')))\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "device = DEVICE\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "EPOCHES = 5\n",
    "\n",
    "train_age(model, loss_fn, optimizer, device, checkpoint_dir, checkpoint_prefix, logger=logger, epoch_start=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
