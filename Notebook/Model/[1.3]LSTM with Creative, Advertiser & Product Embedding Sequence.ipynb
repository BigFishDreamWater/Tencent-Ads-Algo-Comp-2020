{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "train_path = '../../raw_train_artifact'\n",
    "test_path = '../../raw_test_artifact'\n",
    "embedding_path = '../../embedding_artifact'\n",
    "input_path = '../../input_artifact'\n",
    "input_split_path = '../../input_artifact/input_split'\n",
    "model_path = '../../model_artifact'\n",
    "output_path = '../../output_artifact'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gc\n",
    "gc.enable()\n",
    "import time\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns',120)\n",
    "pd.set_option('display.max_rows',2000)\n",
    "pd.set_option('precision',5)\n",
    "pd.set_option('float_format', '{:.5f}'.format)\n",
    "\n",
    "import tqdm\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from gensim.models import Word2Vec\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:24:11 INFO: Restart notebook\n",
      "==========================\n",
      "Fri Jun  5 16:24:11 2020\n",
      "==========================\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "log_path = '[1.3]LSTM with Creative, Advertiser & Product Embedding Sequence.log'\n",
    "    \n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)-s: %(message)s', datefmt='%H:%M:%S')\n",
    "\n",
    "fh = logging.FileHandler(log_path)\n",
    "fh.setLevel(logging.INFO)\n",
    "fh.setFormatter(formatter)\n",
    "logger.addHandler(fh)\n",
    "\n",
    "sh = logging.StreamHandler(sys.stdout)\n",
    "sh.setLevel(logging.INFO)\n",
    "sh.setFormatter(formatter)\n",
    "logger.addHandler(sh)\n",
    "\n",
    "logger.info(f'Restart notebook\\n==========================\\n{time.ctime()}\\n==========================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:24:11 INFO: Device in Use: cuda\n",
      "16:24:11 INFO: CUDA Memory: Total 8.00 GB, Cached 0.00 GB, Allocated 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info('Device in Use: {}'.format(DEVICE))\n",
    "torch.cuda.empty_cache()\n",
    "t = torch.cuda.get_device_properties(DEVICE).total_memory/1024**3\n",
    "c = torch.cuda.memory_cached(DEVICE)/1024**3\n",
    "a = torch.cuda.memory_allocated(DEVICE)/1024**3\n",
    "logger.info('CUDA Memory: Total {:.2f} GB, Cached {:.2f} GB, Allocated {:.2f} GB'.format(t,c,a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_embed_artifact = {\n",
    "    'creative': {\n",
    "        'embedding_artifact': r'C:\\JupyterNotebook\\Tencent-Ads-Algo-Comp-2020\\embedding_artifact\\creative_id_embed_s160_w64_cbow_38168zon',\n",
    "        'train_file_prefix': 'train_creative_agg_user',\n",
    "        'test_file_prefix': 'test_creative_agg_user'\n",
    "    },\n",
    "    'ad': {\n",
    "        'embedding_artifact': r'C:\\JupyterNotebook\\Tencent-Ads-Algo-Comp-2020\\embedding_artifact\\ad_id_embed_s160_w64_cbow_ibfi8g78',\n",
    "        'train_file_prefix': 'train_ad_agg_user',\n",
    "        'test_file_prefix': 'test_ad_agg_user'\n",
    "    },\n",
    "    'advertiser': {\n",
    "        'embedding_artifact': r'C:\\JupyterNotebook\\Tencent-Ads-Algo-Comp-2020\\embedding_artifact\\advertiser_id_embed_s128_w64_cbow_n4re8tds',\n",
    "        'train_file_prefix': 'train_advertiser_agg_user',\n",
    "        'test_file_prefix': 'test_advertiser_agg_user'\n",
    "    },\n",
    "    'product': {\n",
    "        'embedding_artifact': r'C:\\JupyterNotebook\\Tencent-Ads-Algo-Comp-2020\\embedding_artifact\\product_id_embed_s128_w64_cbow_8yemmp45',\n",
    "        'train_file_prefix': 'train_product_agg_user',\n",
    "        'test_file_prefix': 'test_product_agg_user'\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_truth(split_id, logger=None):\n",
    "    \"\"\"\n",
    "    Get user id and ground truth\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    \n",
    "    truth_path = os.path.join(input_split_path, f'train_truth_{split_id}.npy')\n",
    "    with open(truth_path, 'rb') as f:\n",
    "        truth = np.load(f)\n",
    "        \n",
    "    inp_user = truth[:,0]\n",
    "    out_age = torch.from_numpy(truth[:,1]).long()\n",
    "    out_gender = torch.from_numpy(truth[:,2]).long()\n",
    "    \n",
    "    del truth\n",
    "    _ = gc.collect()\n",
    "    \n",
    "    if logger: logger.info(f'Target output ready after {time.time()-start:.2f}s')\n",
    "    return inp_user, out_age, out_gender\n",
    "\n",
    "def get_embed_seq(split_id, embed_var, inp_user, max_seq=100, train=True, logger=None):\n",
    "    \"\"\"\n",
    "    Get corresponding embedding sequence\n",
    "    \"\"\"\n",
    "    global inp_embed_artifact, input_split_path\n",
    "    assert embed_var in inp_embed_artifact\n",
    "    \n",
    "    start = time.time()\n",
    "    embedding = Word2Vec.load(inp_embed_artifact[embed_var]['embedding_artifact'])\n",
    "    if logger: logger.info(f'{embed_var.capitalize()} embedding artifact is loaded after {time.time()-start:.2f}s')\n",
    "    start = time.time()\n",
    "    file_prefix = inp_embed_artifact[embed_var]['train_file_prefix'] if train else inp_embed_artifact[embed_var]['test_file_prefix']\n",
    "    raw_path = os.path.join(input_split_path, f'{file_prefix}_{split_id}.json')\n",
    "    with open(raw_path, 'r') as f:\n",
    "        raw = json.load(f)\n",
    "    inp_seq = []\n",
    "    for user in inp_user:\n",
    "        inp_seq.append(torch.from_numpy(np.stack([embedding.wv[key] for key in raw[str(user)][:max_seq]], axis=0)).float())\n",
    "    inp_last_idx = np.array([i.shape[0] for i in inp_seq])-1\n",
    "    \n",
    "    del embedding, raw\n",
    "    _ = gc.collect()\n",
    "    \n",
    "    if logger: logger.info(f'{embed_var.capitalize()} embedding sequence ready after {time.time()-start:.2f}s')\n",
    "    return inp_seq, inp_last_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train(split_id, max_seq=100, logger=None):\n",
    "    \"\"\"\n",
    "    Get ground truth, and embedding sequence for creative, product and advertiser\n",
    "    \"\"\"\n",
    "    if logger: logger.info(f'Preparing Training Split-{split_id}')\n",
    "        \n",
    "    inp_user, out_age, out_gender = get_truth(split_id, logger=logger)\n",
    "    inp_creative_seq, inp_last_idx = get_embed_seq(split_id, 'creative',inp_user, max_seq=max_seq, logger=logger)\n",
    "    inp_advertiser_seq, _ = get_embed_seq(split_id, 'advertiser',inp_user, max_seq=max_seq, logger=logger)\n",
    "    inp_product_seq, _ = get_embed_seq(split_id, 'product',inp_user, max_seq=max_seq, logger=logger)\n",
    "    \n",
    "    del inp_user\n",
    "    _ = gc.collect()\n",
    "    \n",
    "    return out_age, out_gender, inp_creative_seq, inp_advertiser_seq, inp_product_seq, inp_last_idx   \n",
    "\n",
    "def prepare_test(split_id, max_seq=100, logger=None):\n",
    "    global input_split_path\n",
    "    if logger: logger.info(f'Preparing Training Split-{split_id}')\n",
    "        \n",
    "    idx_path = os.path.join(input_split_path, 'test_idx_shuffle.npy')\n",
    "    with open(idx_path, 'rb') as f:\n",
    "        test_idx = np.load(f)\n",
    "    inp_user = test_idx[(split_id-1)*100000:split_id*100000]\n",
    "    del test_idx\n",
    "    _ = gc.collect()\n",
    "    \n",
    "    inp_creative_seq, inp_last_idx = get_embed_seq(split_id, 'creative',inp_user, max_seq=max_seq, train=False, logger=logger)\n",
    "    inp_advertiser_seq, _ = get_embed_seq(split_id, 'advertiser',inp_user, max_seq=max_seq, train=False, logger=logger)\n",
    "    inp_product_seq, _ = get_embed_seq(split_id, 'product',inp_user, max_seq=max_seq, train=False, logger=logger)\n",
    "    \n",
    "    _ = gc.collect()\n",
    "    \n",
    "    return inp_user, inp_creative_seq, inp_advertiser_seq, inp_product_seq, inp_last_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Extraction_Layer(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM feature extration layer\n",
    "    - Layer 1: BiLSTM + Dropout + Layernorm\n",
    "    - Layer 2: LSTM with Residual Connection + Dropout + Layernorm\n",
    "    - Layer 3: LSTM + Batchnorm + ReLU + Dropout\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_size, lstm_hidden_size, rnn_dropout=0.2, mlp_dropout=0.4, **kwargs):\n",
    "        super(LSTM_Extraction_Layer, self).__init__(**kwargs)\n",
    "        self.embed_size = embed_size\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.rnn_dropout = rnn_dropout\n",
    "        self.mlp_dropout = mlp_dropout\n",
    "        \n",
    "        self.bi_lstm = nn.LSTM(input_size=embed_size, hidden_size=lstm_hidden_size, bias=True, bidirectional=True)\n",
    "        self.rnn_dropout_1 = nn.Dropout(p=rnn_dropout)\n",
    "        self.layernorm_1 = nn.LayerNorm(2*lstm_hidden_size)\n",
    "        self.lstm_1 = nn.LSTM(input_size=2*lstm_hidden_size, hidden_size=2*lstm_hidden_size)\n",
    "        self.rnn_dropout_2 = nn.Dropout(p=rnn_dropout)\n",
    "        self.layernorm_2 = nn.LayerNorm(2*lstm_hidden_size)\n",
    "        self.lstm_2 = nn.LSTM(input_size=2*lstm_hidden_size, hidden_size=2*lstm_hidden_size)\n",
    "        self.batchnorm = nn.BatchNorm1d(2*lstm_hidden_size)\n",
    "        self.mlp_dropout = nn.Dropout(p=mlp_dropout)\n",
    "        \n",
    "    def forward(self, inp_embed, inp_last_idx):\n",
    "        bilstm_out, _ = self.bi_lstm(inp_embed.permute(1,0,2))                            # (max_seq_length, batch_size, embed_size) -> (max_seq_length, batch_size, 2*lstm_hidden_size)\n",
    "        bilstm_out = self.layernorm_1(self.rnn_dropout_1(bilstm_out))                     # (max_seq_length, batch_size, 2*lstm_hidden_size)\n",
    "        lstm_out, _ = self.lstm_1(bilstm_out)                                             # (max_seq_length, batch_size, 2*lstm_hidden_size)\n",
    "        lstm_out = self.rnn_dropout_2(lstm_out)                                           # (max_seq_length, batch_size, 2*lstm_hidden_size)\n",
    "        lstm_out = self.layernorm_2(lstm_out+bilstm_out)                                  # (max_seq_length, batch_size, 2*lstm_hidden_size)\n",
    "        lstm_out, _ = self.lstm_2(lstm_out)                                               # (max_seq_length, batch_size, 2*lstm_hidden_size)\n",
    "        lstm_out = lstm_out.permute(1,0,2)[np.arange(len(inp_last_idx)), inp_last_idx,:]  # (batch_size, 2*lstm_hidden_size)\n",
    "        lstm_out = self.mlp_dropout(F.relu(self.batchnorm(lstm_out)))                     # (batch_size, 2*lstm_hidden_size)\n",
    "        return lstm_out\n",
    "    \n",
    "class MLP_Classification_Layer(nn.Module):\n",
    "    \"\"\"\n",
    "    Multilayer Perception Classification Layer\n",
    "    - Layer 1: Linear + Batchnorm + ReLU + Dropout\n",
    "    - Layer 2: Linear + Batchnorm + ReLU + Dropout\n",
    "    - Layer 3: Linear\n",
    "    \"\"\"\n",
    "    def __init__(self, inp_size, out_size, dropout=0.4, **kwargs):\n",
    "        super(MLP_Classification_Layer, self).__init__(**kwargs)\n",
    "        self.inp_size = inp_size\n",
    "        self.out_size = out_size\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.mlp_1 = nn.Linear(inp_size, 1024)\n",
    "        self.batchnorm_1 = nn.BatchNorm1d(1024)\n",
    "        self.mlp_dropout_1 = nn.Dropout(p=dropout)\n",
    "        self.mlp_2 = nn.Linear(1024, 512)\n",
    "        self.batchnorm_2 = nn.BatchNorm1d(512)\n",
    "        self.mlp_dropout_2 = nn.Dropout(p=dropout)\n",
    "        self.mlp_3 = nn.Linear(512, out_size)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        mlp_out = self.mlp_1(inp)                                                         # (batch_size, 1024)\n",
    "        mlp_out = self.mlp_dropout_1(F.relu(self.batchnorm_1(mlp_out)))                   # (batch_size, 1024)\n",
    "        mlp_out = self.mlp_2(mlp_out)                                                     # (batch_size, 512)\n",
    "        mlp_out = self.mlp_dropout_2(F.relu(self.batchnorm_2(mlp_out)))                   # (batch_size, 512)\n",
    "        mlp_out = self.mlp_3(mlp_out)                                                     # (batch_size, out_size)\n",
    "        return mlp_out   \n",
    "    \n",
    "class Multi_Seq_LSTM_Classifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Use separate LSTM extractor to handle different sequences, concat them and feed backto multilayer perception classifier.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_size, lstm_hidden_size, out_size, rnn_dropout=0.2, mlp_dropout=0.4, **kwargs):\n",
    "        super(Multi_Seq_LSTM_Classifier, self).__init__(**kwargs)\n",
    "        assert isinstance(embed_size, list) and isinstance(lstm_hidden_size, list) and len(embed_size)==len(lstm_hidden_size)\n",
    "        \n",
    "        self.embed_size = embed_size\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.out_size = out_size\n",
    "        self.rnn_dropout = rnn_dropout\n",
    "        self.mlp_dropout = mlp_dropout\n",
    "        \n",
    "        self.n_extraction = len(embed_size)\n",
    "        self.mlp_inp_size = sum(map(lambda x:2*x, lstm_hidden_size))\n",
    "        \n",
    "        for index, (e_size, h_size) in enumerate(zip(embed_size, lstm_hidden_size)):\n",
    "            setattr(self, f'extraction_layer_{index}', LSTM_Extraction_Layer(e_size, h_size, rnn_dropout=rnn_dropout, mlp_dropout=mlp_dropout))\n",
    "        self.classification_layer = MLP_Classification_Layer(self.mlp_inp_size, out_size, dropout=mlp_dropout)\n",
    "        \n",
    "    def forward(self, *args):\n",
    "        assert len(args)==self.n_extraction+1\n",
    "        \n",
    "        extract_buffer = [getattr(self, f'extraction_layer_{index}')(inp_embed, args[-1]) for index, inp_embed in enumerate(args[:-1])]\n",
    "        out = torch.cat(extract_buffer, 1)\n",
    "        out = self.classification_layer(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Age Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHES = 5\n",
    "BATCH_SIZE = 256\n",
    "div, mod = divmod(90000, BATCH_SIZE)\n",
    "N_BATCH = div + min(mod, 1)\n",
    "\n",
    "def train_age(model, loss_fn, optimizer, device, checkpoint_dir, checkpoint_prefix, logger=None, epoch_start=0):\n",
    "    global EPOCHES, BATCH_SIZE, N_BATCH, TEST_SIZE\n",
    "    if not os.path.isdir(checkpoint_dir):\n",
    "        os.mkdir(checkpoint_dir)\n",
    "    \n",
    "    for epoch in range(1+epoch_start, EPOCHES+1+epoch_start):\n",
    "        if logger: \n",
    "            logger.info('=========================')\n",
    "            logger.info(f'Processing Epoch {epoch}/{EPOCHES+epoch_start}')\n",
    "            logger.info('=========================')\n",
    "            \n",
    "        train_file = [1,2,3,4,5,6,7,8,9]\n",
    "        test_file = [10]\n",
    "            \n",
    "        train_running_loss, train_n_batch = 0, 0\n",
    "        pred_y, true_y = [], []\n",
    "        for index, split_id in enumerate(train_file, start=1):\n",
    "            out_age, out_gender, inp_creative_seq, inp_advertiser_seq, inp_product_seq, inp_last_idx = prepare_train(split_id)\n",
    "            model.train()\n",
    "            \n",
    "            for batch_index in range(N_BATCH):\n",
    "                x1 = torch.nn.utils.rnn.pad_sequence(inp_creative_seq[batch_index*BATCH_SIZE:(batch_index+1)*BATCH_SIZE], batch_first=True, padding_value=0).to(device)\n",
    "                x2 = torch.nn.utils.rnn.pad_sequence(inp_advertiser_seq[batch_index*BATCH_SIZE:(batch_index+1)*BATCH_SIZE], batch_first=True, padding_value=0).to(device)\n",
    "                x3 = torch.nn.utils.rnn.pad_sequence(inp_product_seq[batch_index*BATCH_SIZE:(batch_index+1)*BATCH_SIZE], batch_first=True, padding_value=0).to(device)\n",
    "                x4 = inp_last_idx[batch_index*BATCH_SIZE:(batch_index+1)*BATCH_SIZE]\n",
    "                y = out_age[batch_index*BATCH_SIZE:(batch_index+1)*BATCH_SIZE].to(device)\n",
    "                optimizer.zero_grad()\n",
    "                yp = F.softmax(model(x1, x2, x3, x4), 1)\n",
    "                loss = loss_fn(yp, y)\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=100)\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_running_loss += loss.item()\n",
    "                train_n_batch += 1\n",
    "                \n",
    "                del x1, x2, x3, x4, y, yp\n",
    "                _ = gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            if logger:\n",
    "                logger.info(f'Epoch {epoch}/{EPOCHES+epoch_start} - Training Split {index}/{len(train_file)} Done - Train Loss: {train_running_loss/train_n_batch:.6f}')\n",
    "            \n",
    "            del out_age, out_gender, inp_creative_seq, inp_advertiser_seq, inp_product_seq, inp_last_idx\n",
    "            _ = gc.collect()\n",
    "            torch.cuda.empty_cache()   \n",
    "        \n",
    "        model.eval()\n",
    "        test_running_loss, test_n_batch = 0, 0\n",
    "        true_y, pred_y = [], []\n",
    "        \n",
    "        for index, split_id in enumerate(test_file, start=1):\n",
    "            out_age, out_gender, inp_creative_seq, inp_advertiser_seq, inp_product_seq, inp_last_idx = prepare_train(split_id)\n",
    "            for batch_index in range(N_BATCH):\n",
    "                x1 = torch.nn.utils.rnn.pad_sequence(inp_creative_seq[batch_index*BATCH_SIZE:(batch_index+1)*BATCH_SIZE], batch_first=True, padding_value=0).to(device)\n",
    "                x2 = torch.nn.utils.rnn.pad_sequence(inp_advertiser_seq[batch_index*BATCH_SIZE:(batch_index+1)*BATCH_SIZE], batch_first=True, padding_value=0).to(device)\n",
    "                x3 = torch.nn.utils.rnn.pad_sequence(inp_product_seq[batch_index*BATCH_SIZE:(batch_index+1)*BATCH_SIZE], batch_first=True, padding_value=0).to(device)\n",
    "                x4 = inp_last_idx[batch_index*BATCH_SIZE:(batch_index+1)*BATCH_SIZE]\n",
    "                y = out_age[batch_index*BATCH_SIZE:(batch_index+1)*BATCH_SIZE].to(device)\n",
    "                yp = F.softmax(model(x1, x2, x3, x4), 1)\n",
    "                loss = loss_fn(yp, y)\n",
    "            \n",
    "                test_running_loss += loss.item()\n",
    "                test_n_batch += 1\n",
    "            \n",
    "                pred_y.extend(list(yp.cpu().detach().numpy()))\n",
    "                true_y.extend(list(y.cpu().detach().numpy()))\n",
    "            \n",
    "                del x1, x2, x3, x4, y, yp\n",
    "                _ = gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            del out_age, out_gender, inp_creative_seq, inp_advertiser_seq, inp_product_seq, inp_last_idx\n",
    "            _ = gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        pred = np.argmax(np.array(pred_y), 1)\n",
    "        true = np.array(true_y).reshape((-1,))\n",
    "        acc_score = accuracy_score(true, pred)\n",
    "        \n",
    "        if logger:\n",
    "            logger.info(f'Epoch {epoch}/{EPOCHES+epoch_start} Done - Test Loss: {test_running_loss/test_n_batch:.6f}, Test Accuracy: {acc_score:.6f}')\n",
    "            \n",
    "        ck_file_name = f'{checkpoint_prefix}_{epoch}.pth'\n",
    "        ck_file_path = os.path.join(checkpoint_dir, ck_file_name)\n",
    "        \n",
    "        torch.save(model.state_dict(), ck_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07:40:16 INFO: =========================\n",
      "07:40:16 INFO: Processing Epoch 1/5\n",
      "07:40:16 INFO: =========================\n",
      "07:46:12 INFO: Epoch 1/5 - Training Split 1/9 Done - Train Loss: 2.137750\n",
      "07:52:21 INFO: Epoch 1/5 - Training Split 2/9 Done - Train Loss: 2.123506\n",
      "07:58:28 INFO: Epoch 1/5 - Training Split 3/9 Done - Train Loss: 2.114756\n",
      "08:04:33 INFO: Epoch 1/5 - Training Split 4/9 Done - Train Loss: 2.108503\n",
      "08:10:39 INFO: Epoch 1/5 - Training Split 5/9 Done - Train Loss: 2.103345\n",
      "08:16:41 INFO: Epoch 1/5 - Training Split 6/9 Done - Train Loss: 2.099431\n",
      "08:22:41 INFO: Epoch 1/5 - Training Split 7/9 Done - Train Loss: 2.095377\n",
      "08:28:42 INFO: Epoch 1/5 - Training Split 8/9 Done - Train Loss: 2.092019\n",
      "08:34:43 INFO: Epoch 1/5 - Training Split 9/9 Done - Train Loss: 2.089040\n",
      "08:37:49 INFO: Epoch 1/5 Done - Test Loss: 2.065752, Test Accuracy: 0.385378\n",
      "08:37:49 INFO: =========================\n",
      "08:37:49 INFO: Processing Epoch 2/5\n",
      "08:37:49 INFO: =========================\n",
      "08:43:45 INFO: Epoch 2/5 - Training Split 1/9 Done - Train Loss: 2.063969\n",
      "08:49:47 INFO: Epoch 2/5 - Training Split 2/9 Done - Train Loss: 2.061952\n",
      "08:55:51 INFO: Epoch 2/5 - Training Split 3/9 Done - Train Loss: 2.060968\n",
      "09:01:55 INFO: Epoch 2/5 - Training Split 4/9 Done - Train Loss: 2.059977\n",
      "09:08:01 INFO: Epoch 2/5 - Training Split 5/9 Done - Train Loss: 2.058912\n",
      "09:14:13 INFO: Epoch 2/5 - Training Split 6/9 Done - Train Loss: 2.058248\n",
      "09:20:18 INFO: Epoch 2/5 - Training Split 7/9 Done - Train Loss: 2.057306\n",
      "09:26:20 INFO: Epoch 2/5 - Training Split 8/9 Done - Train Loss: 2.056278\n",
      "09:32:25 INFO: Epoch 2/5 - Training Split 9/9 Done - Train Loss: 2.055300\n",
      "09:35:32 INFO: Epoch 2/5 Done - Test Loss: 2.042362, Test Accuracy: 0.409256\n",
      "09:35:33 INFO: =========================\n",
      "09:35:33 INFO: Processing Epoch 3/5\n",
      "09:35:33 INFO: =========================\n",
      "09:41:31 INFO: Epoch 3/5 - Training Split 1/9 Done - Train Loss: 2.046796\n",
      "09:47:38 INFO: Epoch 3/5 - Training Split 2/9 Done - Train Loss: 2.045482\n",
      "09:53:46 INFO: Epoch 3/5 - Training Split 3/9 Done - Train Loss: 2.044550\n",
      "09:59:53 INFO: Epoch 3/5 - Training Split 4/9 Done - Train Loss: 2.044014\n",
      "10:06:00 INFO: Epoch 3/5 - Training Split 5/9 Done - Train Loss: 2.043495\n",
      "10:12:05 INFO: Epoch 3/5 - Training Split 6/9 Done - Train Loss: 2.043220\n",
      "10:18:09 INFO: Epoch 3/5 - Training Split 7/9 Done - Train Loss: 2.042714\n",
      "10:24:11 INFO: Epoch 3/5 - Training Split 8/9 Done - Train Loss: 2.042300\n",
      "10:30:16 INFO: Epoch 3/5 - Training Split 9/9 Done - Train Loss: 2.041732\n",
      "10:33:24 INFO: Epoch 3/5 Done - Test Loss: 2.039149, Test Accuracy: 0.412911\n",
      "10:33:24 INFO: =========================\n",
      "10:33:24 INFO: Processing Epoch 4/5\n",
      "10:33:24 INFO: =========================\n",
      "10:39:19 INFO: Epoch 4/5 - Training Split 1/9 Done - Train Loss: 2.037234\n",
      "10:45:24 INFO: Epoch 4/5 - Training Split 2/9 Done - Train Loss: 2.037378\n",
      "10:51:25 INFO: Epoch 4/5 - Training Split 3/9 Done - Train Loss: 2.036821\n",
      "10:57:28 INFO: Epoch 4/5 - Training Split 4/9 Done - Train Loss: 2.036289\n",
      "11:03:29 INFO: Epoch 4/5 - Training Split 5/9 Done - Train Loss: 2.035882\n",
      "11:09:32 INFO: Epoch 4/5 - Training Split 6/9 Done - Train Loss: 2.035800\n",
      "11:15:34 INFO: Epoch 4/5 - Training Split 7/9 Done - Train Loss: 2.035397\n",
      "11:21:39 INFO: Epoch 4/5 - Training Split 8/9 Done - Train Loss: 2.035174\n",
      "11:27:41 INFO: Epoch 4/5 - Training Split 9/9 Done - Train Loss: 2.035033\n",
      "11:30:52 INFO: Epoch 4/5 Done - Test Loss: 2.034043, Test Accuracy: 0.419000\n",
      "11:30:52 INFO: =========================\n",
      "11:30:52 INFO: Processing Epoch 5/5\n",
      "11:30:52 INFO: =========================\n",
      "11:36:51 INFO: Epoch 5/5 - Training Split 1/9 Done - Train Loss: 2.034805\n",
      "11:42:57 INFO: Epoch 5/5 - Training Split 2/9 Done - Train Loss: 2.033731\n",
      "11:49:03 INFO: Epoch 5/5 - Training Split 3/9 Done - Train Loss: 2.032841\n",
      "11:55:09 INFO: Epoch 5/5 - Training Split 4/9 Done - Train Loss: 2.032317\n",
      "12:01:18 INFO: Epoch 5/5 - Training Split 5/9 Done - Train Loss: 2.031989\n",
      "12:07:24 INFO: Epoch 5/5 - Training Split 6/9 Done - Train Loss: 2.031583\n",
      "12:13:28 INFO: Epoch 5/5 - Training Split 7/9 Done - Train Loss: 2.031188\n",
      "12:19:35 INFO: Epoch 5/5 - Training Split 8/9 Done - Train Loss: 2.030675\n",
      "12:25:39 INFO: Epoch 5/5 - Training Split 9/9 Done - Train Loss: 2.030298\n",
      "12:28:48 INFO: Epoch 5/5 Done - Test Loss: 2.032854, Test Accuracy: 0.418733\n"
     ]
    }
   ],
   "source": [
    "model = Multi_Seq_LSTM_Classifier([160, 128, 128], [256, 256, 256], 10).to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "device = DEVICE\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "checkpoint_dir = os.path.join(model_path, 'Multi_Seq_LSTM_Classifier_Creative_Advertiser_Product_Age')\n",
    "checkpoint_prefix = 'Multi_Seq_LSTM_Classifier_Creative_Advertiser_Product_Age'\n",
    "\n",
    "train_age(model, loss_fn, optimizer, device, checkpoint_dir, checkpoint_prefix, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:45:19 INFO: =========================\n",
      "12:45:19 INFO: Processing Epoch 6/10\n",
      "12:45:19 INFO: =========================\n",
      "12:51:17 INFO: Epoch 6/10 - Training Split 1/9 Done - Train Loss: 2.026355\n",
      "12:57:26 INFO: Epoch 6/10 - Training Split 2/9 Done - Train Loss: 2.026364\n",
      "13:03:32 INFO: Epoch 6/10 - Training Split 3/9 Done - Train Loss: 2.025312\n",
      "13:09:42 INFO: Epoch 6/10 - Training Split 4/9 Done - Train Loss: 2.025212\n",
      "13:15:48 INFO: Epoch 6/10 - Training Split 5/9 Done - Train Loss: 2.024471\n",
      "13:21:54 INFO: Epoch 6/10 - Training Split 6/9 Done - Train Loss: 2.024291\n",
      "13:28:01 INFO: Epoch 6/10 - Training Split 7/9 Done - Train Loss: 2.023931\n",
      "13:34:06 INFO: Epoch 6/10 - Training Split 8/9 Done - Train Loss: 2.023714\n",
      "13:40:12 INFO: Epoch 6/10 - Training Split 9/9 Done - Train Loss: 2.023405\n",
      "13:43:24 INFO: Epoch 6/10 Done - Test Loss: 2.027633, Test Accuracy: 0.425144\n",
      "13:43:24 INFO: =========================\n",
      "13:43:24 INFO: Processing Epoch 7/10\n",
      "13:43:24 INFO: =========================\n",
      "13:49:21 INFO: Epoch 7/10 - Training Split 1/9 Done - Train Loss: 2.021880\n",
      "13:55:30 INFO: Epoch 7/10 - Training Split 2/9 Done - Train Loss: 2.021543\n",
      "14:01:34 INFO: Epoch 7/10 - Training Split 3/9 Done - Train Loss: 2.021204\n",
      "14:07:38 INFO: Epoch 7/10 - Training Split 4/9 Done - Train Loss: 2.020917\n",
      "14:13:43 INFO: Epoch 7/10 - Training Split 5/9 Done - Train Loss: 2.020247\n",
      "14:19:50 INFO: Epoch 7/10 - Training Split 6/9 Done - Train Loss: 2.020182\n",
      "14:25:55 INFO: Epoch 7/10 - Training Split 7/9 Done - Train Loss: 2.020053\n",
      "14:32:02 INFO: Epoch 7/10 - Training Split 8/9 Done - Train Loss: 2.019799\n",
      "14:38:06 INFO: Epoch 7/10 - Training Split 9/9 Done - Train Loss: 2.019610\n",
      "14:41:20 INFO: Epoch 7/10 Done - Test Loss: 2.026283, Test Accuracy: 0.426700\n",
      "14:41:20 INFO: =========================\n",
      "14:41:20 INFO: Processing Epoch 8/10\n",
      "14:41:20 INFO: =========================\n",
      "14:47:17 INFO: Epoch 8/10 - Training Split 1/9 Done - Train Loss: 2.016018\n",
      "14:53:22 INFO: Epoch 8/10 - Training Split 2/9 Done - Train Loss: 2.016927\n",
      "14:59:26 INFO: Epoch 8/10 - Training Split 3/9 Done - Train Loss: 2.016730\n",
      "15:05:31 INFO: Epoch 8/10 - Training Split 4/9 Done - Train Loss: 2.017377\n",
      "15:11:35 INFO: Epoch 8/10 - Training Split 5/9 Done - Train Loss: 2.017080\n",
      "15:17:43 INFO: Epoch 8/10 - Training Split 6/9 Done - Train Loss: 2.016981\n",
      "15:23:49 INFO: Epoch 8/10 - Training Split 7/9 Done - Train Loss: 2.016729\n",
      "15:29:57 INFO: Epoch 8/10 - Training Split 8/9 Done - Train Loss: 2.016495\n",
      "15:36:05 INFO: Epoch 8/10 - Training Split 9/9 Done - Train Loss: 2.016308\n",
      "15:39:14 INFO: Epoch 8/10 Done - Test Loss: 2.023327, Test Accuracy: 0.430011\n",
      "15:39:14 INFO: =========================\n",
      "15:39:14 INFO: Processing Epoch 9/10\n",
      "15:39:14 INFO: =========================\n",
      "15:45:11 INFO: Epoch 9/10 - Training Split 1/9 Done - Train Loss: 2.014871\n",
      "15:51:19 INFO: Epoch 9/10 - Training Split 2/9 Done - Train Loss: 2.015177\n",
      "15:57:23 INFO: Epoch 9/10 - Training Split 3/9 Done - Train Loss: 2.014856\n",
      "16:03:28 INFO: Epoch 9/10 - Training Split 4/9 Done - Train Loss: 2.014802\n",
      "16:09:34 INFO: Epoch 9/10 - Training Split 5/9 Done - Train Loss: 2.014487\n",
      "16:15:41 INFO: Epoch 9/10 - Training Split 6/9 Done - Train Loss: 2.014527\n",
      "16:21:45 INFO: Epoch 9/10 - Training Split 7/9 Done - Train Loss: 2.014382\n",
      "16:27:49 INFO: Epoch 9/10 - Training Split 8/9 Done - Train Loss: 2.013949\n",
      "16:33:53 INFO: Epoch 9/10 - Training Split 9/9 Done - Train Loss: 2.013781\n",
      "16:37:03 INFO: Epoch 9/10 Done - Test Loss: 2.024405, Test Accuracy: 0.427989\n",
      "16:37:03 INFO: =========================\n",
      "16:37:03 INFO: Processing Epoch 10/10\n",
      "16:37:03 INFO: =========================\n",
      "16:43:00 INFO: Epoch 10/10 - Training Split 1/9 Done - Train Loss: 2.012993\n",
      "16:49:04 INFO: Epoch 10/10 - Training Split 2/9 Done - Train Loss: 2.012751\n",
      "16:55:12 INFO: Epoch 10/10 - Training Split 3/9 Done - Train Loss: 2.012196\n",
      "17:01:16 INFO: Epoch 10/10 - Training Split 4/9 Done - Train Loss: 2.012639\n",
      "17:07:20 INFO: Epoch 10/10 - Training Split 5/9 Done - Train Loss: 2.012547\n",
      "17:13:24 INFO: Epoch 10/10 - Training Split 6/9 Done - Train Loss: 2.012646\n",
      "17:19:27 INFO: Epoch 10/10 - Training Split 7/9 Done - Train Loss: 2.012528\n",
      "17:25:36 INFO: Epoch 10/10 - Training Split 8/9 Done - Train Loss: 2.012264\n",
      "17:31:53 INFO: Epoch 10/10 - Training Split 9/9 Done - Train Loss: 2.012152\n",
      "17:35:08 INFO: Epoch 10/10 Done - Test Loss: 2.026817, Test Accuracy: 0.426322\n"
     ]
    }
   ],
   "source": [
    "model = Multi_Seq_LSTM_Classifier([160, 128, 128], [256, 256, 256], 10)\n",
    "checkpoint_dir = os.path.join(model_path, 'Multi_Seq_LSTM_Classifier_Creative_Advertiser_Product_Age')\n",
    "checkpoint_prefix = 'Multi_Seq_LSTM_Classifier_Creative_Advertiser_Product_Age'\n",
    "model.load_state_dict(torch.load(os.path.join(checkpoint_dir, f'{checkpoint_prefix}_5.pth')))\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "device = DEVICE\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "train_age(model, loss_fn, optimizer, device, checkpoint_dir, checkpoint_prefix, logger=logger, epoch_start=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:35:49 INFO: =========================\n",
      "17:35:49 INFO: Processing Epoch 11/15\n",
      "17:35:49 INFO: =========================\n",
      "17:41:47 INFO: Epoch 11/15 - Training Split 1/9 Done - Train Loss: 2.010616\n",
      "17:47:52 INFO: Epoch 11/15 - Training Split 2/9 Done - Train Loss: 2.010386\n",
      "17:53:55 INFO: Epoch 11/15 - Training Split 3/9 Done - Train Loss: 2.009978\n",
      "17:59:57 INFO: Epoch 11/15 - Training Split 4/9 Done - Train Loss: 2.010627\n",
      "18:05:58 INFO: Epoch 11/15 - Training Split 5/9 Done - Train Loss: 2.010074\n",
      "18:11:59 INFO: Epoch 11/15 - Training Split 6/9 Done - Train Loss: 2.010085\n",
      "18:18:01 INFO: Epoch 11/15 - Training Split 7/9 Done - Train Loss: 2.009814\n",
      "18:24:03 INFO: Epoch 11/15 - Training Split 8/9 Done - Train Loss: 2.009437\n",
      "18:30:08 INFO: Epoch 11/15 - Training Split 9/9 Done - Train Loss: 2.009426\n",
      "18:33:17 INFO: Epoch 11/15 Done - Test Loss: 2.024000, Test Accuracy: 0.429111\n",
      "18:33:17 INFO: =========================\n",
      "18:33:17 INFO: Processing Epoch 12/15\n",
      "18:33:17 INFO: =========================\n",
      "18:39:15 INFO: Epoch 12/15 - Training Split 1/9 Done - Train Loss: 2.007922\n",
      "18:45:17 INFO: Epoch 12/15 - Training Split 2/9 Done - Train Loss: 2.008054\n",
      "18:51:22 INFO: Epoch 12/15 - Training Split 3/9 Done - Train Loss: 2.007950\n",
      "18:57:27 INFO: Epoch 12/15 - Training Split 4/9 Done - Train Loss: 2.008134\n",
      "19:03:28 INFO: Epoch 12/15 - Training Split 5/9 Done - Train Loss: 2.007977\n",
      "19:09:29 INFO: Epoch 12/15 - Training Split 6/9 Done - Train Loss: 2.007858\n",
      "19:15:30 INFO: Epoch 12/15 - Training Split 7/9 Done - Train Loss: 2.007831\n",
      "19:21:31 INFO: Epoch 12/15 - Training Split 8/9 Done - Train Loss: 2.007458\n",
      "19:27:37 INFO: Epoch 12/15 - Training Split 9/9 Done - Train Loss: 2.007276\n",
      "19:30:45 INFO: Epoch 12/15 Done - Test Loss: 2.022471, Test Accuracy: 0.430989\n",
      "19:30:45 INFO: =========================\n",
      "19:30:45 INFO: Processing Epoch 13/15\n",
      "19:30:45 INFO: =========================\n",
      "19:36:40 INFO: Epoch 13/15 - Training Split 1/9 Done - Train Loss: 2.006155\n",
      "19:42:42 INFO: Epoch 13/15 - Training Split 2/9 Done - Train Loss: 2.006398\n",
      "19:48:42 INFO: Epoch 13/15 - Training Split 3/9 Done - Train Loss: 2.006263\n",
      "19:54:43 INFO: Epoch 13/15 - Training Split 4/9 Done - Train Loss: 2.006475\n",
      "20:00:44 INFO: Epoch 13/15 - Training Split 5/9 Done - Train Loss: 2.006024\n",
      "20:06:45 INFO: Epoch 13/15 - Training Split 6/9 Done - Train Loss: 2.006409\n",
      "20:12:47 INFO: Epoch 13/15 - Training Split 7/9 Done - Train Loss: 2.006184\n",
      "20:18:47 INFO: Epoch 13/15 - Training Split 8/9 Done - Train Loss: 2.005833\n",
      "20:24:48 INFO: Epoch 13/15 - Training Split 9/9 Done - Train Loss: 2.005709\n",
      "20:27:58 INFO: Epoch 13/15 Done - Test Loss: 2.022355, Test Accuracy: 0.431411\n",
      "20:27:58 INFO: =========================\n",
      "20:27:58 INFO: Processing Epoch 14/15\n",
      "20:27:58 INFO: =========================\n",
      "20:33:52 INFO: Epoch 14/15 - Training Split 1/9 Done - Train Loss: 2.004904\n",
      "20:39:54 INFO: Epoch 14/15 - Training Split 2/9 Done - Train Loss: 2.005357\n",
      "20:45:56 INFO: Epoch 14/15 - Training Split 3/9 Done - Train Loss: 2.004699\n",
      "20:51:57 INFO: Epoch 14/15 - Training Split 4/9 Done - Train Loss: 2.004969\n",
      "20:57:58 INFO: Epoch 14/15 - Training Split 5/9 Done - Train Loss: 2.004277\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "transform: failed to synchronize: cudaErrorLaunchFailure: unspecified launch failure",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-487b6de53c8a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mtrain_age\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint_prefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-4359ddf4a48e>\u001b[0m in \u001b[0;36mtrain_age\u001b[1;34m(model, loss_fn, optimizer, device, checkpoint_dir, checkpoint_prefix, logger, epoch_start)\u001b[0m\n\u001b[0;32m     34\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yifan wu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \"\"\"\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yifan wu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mRuntimeError\u001b[0m: transform: failed to synchronize: cudaErrorLaunchFailure: unspecified launch failure"
     ]
    }
   ],
   "source": [
    "model = Multi_Seq_LSTM_Classifier([160, 128, 128], [256, 256, 256], 10)\n",
    "checkpoint_dir = os.path.join(model_path, 'Multi_Seq_LSTM_Classifier_Creative_Advertiser_Product_Age')\n",
    "checkpoint_prefix = 'Multi_Seq_LSTM_Classifier_Creative_Advertiser_Product_Age'\n",
    "model.load_state_dict(torch.load(os.path.join(checkpoint_dir, f'{checkpoint_prefix}_10.pth')))\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "device = DEVICE\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "train_age(model, loss_fn, optimizer, device, checkpoint_dir, checkpoint_prefix, logger=logger, epoch_start=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21:12:54 INFO: =========================\n",
      "21:12:54 INFO: Processing Epoch 14/15\n",
      "21:12:54 INFO: =========================\n",
      "21:18:51 INFO: Epoch 14/15 - Training Split 1/9 Done - Train Loss: 2.005732\n",
      "21:25:04 INFO: Epoch 14/15 - Training Split 2/9 Done - Train Loss: 2.006485\n",
      "21:31:11 INFO: Epoch 14/15 - Training Split 3/9 Done - Train Loss: 2.006036\n",
      "21:37:18 INFO: Epoch 14/15 - Training Split 4/9 Done - Train Loss: 2.005656\n",
      "21:43:22 INFO: Epoch 14/15 - Training Split 5/9 Done - Train Loss: 2.005224\n",
      "21:49:28 INFO: Epoch 14/15 - Training Split 6/9 Done - Train Loss: 2.005255\n",
      "21:55:35 INFO: Epoch 14/15 - Training Split 7/9 Done - Train Loss: 2.004757\n",
      "22:01:40 INFO: Epoch 14/15 - Training Split 8/9 Done - Train Loss: 2.004534\n",
      "22:07:44 INFO: Epoch 14/15 - Training Split 9/9 Done - Train Loss: 2.004436\n",
      "22:10:51 INFO: Epoch 14/15 Done - Test Loss: 2.021484, Test Accuracy: 0.432322\n",
      "22:10:51 INFO: =========================\n",
      "22:10:51 INFO: Processing Epoch 15/15\n",
      "22:10:51 INFO: =========================\n",
      "22:16:48 INFO: Epoch 15/15 - Training Split 1/9 Done - Train Loss: 2.003708\n",
      "22:22:51 INFO: Epoch 15/15 - Training Split 2/9 Done - Train Loss: 2.003860\n",
      "22:28:58 INFO: Epoch 15/15 - Training Split 3/9 Done - Train Loss: 2.003353\n",
      "22:35:02 INFO: Epoch 15/15 - Training Split 4/9 Done - Train Loss: 2.003456\n",
      "22:41:09 INFO: Epoch 15/15 - Training Split 5/9 Done - Train Loss: 2.003005\n",
      "22:47:16 INFO: Epoch 15/15 - Training Split 6/9 Done - Train Loss: 2.003044\n",
      "22:53:23 INFO: Epoch 15/15 - Training Split 7/9 Done - Train Loss: 2.002601\n",
      "22:59:27 INFO: Epoch 15/15 - Training Split 8/9 Done - Train Loss: 2.002258\n",
      "23:05:32 INFO: Epoch 15/15 - Training Split 9/9 Done - Train Loss: 2.002360\n",
      "23:08:43 INFO: Epoch 15/15 Done - Test Loss: 2.021504, Test Accuracy: 0.432089\n"
     ]
    }
   ],
   "source": [
    "model = Multi_Seq_LSTM_Classifier([160, 128, 128], [256, 256, 256], 10)\n",
    "checkpoint_dir = os.path.join(model_path, 'Multi_Seq_LSTM_Classifier_Creative_Advertiser_Product_Age')\n",
    "checkpoint_prefix = 'Multi_Seq_LSTM_Classifier_Creative_Advertiser_Product_Age'\n",
    "model.load_state_dict(torch.load(os.path.join(checkpoint_dir, f'{checkpoint_prefix}_13.pth')))\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "device = DEVICE\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "EPOCHES = 2\n",
    "\n",
    "train_age(model, loss_fn, optimizer, device, checkpoint_dir, checkpoint_prefix, logger=logger, epoch_start=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:11:50 INFO: =========================\n",
      "09:11:50 INFO: Processing Epoch 16/20\n",
      "09:11:50 INFO: =========================\n",
      "09:17:21 INFO: Epoch 16/20 - Training Split 1/9 Done - Train Loss: 2.000179\n",
      "09:23:08 INFO: Epoch 16/20 - Training Split 2/9 Done - Train Loss: 2.001224\n",
      "09:28:49 INFO: Epoch 16/20 - Training Split 3/9 Done - Train Loss: 2.000410\n",
      "09:34:27 INFO: Epoch 16/20 - Training Split 4/9 Done - Train Loss: 1.999564\n",
      "09:40:08 INFO: Epoch 16/20 - Training Split 5/9 Done - Train Loss: 1.998789\n",
      "09:45:46 INFO: Epoch 16/20 - Training Split 6/9 Done - Train Loss: 1.998665\n",
      "09:51:23 INFO: Epoch 16/20 - Training Split 7/9 Done - Train Loss: 1.998106\n",
      "09:57:02 INFO: Epoch 16/20 - Training Split 8/9 Done - Train Loss: 1.997599\n",
      "10:02:42 INFO: Epoch 16/20 - Training Split 9/9 Done - Train Loss: 1.997302\n",
      "10:05:41 INFO: Epoch 16/20 Done - Test Loss: 2.021721, Test Accuracy: 0.431467\n",
      "10:05:41 INFO: =========================\n",
      "10:05:41 INFO: Processing Epoch 17/20\n",
      "10:05:41 INFO: =========================\n",
      "10:11:17 INFO: Epoch 17/20 - Training Split 1/9 Done - Train Loss: 1.993939\n",
      "10:16:57 INFO: Epoch 17/20 - Training Split 2/9 Done - Train Loss: 1.995548\n",
      "10:22:40 INFO: Epoch 17/20 - Training Split 3/9 Done - Train Loss: 1.995576\n",
      "10:28:21 INFO: Epoch 17/20 - Training Split 4/9 Done - Train Loss: 1.995751\n",
      "10:34:00 INFO: Epoch 17/20 - Training Split 5/9 Done - Train Loss: 1.995250\n",
      "10:39:41 INFO: Epoch 17/20 - Training Split 6/9 Done - Train Loss: 1.995491\n",
      "10:45:24 INFO: Epoch 17/20 - Training Split 7/9 Done - Train Loss: 1.994945\n",
      "10:51:05 INFO: Epoch 17/20 - Training Split 8/9 Done - Train Loss: 1.994590\n",
      "10:56:46 INFO: Epoch 17/20 - Training Split 9/9 Done - Train Loss: 1.994392\n",
      "10:59:44 INFO: Epoch 17/20 Done - Test Loss: 2.020300, Test Accuracy: 0.434144\n",
      "10:59:44 INFO: =========================\n",
      "10:59:44 INFO: Processing Epoch 18/20\n",
      "10:59:44 INFO: =========================\n",
      "11:05:16 INFO: Epoch 18/20 - Training Split 1/9 Done - Train Loss: 1.991912\n",
      "11:10:57 INFO: Epoch 18/20 - Training Split 2/9 Done - Train Loss: 1.993284\n",
      "11:16:38 INFO: Epoch 18/20 - Training Split 3/9 Done - Train Loss: 1.993252\n",
      "11:22:21 INFO: Epoch 18/20 - Training Split 4/9 Done - Train Loss: 1.993482\n",
      "11:28:02 INFO: Epoch 18/20 - Training Split 5/9 Done - Train Loss: 1.992807\n",
      "11:33:46 INFO: Epoch 18/20 - Training Split 6/9 Done - Train Loss: 1.992750\n",
      "11:39:25 INFO: Epoch 18/20 - Training Split 7/9 Done - Train Loss: 1.992508\n",
      "11:45:06 INFO: Epoch 18/20 - Training Split 8/9 Done - Train Loss: 1.992031\n",
      "11:50:47 INFO: Epoch 18/20 - Training Split 9/9 Done - Train Loss: 1.991980\n",
      "11:53:45 INFO: Epoch 18/20 Done - Test Loss: 2.022652, Test Accuracy: 0.431100\n",
      "11:53:45 INFO: =========================\n",
      "11:53:45 INFO: Processing Epoch 19/20\n",
      "11:53:45 INFO: =========================\n",
      "11:59:18 INFO: Epoch 19/20 - Training Split 1/9 Done - Train Loss: 1.990639\n",
      "12:04:58 INFO: Epoch 19/20 - Training Split 2/9 Done - Train Loss: 1.992547\n",
      "12:10:40 INFO: Epoch 19/20 - Training Split 3/9 Done - Train Loss: 1.992149\n",
      "12:16:21 INFO: Epoch 19/20 - Training Split 4/9 Done - Train Loss: 1.991897\n",
      "12:22:02 INFO: Epoch 19/20 - Training Split 5/9 Done - Train Loss: 1.991019\n",
      "12:27:43 INFO: Epoch 19/20 - Training Split 6/9 Done - Train Loss: 1.991079\n",
      "12:33:24 INFO: Epoch 19/20 - Training Split 7/9 Done - Train Loss: 1.990763\n",
      "12:39:07 INFO: Epoch 19/20 - Training Split 8/9 Done - Train Loss: 1.990384\n",
      "12:44:51 INFO: Epoch 19/20 - Training Split 9/9 Done - Train Loss: 1.990371\n",
      "12:47:52 INFO: Epoch 19/20 Done - Test Loss: 2.021840, Test Accuracy: 0.432744\n",
      "12:47:52 INFO: =========================\n",
      "12:47:52 INFO: Processing Epoch 20/20\n",
      "12:47:52 INFO: =========================\n",
      "12:53:26 INFO: Epoch 20/20 - Training Split 1/9 Done - Train Loss: 1.988667\n",
      "12:59:05 INFO: Epoch 20/20 - Training Split 2/9 Done - Train Loss: 1.989691\n",
      "13:04:46 INFO: Epoch 20/20 - Training Split 3/9 Done - Train Loss: 1.989020\n",
      "13:10:26 INFO: Epoch 20/20 - Training Split 4/9 Done - Train Loss: 1.989076\n",
      "13:16:06 INFO: Epoch 20/20 - Training Split 5/9 Done - Train Loss: 1.988738\n",
      "13:21:47 INFO: Epoch 20/20 - Training Split 6/9 Done - Train Loss: 1.988779\n",
      "13:27:30 INFO: Epoch 20/20 - Training Split 7/9 Done - Train Loss: 1.988719\n",
      "13:33:10 INFO: Epoch 20/20 - Training Split 8/9 Done - Train Loss: 1.988339\n",
      "13:38:54 INFO: Epoch 20/20 - Training Split 9/9 Done - Train Loss: 1.988219\n",
      "13:41:51 INFO: Epoch 20/20 Done - Test Loss: 2.020726, Test Accuracy: 0.433778\n"
     ]
    }
   ],
   "source": [
    "model = Multi_Seq_LSTM_Classifier([160, 128, 128], [256, 256, 256], 10)\n",
    "checkpoint_dir = os.path.join(model_path, 'Multi_Seq_LSTM_Classifier_Creative_Advertiser_Product_Age')\n",
    "checkpoint_prefix = 'Multi_Seq_LSTM_Classifier_Creative_Advertiser_Product_Age'\n",
    "model.load_state_dict(torch.load(os.path.join(checkpoint_dir, f'{checkpoint_prefix}_15.pth')))\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "device = DEVICE\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "EPOCHES = 5\n",
    "BATCH_SIZE = 384\n",
    "div, mod = divmod(90000, BATCH_SIZE)\n",
    "N_BATCH = div + min(mod, 1)\n",
    "\n",
    "train_age(model, loss_fn, optimizer, device, checkpoint_dir, checkpoint_prefix, logger=logger, epoch_start=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:02:59 INFO: =========================\n",
      "14:02:59 INFO: Processing Epoch 21/22\n",
      "14:02:59 INFO: =========================\n",
      "14:08:31 INFO: Epoch 21/22 - Training Split 1/9 Done - Train Loss: 1.988061\n",
      "14:14:19 INFO: Epoch 21/22 - Training Split 2/9 Done - Train Loss: 1.989234\n",
      "14:20:02 INFO: Epoch 21/22 - Training Split 3/9 Done - Train Loss: 1.988502\n",
      "14:25:44 INFO: Epoch 21/22 - Training Split 4/9 Done - Train Loss: 1.988246\n",
      "14:31:26 INFO: Epoch 21/22 - Training Split 5/9 Done - Train Loss: 1.987627\n",
      "14:37:07 INFO: Epoch 21/22 - Training Split 6/9 Done - Train Loss: 1.987611\n",
      "14:42:47 INFO: Epoch 21/22 - Training Split 7/9 Done - Train Loss: 1.987493\n",
      "14:48:26 INFO: Epoch 21/22 - Training Split 8/9 Done - Train Loss: 1.987094\n",
      "14:54:07 INFO: Epoch 21/22 - Training Split 9/9 Done - Train Loss: 1.987001\n",
      "14:57:08 INFO: Epoch 21/22 Done - Test Loss: 2.022222, Test Accuracy: 0.432000\n",
      "14:57:08 INFO: =========================\n",
      "14:57:08 INFO: Processing Epoch 22/22\n",
      "14:57:08 INFO: =========================\n",
      "15:02:48 INFO: Epoch 22/22 - Training Split 1/9 Done - Train Loss: 1.984551\n",
      "15:08:33 INFO: Epoch 22/22 - Training Split 2/9 Done - Train Loss: 1.985963\n",
      "15:14:19 INFO: Epoch 22/22 - Training Split 3/9 Done - Train Loss: 1.986045\n",
      "15:20:02 INFO: Epoch 22/22 - Training Split 4/9 Done - Train Loss: 1.986595\n",
      "15:25:45 INFO: Epoch 22/22 - Training Split 5/9 Done - Train Loss: 1.986589\n",
      "15:31:29 INFO: Epoch 22/22 - Training Split 6/9 Done - Train Loss: 1.986701\n",
      "15:37:11 INFO: Epoch 22/22 - Training Split 7/9 Done - Train Loss: 1.986563\n",
      "15:42:55 INFO: Epoch 22/22 - Training Split 8/9 Done - Train Loss: 1.986217\n",
      "15:48:39 INFO: Epoch 22/22 - Training Split 9/9 Done - Train Loss: 1.986141\n",
      "15:51:38 INFO: Epoch 22/22 Done - Test Loss: 2.020091, Test Accuracy: 0.434556\n"
     ]
    }
   ],
   "source": [
    "model = Multi_Seq_LSTM_Classifier([160, 128, 128], [256, 256, 256], 10)\n",
    "checkpoint_dir = os.path.join(model_path, 'Multi_Seq_LSTM_Classifier_Creative_Advertiser_Product_Age')\n",
    "checkpoint_prefix = 'Multi_Seq_LSTM_Classifier_Creative_Advertiser_Product_Age'\n",
    "model.load_state_dict(torch.load(os.path.join(checkpoint_dir, f'{checkpoint_prefix}_20.pth')))\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "device = DEVICE\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "EPOCHES = 2\n",
    "BATCH_SIZE = 384\n",
    "div, mod = divmod(90000, BATCH_SIZE)\n",
    "N_BATCH = div + min(mod, 1)\n",
    "\n",
    "train_age(model, loss_fn, optimizer, device, checkpoint_dir, checkpoint_prefix, logger=logger, epoch_start=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Gender Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHES = 5\n",
    "BATCH_SIZE = 384\n",
    "div, mod = divmod(90000, BATCH_SIZE)\n",
    "N_BATCH = div + min(mod, 1)\n",
    "\n",
    "def train_gender(model, loss_fn, optimizer, device, checkpoint_dir, checkpoint_prefix, logger=None, epoch_start=0):\n",
    "    global EPOCHES, BATCH_SIZE, N_BATCH, TEST_SIZE\n",
    "    if not os.path.isdir(checkpoint_dir):\n",
    "        os.mkdir(checkpoint_dir)\n",
    "    \n",
    "    for epoch in range(1+epoch_start, EPOCHES+1+epoch_start):\n",
    "        if logger: \n",
    "            logger.info('=========================')\n",
    "            logger.info(f'Processing Epoch {epoch}/{EPOCHES+epoch_start}')\n",
    "            logger.info('=========================')\n",
    "            \n",
    "        train_file = [1,2,3,4,5,6,7,8,9]\n",
    "        test_file = [10]\n",
    "            \n",
    "        train_running_loss, train_n_batch = 0, 0\n",
    "        pred_y, true_y = [], []\n",
    "        for index, split_id in enumerate(train_file, start=1):\n",
    "            out_age, out_gender, inp_creative_seq, inp_advertiser_seq, inp_product_seq, inp_last_idx = prepare_train(split_id)\n",
    "            model.train()\n",
    "            \n",
    "            for batch_index in range(N_BATCH):\n",
    "                x1 = torch.nn.utils.rnn.pad_sequence(inp_creative_seq[batch_index*BATCH_SIZE:(batch_index+1)*BATCH_SIZE], batch_first=True, padding_value=0).to(device)\n",
    "                x2 = torch.nn.utils.rnn.pad_sequence(inp_advertiser_seq[batch_index*BATCH_SIZE:(batch_index+1)*BATCH_SIZE], batch_first=True, padding_value=0).to(device)\n",
    "                x3 = torch.nn.utils.rnn.pad_sequence(inp_product_seq[batch_index*BATCH_SIZE:(batch_index+1)*BATCH_SIZE], batch_first=True, padding_value=0).to(device)\n",
    "                x4 = inp_last_idx[batch_index*BATCH_SIZE:(batch_index+1)*BATCH_SIZE]\n",
    "                y = out_gender[batch_index*BATCH_SIZE:(batch_index+1)*BATCH_SIZE].to(device)\n",
    "                optimizer.zero_grad()\n",
    "                yp = F.softmax(model(x1, x2, x3, x4), 1)\n",
    "                loss = loss_fn(yp, y)\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=100)\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_running_loss += loss.item()\n",
    "                train_n_batch += 1\n",
    "                \n",
    "                del x1, x2, x3, x4, y, yp\n",
    "                _ = gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            if logger:\n",
    "                logger.info(f'Epoch {epoch}/{EPOCHES+epoch_start} - Training Split {index}/{len(train_file)} Done - Train Loss: {train_running_loss/train_n_batch:.6f}')\n",
    "            \n",
    "            del out_age, out_gender, inp_creative_seq, inp_advertiser_seq, inp_product_seq, inp_last_idx\n",
    "            _ = gc.collect()\n",
    "            torch.cuda.empty_cache()   \n",
    "        \n",
    "        model.eval()\n",
    "        test_running_loss, test_n_batch = 0, 0\n",
    "        true_y, pred_y = [], []\n",
    "        \n",
    "        for index, split_id in enumerate(test_file, start=1):\n",
    "            out_age, out_gender, inp_creative_seq, inp_advertiser_seq, inp_product_seq, inp_last_idx = prepare_train(split_id)\n",
    "            for batch_index in range(N_BATCH):\n",
    "                x1 = torch.nn.utils.rnn.pad_sequence(inp_creative_seq[batch_index*BATCH_SIZE:(batch_index+1)*BATCH_SIZE], batch_first=True, padding_value=0).to(device)\n",
    "                x2 = torch.nn.utils.rnn.pad_sequence(inp_advertiser_seq[batch_index*BATCH_SIZE:(batch_index+1)*BATCH_SIZE], batch_first=True, padding_value=0).to(device)\n",
    "                x3 = torch.nn.utils.rnn.pad_sequence(inp_product_seq[batch_index*BATCH_SIZE:(batch_index+1)*BATCH_SIZE], batch_first=True, padding_value=0).to(device)\n",
    "                x4 = inp_last_idx[batch_index*BATCH_SIZE:(batch_index+1)*BATCH_SIZE]\n",
    "                y = out_gender[batch_index*BATCH_SIZE:(batch_index+1)*BATCH_SIZE].to(device)\n",
    "                yp = F.softmax(model(x1, x2, x3, x4), 1)\n",
    "                loss = loss_fn(yp, y)\n",
    "            \n",
    "                test_running_loss += loss.item()\n",
    "                test_n_batch += 1\n",
    "            \n",
    "                pred_y.extend(list(yp.cpu().detach().numpy()))\n",
    "                true_y.extend(list(y.cpu().detach().numpy()))\n",
    "            \n",
    "                del x1, x2, x3, x4, y, yp\n",
    "                _ = gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            del out_age, out_gender, inp_creative_seq, inp_advertiser_seq, inp_product_seq, inp_last_idx\n",
    "            _ = gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        pred = np.argmax(np.array(pred_y), 1)\n",
    "        true = np.array(true_y).reshape((-1,))\n",
    "        acc_score = accuracy_score(true, pred)\n",
    "        \n",
    "        if logger:\n",
    "            logger.info(f'Epoch {epoch}/{EPOCHES+epoch_start} Done - Test Loss: {test_running_loss/test_n_batch:.6f}, Test Accuracy: {acc_score:.6f}')\n",
    "            \n",
    "        ck_file_name = f'{checkpoint_prefix}_{epoch}.pth'\n",
    "        ck_file_path = os.path.join(checkpoint_dir, ck_file_name)\n",
    "        \n",
    "        torch.save(model.state_dict(), ck_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:24:21 INFO: =========================\n",
      "16:24:21 INFO: Processing Epoch 1/5\n",
      "16:24:21 INFO: =========================\n",
      "16:29:54 INFO: Epoch 1/5 - Training Split 1/9 Done - Train Loss: 0.409208\n",
      "16:35:39 INFO: Epoch 1/5 - Training Split 2/9 Done - Train Loss: 0.399475\n",
      "16:41:22 INFO: Epoch 1/5 - Training Split 3/9 Done - Train Loss: 0.394416\n",
      "16:47:02 INFO: Epoch 1/5 - Training Split 4/9 Done - Train Loss: 0.390832\n",
      "16:52:43 INFO: Epoch 1/5 - Training Split 5/9 Done - Train Loss: 0.388329\n",
      "16:58:23 INFO: Epoch 1/5 - Training Split 6/9 Done - Train Loss: 0.386520\n",
      "17:04:02 INFO: Epoch 1/5 - Training Split 7/9 Done - Train Loss: 0.385111\n",
      "17:09:37 INFO: Epoch 1/5 - Training Split 8/9 Done - Train Loss: 0.384008\n",
      "17:15:14 INFO: Epoch 1/5 - Training Split 9/9 Done - Train Loss: 0.383014\n",
      "17:18:09 INFO: Epoch 1/5 Done - Test Loss: 0.375178, Test Accuracy: 0.935744\n",
      "17:18:09 INFO: =========================\n",
      "17:18:09 INFO: Processing Epoch 2/5\n",
      "17:18:09 INFO: =========================\n",
      "17:23:38 INFO: Epoch 2/5 - Training Split 1/9 Done - Train Loss: 0.374973\n",
      "17:29:17 INFO: Epoch 2/5 - Training Split 2/9 Done - Train Loss: 0.375011\n",
      "17:34:53 INFO: Epoch 2/5 - Training Split 3/9 Done - Train Loss: 0.374788\n",
      "17:40:32 INFO: Epoch 2/5 - Training Split 4/9 Done - Train Loss: 0.374373\n",
      "17:46:15 INFO: Epoch 2/5 - Training Split 5/9 Done - Train Loss: 0.374170\n",
      "17:51:56 INFO: Epoch 2/5 - Training Split 6/9 Done - Train Loss: 0.373936\n",
      "17:57:35 INFO: Epoch 2/5 - Training Split 7/9 Done - Train Loss: 0.373748\n",
      "18:03:17 INFO: Epoch 2/5 - Training Split 8/9 Done - Train Loss: 0.373560\n",
      "18:08:56 INFO: Epoch 2/5 - Training Split 9/9 Done - Train Loss: 0.373361\n",
      "18:11:54 INFO: Epoch 2/5 Done - Test Loss: 0.372648, Test Accuracy: 0.938489\n",
      "18:11:54 INFO: =========================\n",
      "18:11:54 INFO: Processing Epoch 3/5\n",
      "18:11:54 INFO: =========================\n",
      "18:17:26 INFO: Epoch 3/5 - Training Split 1/9 Done - Train Loss: 0.372012\n",
      "18:23:04 INFO: Epoch 3/5 - Training Split 2/9 Done - Train Loss: 0.372322\n",
      "18:28:43 INFO: Epoch 3/5 - Training Split 3/9 Done - Train Loss: 0.372106\n",
      "18:34:25 INFO: Epoch 3/5 - Training Split 4/9 Done - Train Loss: 0.371626\n",
      "18:40:06 INFO: Epoch 3/5 - Training Split 5/9 Done - Train Loss: 0.371449\n",
      "18:45:47 INFO: Epoch 3/5 - Training Split 6/9 Done - Train Loss: 0.371224\n",
      "18:51:25 INFO: Epoch 3/5 - Training Split 7/9 Done - Train Loss: 0.371030\n",
      "18:57:03 INFO: Epoch 3/5 - Training Split 8/9 Done - Train Loss: 0.370943\n",
      "19:02:43 INFO: Epoch 3/5 - Training Split 9/9 Done - Train Loss: 0.370729\n",
      "19:05:41 INFO: Epoch 3/5 Done - Test Loss: 0.371815, Test Accuracy: 0.938600\n",
      "19:05:41 INFO: =========================\n",
      "19:05:41 INFO: Processing Epoch 4/5\n",
      "19:05:41 INFO: =========================\n",
      "19:11:12 INFO: Epoch 4/5 - Training Split 1/9 Done - Train Loss: 0.370028\n",
      "19:16:50 INFO: Epoch 4/5 - Training Split 2/9 Done - Train Loss: 0.370225\n",
      "19:22:27 INFO: Epoch 4/5 - Training Split 3/9 Done - Train Loss: 0.369828\n",
      "19:28:06 INFO: Epoch 4/5 - Training Split 4/9 Done - Train Loss: 0.369423\n",
      "19:33:44 INFO: Epoch 4/5 - Training Split 5/9 Done - Train Loss: 0.369325\n",
      "19:39:22 INFO: Epoch 4/5 - Training Split 6/9 Done - Train Loss: 0.369098\n",
      "19:45:01 INFO: Epoch 4/5 - Training Split 7/9 Done - Train Loss: 0.368880\n",
      "19:50:40 INFO: Epoch 4/5 - Training Split 8/9 Done - Train Loss: 0.368787\n",
      "19:56:22 INFO: Epoch 4/5 - Training Split 9/9 Done - Train Loss: 0.368626\n",
      "19:59:25 INFO: Epoch 4/5 Done - Test Loss: 0.371240, Test Accuracy: 0.940267\n",
      "19:59:25 INFO: =========================\n",
      "19:59:25 INFO: Processing Epoch 5/5\n",
      "19:59:25 INFO: =========================\n",
      "20:05:00 INFO: Epoch 5/5 - Training Split 1/9 Done - Train Loss: 0.367964\n",
      "20:10:39 INFO: Epoch 5/5 - Training Split 2/9 Done - Train Loss: 0.367958\n",
      "20:16:18 INFO: Epoch 5/5 - Training Split 3/9 Done - Train Loss: 0.367862\n",
      "20:21:59 INFO: Epoch 5/5 - Training Split 4/9 Done - Train Loss: 0.367591\n",
      "20:27:36 INFO: Epoch 5/5 - Training Split 5/9 Done - Train Loss: 0.367541\n",
      "20:33:14 INFO: Epoch 5/5 - Training Split 6/9 Done - Train Loss: 0.367425\n",
      "20:38:51 INFO: Epoch 5/5 - Training Split 7/9 Done - Train Loss: 0.367304\n",
      "20:44:30 INFO: Epoch 5/5 - Training Split 8/9 Done - Train Loss: 0.367207\n",
      "20:50:08 INFO: Epoch 5/5 - Training Split 9/9 Done - Train Loss: 0.367072\n",
      "20:53:03 INFO: Epoch 5/5 Done - Test Loss: 0.372414, Test Accuracy: 0.938378\n"
     ]
    }
   ],
   "source": [
    "model = Multi_Seq_LSTM_Classifier([160, 128, 128], [256, 256, 256], 2).to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "device = DEVICE\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "checkpoint_dir = os.path.join(model_path, 'Multi_Seq_LSTM_Classifier_Creative_Advertiser_Product_Gender')\n",
    "checkpoint_prefix = 'Multi_Seq_LSTM_Classifier_Creative_Advertiser_Product_Gender'\n",
    "\n",
    "train_gender(model, loss_fn, optimizer, device, checkpoint_dir, checkpoint_prefix, logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHES = 5\n",
    "BATCH_SIZE = 512\n",
    "N_BATCH = 90000//BATCH_SIZE\n",
    "TEST_SIZE = 90000%BATCH_SIZE\n",
    "\n",
    "def train_age(model, loss_fn, optimizer, device, checkpoint_dir, checkpoint_prefix, logger=None, epoch_start=0):\n",
    "    global EPOCHES, BATCH_SIZE, N_BATCH, TEST_SIZE\n",
    "    if not os.path.isdir(checkpoint_dir):\n",
    "        os.mkdir(checkpoint_dir)\n",
    "    \n",
    "    for epoch in range(1+epoch_start, EPOCHES+1+epoch_start):\n",
    "        if logger: \n",
    "            logger.info('=========================')\n",
    "            logger.info(f'Processing Epoch {epoch}/{EPOCHES+epoch_start}')\n",
    "            logger.info('=========================')\n",
    "            \n",
    "        train_file = [1,2,3,4,5,6,7,8,9]\n",
    "        test_file = [10]\n",
    "            \n",
    "        train_running_loss, train_n_batch = 0, 0\n",
    "        pred_y, true_y = [], []\n",
    "        for index, split_id in enumerate(train_file, start=1):\n",
    "            out_age, out_gender, inp_creative_seq, inp_advertiser_seq, inp_product_seq, inp_last_idx = prepare_train(split_id)\n",
    "            train_creative_seq, test_creative_seq = inp_creative_seq[:-TEST_SIZE], inp_creative_seq[-TEST_SIZE:]\n",
    "            train_advertiser_seq, test_advertiser_seq = inp_advertiser_seq[:-TEST_SIZE], inp_advertiser_seq[-TEST_SIZE:]\n",
    "            train_product_seq, test_product_seq = inp_product_seq[:-TEST_SIZE], inp_product_seq[-TEST_SIZE:]\n",
    "            train_last_idx, test_last_idx = inp_last_idx[:-TEST_SIZE], inp_last_idx[-TEST_SIZE:]\n",
    "            train_age, test_age = out_age[:-TEST_SIZE], out_age[-TEST_SIZE:]\n",
    "            \n",
    "            model.train()\n",
    "            \n",
    "            for batch_index in range(N_BATCH):\n",
    "                x1 = torch.nn.utils.rnn.pad_sequence(train_creative_seq[batch_index*BATCH_SIZE:(batch_index+1)*BATCH_SIZE], batch_first=True, padding_value=0).to(device)\n",
    "                x2 = torch.nn.utils.rnn.pad_sequence(train_advertiser_seq[batch_index*BATCH_SIZE:(batch_index+1)*BATCH_SIZE], batch_first=True, padding_value=0).to(device)\n",
    "                x3 = torch.nn.utils.rnn.pad_sequence(train_product_seq[batch_index*BATCH_SIZE:(batch_index+1)*BATCH_SIZE], batch_first=True, padding_value=0).to(device)\n",
    "                x4 = train_last_idx[batch_index*BATCH_SIZE:(batch_index+1)*BATCH_SIZE]\n",
    "                y = train_age[batch_index*BATCH_SIZE:(batch_index+1)*BATCH_SIZE].to(device)\n",
    "                optimizer.zero_grad()\n",
    "                yp = F.softmax(model(x1, x2, x3, x4), 1)\n",
    "                loss = loss_fn(yp, y)\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=100)\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_running_loss += loss.item()\n",
    "                train_n_batch += 1\n",
    "                \n",
    "                del x1, x2, x3, x4, y, yp\n",
    "                _ = gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            model.eval()\n",
    "            \n",
    "            x1 = torch.nn.utils.rnn.pad_sequence(test_creative_seq, batch_first=True, padding_value=0).to(device)\n",
    "            x2 = torch.nn.utils.rnn.pad_sequence(test_advertiser_seq, batch_first=True, padding_value=0).to(device)\n",
    "            x3 = torch.nn.utils.rnn.pad_sequence(test_product_seq, batch_first=True, padding_value=0).to(device)\n",
    "            x4 = test_last_idx\n",
    "            y = test_age.to(device)\n",
    "            yp = F.softmax(model(x1, x2, x3, x4), 1)\n",
    "            loss = loss_fn(yp, y)\n",
    "            \n",
    "            pred_y.extend(list(yp.cpu().detach().numpy()))\n",
    "            true_y.extend(list(y.cpu().detach().numpy()))\n",
    "            \n",
    "            del x1, x2, x3, x4, y, yp\n",
    "            _ = gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            pred = np.argmax(np.array(pred_y), 1)\n",
    "            true = np.array(true_y).reshape((-1,))\n",
    "            acc_score = accuracy_score(true, pred)\n",
    "            \n",
    "            if logger:\n",
    "                logger.info(f'Epoch {epoch}/{EPOCHES+epoch_start} - Training Split {index}/{len(train_file)} Done - Train Loss: {train_running_loss/train_n_batch:.6f}, Val Loss: {loss.item():.6f}, Val Accuracy: {acc_score:.6f}')\n",
    "            \n",
    "            del out_age, out_gender, inp_creative_seq, inp_advertiser_seq, inp_product_seq, inp_last_idx\n",
    "            del train_creative_seq, test_creative_seq, train_advertiser_seq, test_advertiser_seq, train_product_seq, test_product_seq, train_last_idx, test_last_idx, train_age, test_age\n",
    "            _ = gc.collect()\n",
    "            torch.cuda.empty_cache()   \n",
    "        \n",
    "        model.eval()\n",
    "        test_running_loss, test_n_batch = 0, 0\n",
    "        true_y, pred_y = [], []\n",
    "        \n",
    "        for index, split_id in enumerate(test_file, start=1):\n",
    "            out_age, out_gender, inp_creative_seq, inp_advertiser_seq, inp_product_seq, inp_last_idx = prepare_train(split_id)\n",
    "            for batch_index in range(N_BATCH+1):\n",
    "                x1 = torch.nn.utils.rnn.pad_sequence(inp_creative_seq[batch_index*BATCH_SIZE:(batch_index+1)*BATCH_SIZE], batch_first=True, padding_value=0).to(device)\n",
    "                x2 = torch.nn.utils.rnn.pad_sequence(inp_advertiser_seq[batch_index*BATCH_SIZE:(batch_index+1)*BATCH_SIZE], batch_first=True, padding_value=0).to(device)\n",
    "                x3 = torch.nn.utils.rnn.pad_sequence(inp_product_seq[batch_index*BATCH_SIZE:(batch_index+1)*BATCH_SIZE], batch_first=True, padding_value=0).to(device)\n",
    "                x4 = inp_last_idx[batch_index*BATCH_SIZE:(batch_index+1)*BATCH_SIZE]\n",
    "                y = out_age[batch_index*BATCH_SIZE:(batch_index+1)*BATCH_SIZE].to(device)\n",
    "                yp = F.softmax(model(x1, x2, x3, x4), 1)\n",
    "                loss = loss_fn(yp, y)\n",
    "            \n",
    "                test_running_loss += loss.item()\n",
    "                test_n_batch += 1\n",
    "            \n",
    "                pred_y.extend(list(yp.cpu().detach().numpy()))\n",
    "                true_y.extend(list(y.cpu().detach().numpy()))\n",
    "            \n",
    "                del x1, x2, x3, x4, y, yp\n",
    "                _ = gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            del out_age, out_gender, inp_creative_seq, inp_advertiser_seq, inp_product_seq, inp_last_idx\n",
    "            _ = gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        pred = np.argmax(np.array(pred_y), 1)\n",
    "        true = np.array(true_y).reshape((-1,))\n",
    "        acc_score = accuracy_score(true, pred)\n",
    "        \n",
    "        if logger:\n",
    "            logger.info(f'Epoch {epoch}/{EPOCHES+epoch_start} Done - Test Loss: {test_running_loss/test_n_batch:.6f}, Test Accuracy: {acc_score:.6f}')\n",
    "            \n",
    "        ck_file_name = f'{checkpoint_prefix}_{epoch}.pth'\n",
    "        ck_file_path = os.path.join(checkpoint_dir, ck_file_name)\n",
    "        \n",
    "        torch.save(model.state_dict(), ck_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
