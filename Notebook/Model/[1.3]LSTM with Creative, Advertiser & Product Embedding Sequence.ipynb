{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "train_path = '../../raw_train_artifact'\n",
    "test_path = '../../raw_test_artifact'\n",
    "embedding_path = '../../embedding_artifact'\n",
    "input_path = '../../input_artifact'\n",
    "input_split_path = '../../input_artifact/input_split'\n",
    "model_path = '../../model_artifact'\n",
    "output_path = '../../output_artifact'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gc\n",
    "gc.enable()\n",
    "import time\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns',120)\n",
    "pd.set_option('display.max_rows',2000)\n",
    "pd.set_option('precision',5)\n",
    "pd.set_option('float_format', '{:.5f}'.format)\n",
    "\n",
    "import tqdm\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from gensim.models import Word2Vec\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:49:03 INFO: Restart notebook\n",
      "==========================\n",
      "Wed Jun  3 17:49:03 2020\n",
      "==========================\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "log_path = '[1.3]LSTM with Creative, Advertiser & Product Embedding Sequence.log'\n",
    "    \n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)-s: %(message)s', datefmt='%H:%M:%S')\n",
    "\n",
    "fh = logging.FileHandler(log_path)\n",
    "fh.setLevel(logging.INFO)\n",
    "fh.setFormatter(formatter)\n",
    "logger.addHandler(fh)\n",
    "\n",
    "sh = logging.StreamHandler(sys.stdout)\n",
    "sh.setLevel(logging.INFO)\n",
    "sh.setFormatter(formatter)\n",
    "logger.addHandler(sh)\n",
    "\n",
    "logger.info(f'Restart notebook\\n==========================\\n{time.ctime()}\\n==========================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:49:03 INFO: Device in Use: cuda\n",
      "17:49:03 INFO: CUDA Memory: Total 8.00 GB, Cached 0.00 GB, Allocated 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info('Device in Use: {}'.format(DEVICE))\n",
    "torch.cuda.empty_cache()\n",
    "t = torch.cuda.get_device_properties(DEVICE).total_memory/1024**3\n",
    "c = torch.cuda.memory_cached(DEVICE)/1024**3\n",
    "a = torch.cuda.memory_allocated(DEVICE)/1024**3\n",
    "logger.info('CUDA Memory: Total {:.2f} GB, Cached {:.2f} GB, Allocated {:.2f} GB'.format(t,c,a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_embed_artifact = {\n",
    "    'creative': {\n",
    "        'embedding_artifact': r'C:\\JupyterNotebook\\Tencent-Ads-Algo-Comp-2020\\embedding_artifact\\creative_id_embed_s160_w64_cbow_38168zon',\n",
    "        'train_file_prefix': 'train_creative_agg_user',\n",
    "        'test_file_prefix': 'test_creative_agg_user'\n",
    "    },\n",
    "    'ad': {\n",
    "        'embedding_artifact': r'C:\\JupyterNotebook\\Tencent-Ads-Algo-Comp-2020\\embedding_artifact\\ad_id_embed_s160_w64_cbow_ibfi8g78',\n",
    "        'train_file_prefix': 'train_ad_agg_user',\n",
    "        'test_file_prefix': 'test_ad_agg_user'\n",
    "    },\n",
    "    'advertiser': {\n",
    "        'embedding_artifact': r'C:\\JupyterNotebook\\Tencent-Ads-Algo-Comp-2020\\embedding_artifact\\advertiser_id_embed_s128_w64_cbow_n4re8tds',\n",
    "        'train_file_prefix': 'train_advertiser_agg_user',\n",
    "        'test_file_prefix': 'test_advertiser_agg_user'\n",
    "    },\n",
    "    'product': {\n",
    "        'embedding_artifact': r'C:\\JupyterNotebook\\Tencent-Ads-Algo-Comp-2020\\embedding_artifact\\product_id_embed_s128_w64_cbow_8yemmp45',\n",
    "        'train_file_prefix': 'train_product_agg_user',\n",
    "        'test_file_prefix': 'test_product_agg_user'\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_truth(split_id, logger=None):\n",
    "    \"\"\"\n",
    "    Get user id and ground truth\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    \n",
    "    truth_path = os.path.join(input_split_path, f'train_truth_{split_id}.npy')\n",
    "    with open(truth_path, 'rb') as f:\n",
    "        truth = np.load(f)\n",
    "        \n",
    "    inp_user = truth[:,0]\n",
    "    out_age = torch.from_numpy(truth[:,1]).long()\n",
    "    out_gender = torch.from_numpy(truth[:,2]).long()\n",
    "    \n",
    "    del truth\n",
    "    _ = gc.collect()\n",
    "    \n",
    "    if logger: logger.info(f'Target output ready after {time.time()-start:.2f}s')\n",
    "    return inp_user, out_age, out_gender\n",
    "\n",
    "def get_embed_seq(split_id, embed_var, inp_user, max_seq=100, train=True, logger=None):\n",
    "    \"\"\"\n",
    "    Get corresponding embedding sequence\n",
    "    \"\"\"\n",
    "    global inp_embed_artifact, input_split_path\n",
    "    assert embed_var in inp_embed_artifact\n",
    "    \n",
    "    start = time.time()\n",
    "    embedding = Word2Vec.load(inp_embed_artifact[embed_var]['embedding_artifact'])\n",
    "    if logger: logger.info(f'{embed_var.capitalize()} embedding artifact is loaded after {time.time()-start:.2f}s')\n",
    "    start = time.time()\n",
    "    file_prefix = inp_embed_artifact[embed_var]['train_file_prefix'] if train else inp_embed_artifact[embed_var]['test_file_prefix']\n",
    "    raw_path = os.path.join(input_split_path, f'{file_prefix}_{split_id}.json')\n",
    "    with open(raw_path, 'r') as f:\n",
    "        raw = json.load(f)\n",
    "    inp_seq = []\n",
    "    for user in inp_user:\n",
    "        inp_seq.append(torch.from_numpy(np.stack([embedding.wv[key] for key in raw[str(user)][:max_seq]], axis=0)).float())\n",
    "    inp_last_idx = np.array([i.shape[0] for i in inp_seq])-1\n",
    "    \n",
    "    del embedding, raw\n",
    "    _ = gc.collect()\n",
    "    \n",
    "    if logger: logger.info(f'{embed_var.capitalize()} embedding sequence ready after {time.time()-start:.2f}s')\n",
    "    return inp_seq, inp_last_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train(split_id, max_seq=100, logger=None):\n",
    "    \"\"\"\n",
    "    Get ground truth, and embedding sequence for creative, product and advertiser\n",
    "    \"\"\"\n",
    "    if logger: logger.info(f'Preparing Training Split-{split_id}')\n",
    "        \n",
    "    inp_user, out_age, out_gender = get_truth(split_id, logger=logger)\n",
    "    inp_creative_seq, inp_last_idx = get_embed_seq(split_id, 'creative',inp_user, max_seq=max_seq, logger=logger)\n",
    "    inp_advertiser_seq, _ = get_embed_seq(split_id, 'advertiser',inp_user, max_seq=max_seq, logger=logger)\n",
    "    inp_product_seq, _ = get_embed_seq(split_id, 'product',inp_user, max_seq=max_seq, logger=logger)\n",
    "    \n",
    "    del inp_user\n",
    "    _ = gc.collect()\n",
    "    \n",
    "    return out_age, out_gender, inp_creative_seq, inp_advertiser_seq, inp_product_seq, inp_last_idx   \n",
    "\n",
    "def prepare_test(split_id, max_seq=100, logger=None):\n",
    "    global input_split_path\n",
    "    if logger: logger.info(f'Preparing Training Split-{split_id}')\n",
    "        \n",
    "    idx_path = os.path.join(input_split_path, 'test_idx_shuffle.npy')\n",
    "    with open(idx_path, 'rb') as f:\n",
    "        test_idx = np.load(f)\n",
    "    inp_user = test_idx[(split_id-1)*100000:split_id*100000]\n",
    "    del test_idx\n",
    "    _ = gc.collect()\n",
    "    \n",
    "    inp_creative_seq, inp_last_idx = get_embed_seq(split_id, 'creative',inp_user, max_seq=max_seq, train=False, logger=logger)\n",
    "    inp_advertiser_seq, _ = get_embed_seq(split_id, 'advertiser',inp_user, max_seq=max_seq, train=False, logger=logger)\n",
    "    inp_product_seq, _ = get_embed_seq(split_id, 'product',inp_user, max_seq=max_seq, train=False, logger=logger)\n",
    "    \n",
    "    _ = gc.collect()\n",
    "    \n",
    "    return inp_user, inp_creative_seq, inp_advertiser_seq, inp_product_seq, inp_last_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Extraction_Layer(nn.Module):\n",
    "    \"\"\"\n",
    "    Feature extration layer\n",
    "    - Layer 1: BiLSTM + Dropout + Layernorm\n",
    "    - Layer 2: LSTM with Residual Connection + Dropout + Layernorm\n",
    "    - Layer 3: LSTM + Batchnorm + ReLU + Dropout\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_size, lstm_hidden_size, rnn_dropout=0.2, mlp_dropout=0.4, **kwargs):\n",
    "        super(LSTM_Extraction_Layer, self).__init__(**kwargs)\n",
    "        self.embed_size = embed_size\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.bi_lstm = nn.LSTM(input_size=embed_size, hidden_size=lstm_hidden_size, bias=True, bidirectional=True)\n",
    "        self.rnn_dropout_1 = nn.Dropout(p=rnn_dropout)\n",
    "        self.layernorm_1 = nn.LayerNorm(2*lstm_hidden_size)\n",
    "        self.lstm_1 = nn.LSTM(input_size=2*lstm_hidden_size, hidden_size=2*lstm_hidden_size)\n",
    "        self.rnn_dropout_2 = nn.Dropout(p=rnn_dropout)\n",
    "        self.layernorm_2 = nn.LayerNorm(2*lstm_hidden_size)\n",
    "        self.lstm_2 = nn.LSTM(input_size=2*lstm_hidden_size, hidden_size=2*lstm_hidden_size)\n",
    "        self.batchnorm = nn.BatchNorm1d(2*lstm_hidden_size)\n",
    "        self.mlp_dropout = nn.Dropout(p=mlp_dropout)\n",
    "        \n",
    "    def forward(self, inp_embed, inp_last_idx):\n",
    "        bilstm_out, _ = self.bi_lstm(inp_embed.permute(1,0,2))                            # (max_seq_length, batch_size, embed_size) -> (max_seq_length, batch_size, 2*lstm_hidden_size)\n",
    "        bilstm_out = self.layernorm_1(self.rnn_dropout_1(bilstm_out))                     # (max_seq_length, batch_size, 2*lstm_hidden_size)\n",
    "        lstm_out, _ = self.lstm_1(bilstm_out)                                             # (max_seq_length, batch_size, 2*lstm_hidden_size)\n",
    "        lstm_out = self.rnn_dropout_2(lstm_out)                                           # (max_seq_length, batch_size, 2*lstm_hidden_size)\n",
    "        lstm_out = self.layernorm_2(lstm_out+bilstm_out)                                  # (max_seq_length, batch_size, 2*lstm_hidden_size)\n",
    "        lstm_out, _ = self.lstm_2(lstm_out)                                               # (max_seq_length, batch_size, 2*lstm_hidden_size)\n",
    "        lstm_out = lstm_out.permute(1,0,2)[np.arange(len(inp_last_idx)), inp_last_idx,:]  # (batch_size, 2*lstm_hidden_size)\n",
    "        lstm_out = self.mlp_dropout(F.relu(self.batchnorm(lstm_out)))                     # (batch_size, 2*lstm_hidden_size)\n",
    "        return lstm_out\n",
    "    \n",
    "class MLP_Classification_Layer(nn.Module):\n",
    "    \"\"\"\n",
    "    Multilayer Perception Classification Layer\n",
    "    - Layer 1: Linear + Batchnorm + ReLU + Dropout\n",
    "    - Layer 2: Linear + Batchnorm + ReLU + Dropout\n",
    "    - Layer 3: Linear\n",
    "    \"\"\"\n",
    "    def __init__(self, inp_size, out_size, dropout=0.4, **kwargs):\n",
    "        super(MLP_Classification_Layer, self).__init__(**kwargs)\n",
    "        self.inp_size = inp_size\n",
    "        self.out_size = out_size\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.mlp_1 = nn.Linear(inp_size, 1024)\n",
    "        self.batchnorm_1 = nn.BatchNorm1d(1024)\n",
    "        self.mlp_dropout_1 = nn.Dropout(p=mlp_dropout)\n",
    "        self.mlp_2 = nn.Linear(1024, 512)\n",
    "        self.batchnorm_2 = nn.BatchNorm1d(512)\n",
    "        self.mlp_dropout_2 = nn.Dropout(p=mlp_dropout)\n",
    "        self.mlp_3 = nn.Linear(512, out_size)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        mlp_out = self.mlp_1(inp)                                                         # (batch_size, 1024)\n",
    "        mlp_out = self.mlp_dropout_1(F.relu(self.batchnorm_1(mlp_out)))                   # (batch_size, 1024)\n",
    "        mlp_out = self.mlp_2(mlp_out)                                                     # (batch_size, 512)\n",
    "        mlp_out = self.mlp_dropout_2(F.relu(self.batchnorm_2(mlp_out)))                   # (batch_size, 512)\n",
    "        mlp_out = self.mlp_3(mlp_out)                                                     # (batch_size, out_size)\n",
    "        return mlp_out   \n",
    "    \n",
    "class Multi_Seq_LSTM_Classifier(nn.Module):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Classifier(nn.Module):\n",
    "    def __init__(self, embed_size, lstm_hidden_size, out_size, rnn_dropout=0.2, mlp_dropout=0.4, **kwargs):\n",
    "        super(LSTM_Classifier, self).__init__(**kwargs)\n",
    "        self.embed_size = embed_size\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.out_size = out_size\n",
    "        self.rnn_dropout = rnn_dropout\n",
    "        self.mlp_dropout = mlp_dropout\n",
    "        \n",
    "        self.bi_lstm = nn.LSTM(input_size=embed_size, hidden_size=lstm_hidden_size, bias=True, bidirectional=True)\n",
    "        self.rnn_dropout_1 = nn.Dropout(p=rnn_dropout)\n",
    "        self.layernorm_1 = nn.LayerNorm(2*lstm_hidden_size)\n",
    "        self.lstm_1 = nn.LSTM(input_size=2*lstm_hidden_size, hidden_size=2*lstm_hidden_size)\n",
    "        self.rnn_dropout_2 = nn.Dropout(p=rnn_dropout)\n",
    "        self.layernorm_2 = nn.LayerNorm(2*lstm_hidden_size)\n",
    "        self.lstm_2 = nn.LSTM(input_size=2*lstm_hidden_size, hidden_size=2*lstm_hidden_size)\n",
    "        self.batchnorm_1 = nn.BatchNorm1d(2*lstm_hidden_size)\n",
    "        self.mlp_dropout_1 = nn.Dropout(p=mlp_dropout)\n",
    "        self.mlp_1 = nn.Linear(2*lstm_hidden_size, 1024)\n",
    "        self.batchnorm_2 = nn.BatchNorm1d(1024)\n",
    "        self.mlp_dropout_2 = nn.Dropout(p=mlp_dropout)\n",
    "        self.mlp_2 = nn.Linear(1024, 512)\n",
    "        self.batchnorm_3 = nn.BatchNorm1d(512)\n",
    "        self.mlp_dropout_3 = nn.Dropout(p=mlp_dropout)\n",
    "        self.mlp_3 = nn.Linear(512, out_size)\n",
    "        \n",
    "    def forward(self, inp_embed, inp_last_idx):\n",
    "        bilstm_out, _ = self.bi_lstm(inp_embed.permute(1,0,2))                            # (max_seq_length, batch_size, embed_size) -> (max_seq_length, batch_size, 2*lstm_hidden_size)\n",
    "        bilstm_out = self.rnn_dropout_1(bilstm_out)                                       # (max_seq_length, batch_size, 2*lstm_hidden_size)\n",
    "        bilstm_out = self.layernorm_1(bilstm_out)                                         # (max_seq_length, batch_size, 2*lstm_hidden_size)\n",
    "        lstm_out, _ = self.lstm_1(bilstm_out)                                             # (max_seq_length, batch_size, 2*lstm_hidden_size)\n",
    "        lstm_out = self.rnn_dropout_2(lstm_out)                                           # (max_seq_length, batch_size, 2*lstm_hidden_size)\n",
    "        lstm_out = self.layernorm_2(lstm_out+bilstm_out)                                  # (max_seq_length, batch_size, 2*lstm_hidden_size)\n",
    "        lstm_out, _ = self.lstm_2(lstm_out)                                               # (max_seq_length, batch_size, 2*lstm_hidden_size)\n",
    "        lstm_out = lstm_out.permute(1,0,2)[np.arange(len(inp_last_idx)), inp_last_idx,:]  # (batch_size, 2*lstm_hidden_size)\n",
    "        lstm_out = self.mlp_dropout_1(F.relu(self.batchnorm_1(lstm_out)))                 # (batch_size, 2*lstm_hidden_size)\n",
    "        mlp_out = self.mlp_1(lstm_out)                                                    # (batch_size, 1024)\n",
    "        mlp_out = self.mlp_dropout_2(F.relu(self.batchnorm_2(mlp_out)))                   # (batch_size, 1024)\n",
    "        mlp_out = self.mlp_2(mlp_out)                                                     # (batch_size, 512)\n",
    "        mlp_out = self.mlp_dropout_3(F.relu(self.batchnorm_3(mlp_out)))                   # (batch_size, 512)\n",
    "        mlp_out = self.mlp_3(mlp_out)                                                     # (batch_size, out_size)\n",
    "        return mlp_out   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
